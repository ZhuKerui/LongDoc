{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYOnCMh83ZRE"
      },
      "outputs": [],
      "source": [
        "import time, datetime, json, os\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "from nltk import sent_tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "torch.backends.cuda.enable_flash_sdp(False)\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from index_files import LongDoc, write_json, QualityDataset, NarrativeQADataset, ReadingAgent, read_json, read_jsonline, LLM, Retriever, RetrieverOutput, GritLM, ChunkInfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# gritlm = GritLM(\"GritLM/GritLM-7B\", device_map=\"auto\", torch_dtype=\"auto\")\n",
        "retriever = Retriever()\n",
        "llm = LLM()\n",
        "# llm = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
        "# llm = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = NarrativeQADataset(llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = QualityDataset(llm, split='dev')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YKNTyDsXNIn"
      },
      "outputs": [],
      "source": [
        "reading_agent = ReadingAgent(dataset, llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Index passages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Set, Dict\n",
        "import itertools\n",
        "import networkx as nx\n",
        "from prompt import *\n",
        "\n",
        "def match_entities(target_ents:List[str], refer_ents:List[str]):\n",
        "    target_ents_emb = retriever.embed_paragraphs(target_ents, True)\n",
        "    refer_ents_emb = retriever.embed_paragraphs(refer_ents, True)\n",
        "    sim_mat:np.ndarray = np.matmul(target_ents_emb, refer_ents_emb.T)\n",
        "    ent_map:Dict[str, str] = {}\n",
        "    for eid, ent in enumerate(target_ents):\n",
        "        max_idx = sim_mat[eid].argmax()\n",
        "        if sim_mat[eid, max_idx] > 0.8:\n",
        "            ent_map[ent] = refer_ents[max_idx]\n",
        "    return ent_map\n",
        "    \n",
        "def parse_entities(responses:List[str]):\n",
        "    ent_lists:List[str] = []\n",
        "    ent_cnt = Counter()\n",
        "    ent_cnt_threshold = len(responses) // 2 + 1\n",
        "    for response in responses:\n",
        "        i = 1\n",
        "        temp_ents = []\n",
        "        for line in response.splitlines():\n",
        "            if line.startswith(f'{i}. '):\n",
        "                temp_ents.append(line.split(' ', 1)[1].strip().strip('.'))\n",
        "                i += 1\n",
        "        ent_lists.append(temp_ents)\n",
        "        ent_cnt.update(temp_ents)\n",
        "    g = nx.Graph()\n",
        "    for list1, list2 in itertools.combinations(ent_lists, 2):\n",
        "        g.add_edges_from(match_entities(list1, list2).items())\n",
        "    ent_cluster:Set[str]\n",
        "    rep_cnt:Dict[str, int] = {}\n",
        "    for ent_cluster in nx.connected_components(g):\n",
        "        cnts = [(ent_cnt[ent], ent) for ent in ent_cluster]\n",
        "        cnts.sort(key=lambda x: x[0], reverse=True)\n",
        "        rep_cnt[cnts[0][1]] = sum([cnt for cnt, _ in cnts])\n",
        "    return [rep for rep, cnt in rep_cnt.items() if cnt >= ent_cnt_threshold]\n",
        "    \n",
        "def parse_ent_description(response:str, important_ents:List[str]):\n",
        "    description_dict:Dict[str, str] = {}\n",
        "    for line in response.splitlines():\n",
        "        if line:\n",
        "            ent, description = line.split(': ', 1)\n",
        "            ent = ent.strip()\n",
        "            description = description.strip()\n",
        "            if ent in important_ents:\n",
        "                description_dict[ent] = description\n",
        "    return description_dict\n",
        "\n",
        "def index_text(paragraphs:List[str], w_note:bool=True, r_num:int=1):\n",
        "    results:List[ChunkInfo] = []\n",
        "    relation_graph = nx.Graph()\n",
        "    for paragraph in tqdm(paragraphs):\n",
        "        if results and w_note:\n",
        "            summary_recap = {pid-r_num: ci.summary for pid, ci in enumerate(results[-r_num:])}\n",
        "            summary_recap_str = '\\n\\n'.join([f'Passage {pid}\\n{summary}' for pid, summary in summary_recap.items()])\n",
        "            list_entity_prompt = LongDocPrompt.list_entity_w_note(summary_recap_str, paragraph)\n",
        "        else:\n",
        "            list_entity_prompt = LongDocPrompt.list_entity(paragraph)\n",
        "        # Extract important entities\n",
        "        chat_response = llm(list_entity_prompt, 5, 0.7)[0]\n",
        "        important_ents = parse_entities(chat_response)\n",
        "        \n",
        "        # Generate entity description, summary, relation description\n",
        "        important_ents_str = '\\n'.join(important_ents)\n",
        "        if results and w_note:\n",
        "            # ent_description_recap:Dict[str, Dict[int, str]] = {}\n",
        "            # relation_description_recap:Dict[int, List[Tuple[List[str], str]]] = {}\n",
        "            # recaps = []\n",
        "            # for rid, result in enumerate(results[-r_num:]):\n",
        "            #     prev_description_dict:Dict[str, str] = result['description_dict']\n",
        "            #     match_dict = match_entities(important_ents, list(prev_description_dict.keys()))\n",
        "            #     prev_description = '\\n'.join([f'{ent}: {prev_description_dict[ent]}' for _, ent in match_dict.items()])\n",
        "            #     recap = f'Passage {rid - r_num}:\\nEntity descriptions:\\n{prev_description}\\nSummary:\\n{result[\"shorten\"]}'\n",
        "            #     recaps.append(recap)\n",
        "            # recap_str = '\\n\\n'.join(recaps)\n",
        "            pass\n",
        "        else:\n",
        "            ent_description_prompt = LongDocPrompt.ent_description(paragraph, important_ents_str, important_ents[0], important_ents[1])\n",
        "            summary_prompt = LongDocPrompt.shorten(paragraph)\n",
        "            relation_description_prompt = LongDocPrompt.relation_description(paragraph, important_ents_str)\n",
        "        \n",
        "        ent_description, relation_description, summary = llm([ent_description_prompt, summary_prompt, relation_description_prompt])\n",
        "        ent_description, relation_description, summary = ent_description[0], relation_description[0], summary[0]\n",
        "        ent_description_dict = parse_ent_description(ent_description, important_ents)\n",
        "        results.append(ChunkInfo(paragraph, summary, important_ents, ent_description_dict, relation_description))\n",
        "        \n",
        "        # if len(results):\n",
        "        #     prompt_ent_description = prompt_ent_description_w_note_template.format(recap=recap_str, paragraph=paragraph, context_type=context_type, important_ents_str=important_ents_str, important_ents_0=important_ents[0], important_ents_1=important_ents[1])\n",
        "        #     prompt_shorten = prompt_shorten_w_note_template.format(recap_str, paragraph)\n",
        "        #     prompt_relation_description = prompt_relation_description_w_note_template.format(recap=recap_str, paragraph=paragraph, context_type=context_type, important_ents_str=important_ents_str)\n",
        "            \n",
        "        #     ent_description, relation_description, shorten = llm([prompt_ent_description, prompt_relation_description, prompt_shorten])\n",
        "        #     ent_description, relation_description, shorten = ent_description[0], relation_description[0], shorten[0]\n",
        "        #     description_dict = {}\n",
        "        #     for line in ent_description.splitlines():\n",
        "        #         if line:\n",
        "        #             ent, description = line.split(': ', 1)\n",
        "        #             ent = ent.strip()\n",
        "        #             description = description.strip()\n",
        "        #             if ent in important_ents:\n",
        "        #                 description_dict[ent] = description\n",
        "        \n",
        "            \n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "paragraphs = ['\\n'.join(p) for p in read_json(os.path.join(dataset.data_dir, f'pages_{1}.json'))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_results, results = index_text(paragraphs)\n",
        "write_json('results.json', results)\n",
        "write_json('test_results.json', test_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = read_json('results.json')\n",
        "test_results = read_json('test_results.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_pid = 12\n",
        "# sent_tokenize(results[test_pid]['shorten'])\n",
        "# results[test_pid]['description_dict']\n",
        "print(results[test_pid]['prompt_shorten'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sent_tokenize(test_results[test_pid]['shorten'])\n",
        "# test_results[test_pid]['description_dict']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(results[test_pid]['paragraph'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DPR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_sents(retriever_tokenizer:AutoTokenizer, p_input_ids:np.ndarray, is_contriever:bool):\n",
        "    sents = sent_tokenize(retriever_tokenizer.decode(p_input_ids[1:-1] if is_contriever else p_input_ids))\n",
        "    sent_lens = [len(retriever_tokenizer.encode(sent)) - 2 for sent in sents]\n",
        "    sent_start = 1 if is_contriever else 0\n",
        "    sent_spans = []\n",
        "    for sid in range(len(sents)):\n",
        "        sent_end = sent_start + sent_lens[sid]\n",
        "        while len(retriever_tokenizer.decode(p_input_ids[sent_start:sent_end]).strip()) < len(sents[sid]):\n",
        "            sent_end += 1\n",
        "        sent_spans.append((sent_start, sent_end))\n",
        "        sent_start = sent_end\n",
        "    return sent_spans\n",
        "\n",
        "def important_page_tokens(retriever_tokenizer:AutoTokenizer, question:str, pages, q_lhs:np.ndarray, q_input_ids:np.ndarray, q_emb, p_lhs, p_input_ids, pids, scores):\n",
        "    print(question)\n",
        "    for i in range(q_lhs.shape[0]):\n",
        "        print(retriever_tokenizer.decode(q_input_ids[i]), np.linalg.norm(q_lhs[i]))\n",
        "    print('\\n')\n",
        "    for rank, (pid, score) in enumerate(zip(pids, scores)):\n",
        "        print(f'Rank {rank}\\nPassage {pid}:\\n{score}\\n')\n",
        "        print(pages[pid])\n",
        "        token_scores = p_lhs[pid].dot(q_emb)\n",
        "        max_indices = np.argsort(token_scores)[::-1][:(token_scores>2).sum()].tolist()\n",
        "        print('\\n\\nHigh scored spans:\\n')\n",
        "        for idx in max_indices:\n",
        "            print(token_scores[idx], f'<{retriever_tokenizer.decode(p_input_ids[pid][max(0, idx - 1): idx + 1])}>', retriever_tokenizer.decode(p_input_ids[pid][max(0, idx - 5): idx + 5]))\n",
        "        print('\\n\\n')\n",
        "        \n",
        "def query_indicatiors(retriever_tokenizer:AutoTokenizer, question:str, pages, q_lhs:np.ndarray, q_input_ids:np.ndarray, p_lhs, p_input_ids, pids, scores):\n",
        "    print(question)\n",
        "    for i in range(q_lhs.shape[0]):\n",
        "        print(retriever_tokenizer.decode(q_input_ids[i]), np.linalg.norm(q_lhs[i]))\n",
        "    print('\\n')\n",
        "    for rank, (pid, score) in enumerate(zip(pids, scores)):\n",
        "        print(f'Rank {rank}\\nPassage {pid}:\\n{score}\\n')\n",
        "        print(pages[pid])\n",
        "        print('\\n\\nHigh scored spans:\\n')\n",
        "        q_token_scores = np.matmul(q_lhs, p_lhs[pid].T)\n",
        "        x = [retriever_tokenizer.decode(q_token) for q_token in q_input_ids]\n",
        "        y = [token_scores.mean() for token_scores in q_token_scores]\n",
        "        x.reverse()\n",
        "        y.reverse()\n",
        "        plt.barh(x, y)\n",
        "        plt.show()\n",
        "        for q_token, token_scores in zip(q_input_ids, q_token_scores):\n",
        "            max_indices = np.argsort(token_scores)[::-1].tolist()[:10]\n",
        "            print(token_scores.mean(), f'<{retriever_tokenizer.decode(q_token)}>', *[(token_scores[idx], f'<{retriever_tokenizer.decode(p_input_ids[pid][max(0, idx - 5): idx + 1])}>') for idx in max_indices])\n",
        "        print('\\n\\n')\n",
        "        \n",
        "def query_indicator_sents(retriever_tokenizer:AutoTokenizer, pages, q_lhs:np.ndarray, q_input_ids:np.ndarray, p_lhs, p_input_ids, test_pid, test_q_token_id:int, is_contriever:bool):\n",
        "    print(retriever_tokenizer.decode(q_input_ids[test_q_token_id]), '\\n')\n",
        "    print(pages[test_pid], '\\n')\n",
        "    sent_spans = split_sents(retriever_tokenizer, p_input_ids[test_pid], is_contriever)\n",
        "    scores = p_lhs[test_pid].dot(q_lhs[test_q_token_id])\n",
        "    sent_scores = [(scores[sent_span[0]:sent_span[1]].mean(), retriever_tokenizer.decode(p_input_ids[test_pid][sent_span[0]:sent_span[1]])) for sent_span in sent_spans]\n",
        "    sent_scores.sort(key=lambda x: x[0], reverse=True)\n",
        "    for score, sent in sent_scores:\n",
        "        print(score, sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_i = 2\n",
        "pages = ['\\n'.join(p) for p in read_json(os.path.join(dataset.data_dir, f'pages_{test_i}.json'))]\n",
        "questions, answers = dataset.get_questions_and_answers(dataset.data[test_i])\n",
        "questions = [q.splitlines()[0] for q in questions]\n",
        "questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qid = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Contriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "q_embedding = retriever.embed_paragraphs([questions[qid]], normalize=True, complete_return=True)\n",
        "page_embeddings = retriever.embed_paragraphs(pages, normalize=True, complete_return=True)\n",
        "c_q_emb, c_q_input_ids, c_q_lhs = q_embedding.embeddings, q_embedding.input_ids, q_embedding.last_hidden_states\n",
        "c_p_emb, c_p_input_ids, c_p_lhs = page_embeddings.embeddings, page_embeddings.input_ids, page_embeddings.last_hidden_states\n",
        "c_retriever_tokenizer = retriever.retriever_tokenizer\n",
        "c_pids, c_scores = retriever.dense_retrieval(c_q_emb, c_p_emb, 10, normalize=False, return_score=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "important_page_tokens(c_retriever_tokenizer, questions[qid], pages, c_q_lhs[0], c_q_input_ids[0], c_q_emb[0], c_p_lhs, c_p_input_ids, c_pids, c_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query_indicatiors(c_retriever_tokenizer, questions[qid], pages, c_q_lhs[0], c_q_input_ids[0], c_p_lhs, c_p_input_ids, c_pids, c_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_q_token_id = 15\n",
        "test_pid = 4\n",
        "query_indicator_sents(c_retriever_tokenizer, pages, c_q_lhs[0], c_q_input_ids[0], c_p_lhs, c_p_input_ids, test_pid, test_q_token_id, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GritLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gritlm_instruction(instruction):\n",
        "    return \"<|user|>\\n\" + instruction + \"\\n<|embed|>\\n\" if instruction else \"<|embed|>\\n\"\n",
        "\n",
        "instruction = \"Retrieve relevant passages from a story to answer a given question.\"\n",
        "# No need to add instruction for retrieval documents\n",
        "g_p_emb, g_p_input_ids, g_p_lhs = gritlm.encode(pages, instruction=gritlm_instruction(\"\"), max_length=2048)\n",
        "g_q_emb, g_q_input_ids, g_q_lhs = gritlm.encode([questions[qid]], instruction=gritlm_instruction(instruction))\n",
        "g_retriever_tokenizer = gritlm.tokenizer\n",
        "g_pids, g_scores = retriever.dense_retrieval(g_q_emb, g_p_emb, 10, normalize=False, return_score=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "important_page_tokens(g_retriever_tokenizer, questions[qid], pages, g_q_lhs[0], g_q_input_ids[0], g_q_emb[0], g_p_lhs, g_p_input_ids, g_pids, g_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query_indicatiors(g_retriever_tokenizer, questions[qid], pages, g_q_lhs[0], g_q_input_ids[0], g_p_lhs, g_p_input_ids, g_pids, g_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_q_token_id = 15\n",
        "test_pid = 4\n",
        "query_indicator_sents(g_retriever_tokenizer, pages, g_q_lhs[0], g_q_input_ids[0], g_p_lhs, g_p_input_ids, test_pid, test_q_token_id, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(pages[3])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
