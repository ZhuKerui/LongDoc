{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MYOnCMh83ZRE"
      },
      "outputs": [],
      "source": [
        "import time, datetime, json, os\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "from nltk import sent_tokenize\n",
        "\n",
        "from index_files import LongDoc, write_json, QualityDataset, NarrativeQADataset, ReadingAgent, read_json, read_jsonline, LLM, Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/keruiz2/miniconda3/envs/longdoc/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n"
          ]
        }
      ],
      "source": [
        "retriever = Retriever()\n",
        "# llm = LLM()\n",
        "llm = 'mistralai/Mistral-7B-Instruct-v0.2'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = NarrativeQADataset(llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = QualityDataset(llm, split='dev')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YKNTyDsXNIn"
      },
      "outputs": [],
      "source": [
        "reading_agent = ReadingAgent(dataset, llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Index passages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Set, Dict\n",
        "import itertools\n",
        "import networkx as nx\n",
        "from prompt import prompt_shorten_template, prompt_ent_description_template, \\\n",
        "    prompt_relation_description_template, prompt_shorten_w_note_template, \\\n",
        "    prompt_ent_description_w_note_template, prompt_relation_description_w_note_template\n",
        "\n",
        "def match_entities(target_ents:List[str], refer_ents:List[str]):\n",
        "    target_ents_emb = retriever.embed_paragraphs(target_ents, True)\n",
        "    refer_ents_emb = retriever.embed_paragraphs(refer_ents, True)\n",
        "    sim_mat:np.ndarray = np.matmul(target_ents_emb, refer_ents_emb.T)\n",
        "    ent_map:Dict[str, str] = {}\n",
        "    for eid, ent in enumerate(target_ents):\n",
        "        max_idx = sim_mat[eid].argmax()\n",
        "        if sim_mat[eid, max_idx] > 0.8:\n",
        "            ent_map[ent] = refer_ents[max_idx]\n",
        "    return ent_map\n",
        "    \n",
        "def index_text(paragraphs:List[str], context_type:str='novel'):\n",
        "    results = []\n",
        "    test_results = []\n",
        "    for paragraph in tqdm(paragraphs):\n",
        "        \n",
        "        # Extract important entities\n",
        "        list_entity_prompt = f'''Context:\\n\\n{paragraph}\\n\\nAbove is part of a {context_type}. List the important named entities in the above context that are relevant to most of its content. Don't give any explanation. Generate your response in the following format: \"Important entities:\\n1. Entity 1\\n2. Entity 2\\n3. Entity 3\\n...\"'''\n",
        "        chat_response = llm(list_entity_prompt, 5, 0.7)[0]\n",
        "        ent_lists:List[str] = []\n",
        "        ent_cnt = Counter()\n",
        "        for response in chat_response:\n",
        "            i = 1\n",
        "            temp_ents = []\n",
        "            for line in response.splitlines():\n",
        "                if line.startswith(f'{i}. '):\n",
        "                    temp_ents.append(line.split(' ', 1)[1].strip().strip('.'))\n",
        "                    i += 1\n",
        "            ent_lists.append(temp_ents)\n",
        "            ent_cnt.update(temp_ents)\n",
        "        g = nx.Graph()\n",
        "        for list1, list2 in itertools.combinations(ent_lists, 2):\n",
        "            g.add_edges_from(match_entities(list1, list2).items())\n",
        "        ent_cluster:Set[str]\n",
        "        rep_cnt = {}\n",
        "        for ent_cluster in nx.connected_components(g):\n",
        "            cnts = [(ent_cnt[ent], ent) for ent in ent_cluster]\n",
        "            cnts.sort(key=lambda x: x[0], reverse=True)\n",
        "            rep_cnt[cnts[0][1]] = sum([cnt for cnt, _ in cnts])\n",
        "        important_ents = [rep for rep, cnt in rep_cnt.items() if cnt >= 3]\n",
        "        \n",
        "        # Generate entity description, summary, relation description\n",
        "        important_ents_str = '\\n'.join(important_ents)\n",
        "        prompt_ent_description = prompt_ent_description_template.format(paragraph=paragraph, context_type=context_type, important_ents_str=important_ents_str, important_ents_0=important_ents[0], important_ents_1=important_ents[1])\n",
        "        prompt_shorten = prompt_shorten_template.format(paragraph)\n",
        "        prompt_relation_description = prompt_relation_description_template.format(paragraph=paragraph, context_type=context_type, important_ents_str=important_ents_str)\n",
        "        \n",
        "        ent_description, relation_description, shorten = llm([prompt_ent_description, prompt_relation_description, prompt_shorten])\n",
        "        ent_description, relation_description, shorten = ent_description[0], relation_description[0], shorten[0]\n",
        "        description_dict = {}\n",
        "        for line in ent_description.splitlines():\n",
        "            if line:\n",
        "                ent, description = line.split(': ', 1)\n",
        "                ent = ent.strip()\n",
        "                description = description.strip()\n",
        "                if ent in important_ents:\n",
        "                    description_dict[ent] = description\n",
        "        test_results.append({\n",
        "            'paragraph': paragraph, \n",
        "            'important_ents': important_ents, \n",
        "            'description_dict': description_dict, \n",
        "            'shorten': shorten, \n",
        "            'relation_description': relation_description\n",
        "        })\n",
        "        \n",
        "        if len(results):\n",
        "            r_num = 1\n",
        "            recaps = []\n",
        "            for rid, result in enumerate(results[-r_num:]):\n",
        "                prev_description_dict:Dict[str, str] = result['description_dict']\n",
        "                match_dict = match_entities(important_ents, list(prev_description_dict.keys()))\n",
        "                prev_description = '\\n'.join([f'{ent}: {prev_description_dict[ent]}' for _, ent in match_dict.items()])\n",
        "                recap = f'Passage {rid - r_num}:\\nEntity descriptions:\\n{prev_description}\\nSummary:\\n{result[\"shorten\"]}'\n",
        "                recaps.append(recap)\n",
        "            recap_str = '\\n\\n'.join(recaps)\n",
        "            prompt_ent_description = prompt_ent_description_w_note_template.format(recap=recap_str, paragraph=paragraph, context_type=context_type, important_ents_str=important_ents_str, important_ents_0=important_ents[0], important_ents_1=important_ents[1])\n",
        "            prompt_shorten = prompt_shorten_w_note_template.format(recap_str, paragraph)\n",
        "            prompt_relation_description = prompt_relation_description_w_note_template.format(recap=recap_str, paragraph=paragraph, context_type=context_type, important_ents_str=important_ents_str)\n",
        "            \n",
        "            ent_description, relation_description, shorten = llm([prompt_ent_description, prompt_relation_description, prompt_shorten])\n",
        "            ent_description, relation_description, shorten = ent_description[0], relation_description[0], shorten[0]\n",
        "            description_dict = {}\n",
        "            for line in ent_description.splitlines():\n",
        "                if line:\n",
        "                    ent, description = line.split(': ', 1)\n",
        "                    ent = ent.strip()\n",
        "                    description = description.strip()\n",
        "                    if ent in important_ents:\n",
        "                        description_dict[ent] = description\n",
        "        \n",
        "            results.append({\n",
        "                'paragraph': paragraph, \n",
        "                'important_ents': important_ents, \n",
        "                'description_dict': description_dict, \n",
        "                'shorten': shorten, \n",
        "                'relation_description': relation_description\n",
        "            })\n",
        "        else:\n",
        "            results.append({\n",
        "                'paragraph': paragraph, \n",
        "                'important_ents': important_ents, \n",
        "                'description_dict': description_dict, \n",
        "                'shorten': shorten, \n",
        "                'relation_description': relation_description\n",
        "            })\n",
        "    return test_results, results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "paragraphs = ['\\n'.join(p) for p in read_json(os.path.join(dataset.data_dir, f'pages_{1}.json'))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_results, results = index_text(paragraphs)\n",
        "write_json('results.json', results)\n",
        "write_json('test_results.json', test_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = read_json('results.json')\n",
        "test_results = read_json('test_results.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_pid = 12\n",
        "# sent_tokenize(results[test_pid]['shorten'])\n",
        "# results[test_pid]['description_dict']\n",
        "print(results[test_pid]['prompt_shorten'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sent_tokenize(test_results[test_pid]['shorten'])\n",
        "# test_results[test_pid]['description_dict']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(results[test_pid]['paragraph'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DPR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_i = 0\n",
        "pages = ['\\n'.join(p) for p in read_json(os.path.join(dataset.data_dir, f'pages_{test_i}.json'))]\n",
        "questions, answers = dataset.get_questions_and_answers(dataset.data[test_i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"How much time has passed between Blake's night with Eldoria and his search for Sabrina York in his mind-world?\",\n",
              " 'Why does Deirdre get so upset when Blake Past suggests she go to prom with the young man?',\n",
              " \"Why does shame flame in Blake's cheeks when Deirdre goes to prepare Eldoria's dias?\",\n",
              " 'Why did Blake create the three female super-images of Miss Stoddart, Officer Finch, and Vera Velvetskin?',\n",
              " 'Sabrina York is ',\n",
              " \"Why doesn't Blake haggle with Eldoria about the price for her services?\"]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "questions = [q.splitlines()[0] for q in questions]\n",
        "questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "qid = 0\n",
        "pids = retriever.dense_retrieval(questions[qid], pages)\n",
        "retrieved_pages = [(pages[i], i) for i in pids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How much time has passed between Blake's night with Eldoria and his search for Sabrina York in his mind-world?\n",
            "\n",
            "\n",
            "Passage 13:\n",
            "\n",
            " By their very nature, mind-countries were confusing. They existed on a plane of reality that bore no apparent relationship to the plane of the so-called objective universe. In fact, so far as was known, this secondary—or subjective—reality was connected to so-called true reality only through the awareness of the various creators. In addition, these countries had no outward shape in the ordinary sense of the word, and while most countries contained certain parallel images, these images were subject to the interpretation of the individual creator. As a result they were seldom identical. It was inevitable that sooner or later some criminal would hit upon the idea of hiding out in his own mind-world till the statute of limitations that applied to his particular crime ran out, and it was equally inevitable that others should follow suit. Society's answer was the psyche-police, and the psyche-police hadn't been in action very long before the first private psycheye appeared. \n",
            " Blake was one of a long line of such operators. \n",
            " So far as he knew, the present case represented the first time a criminal had ever hidden out in the pursuer's mind. It would have been a superb stratagem indeed if, shortly after her entry, Sabrina York had not betrayed her presence. For her point of entry she had used the place-time materialization of the little office Blake had opened on Ex-earth at the beginning of his career. Unaccountably she had ransacked it before moving into a co-terminous memory-image. \n",
            "\n",
            "\n",
            "\n",
            "Passage 14:\n",
            "\n",
            " Even this action wouldn't have given her away, however, if the office hadn't constituted a sentimental memory. Whenever Blake accepted a case he invariably thought of the bleak and lonely little room with its thin-gauge steel desk and battered filing cabinets, and when he had done so after accepting his case—or was it before? He couldn't quite remember—the mental picture that had come into his mind had revealed open drawers, scattered papers and a general air of disarray. \n",
            " He had suspected the truth immediately, and when he had seen the woman's handkerchief with the initials \"SB\" embroidered on it lying by one of the filing cabinets he had known definitely that his quarry was hiding out in his mind. Retiring to his bachelor quarters, he had entered at the same place-time and set off in pursuit. \n",
            " Her only advantage lost, Sabrina York was now at his mercy. Unless she discovered his presence and was able to locate his most recently materialized place-time before he over-took her, her capture was assured. \n",
            " Only two things bothered Blake. The little office was far in his past, and it was unlikely that anyone save the few intimate acquaintances whom he had told about it were aware that it had ever existed. How, then, had a total stranger such as Sabrina York learned enough about it to enable her to use it as a point of entry? \n",
            " The other thing that bothered him was of a much more urgent nature. He had been in enough minds and he had read enough on the subject of Trevorism to know that people were sometimes capable of creating beings considerably higher on the scale of mind-country evolution than ordinary memory-ghosts. One woman whom he had apprehended in her own mind had created a walking-talking Virgin Mary who watched over her wherever she went. And once, after tracking down an ex-enlisted man, he had found his quarry holed up in the memory-image of an army barracks with a ten-star general waiting on him hand and foot. But these, and other, similar, cases, had to do with mal-adjusted people, and moreover, the super-image in each instance had been an image that the person involved had wanted to create. Therefore, even assuming that Blake was less well-adjusted than he considered himself to be, why had he created three such malevolent super-images as Miss Stoddart, Officer Finch, and Vera Velvetskin? They followed him off the campus into a vicarious memory-image of Walden Pond, Thoreau's shack, and the encompassing woods. Judging from the ecstatic \"oh's\" and \"ah's\" they kept giving voice to, the place delighted them. Once, glancing back over his shoulder, he saw them standing in front of Thoreau's shack, looking at it as though it were a doll's house. Not far away, Thoreau was sitting in under a tall pine, gazing up into the branches at a bird that had come through only as a vague blur of beak and feathers. \n",
            "\n",
            "\n",
            "\n",
            "Passage 15:\n",
            "\n",
            " Blake went on. Presently the Walden Pond memory-image gave way to a memory-image of an English park which the ex-Earth government had set aside as a memorial to the English poets and which had impressed Blake sufficiently when he had visited it in his youth to have found a place for itself in the country of his mind. It consisted of reconstructions of famous dwellings out of the lives of the poets, among them, a dwelling out of the life of a poet who was not in the strictest sense of the word English at all—the birthplace of Robert Burns. Oddly enough, it was Burns's birthplace that had impressed Blake most. Now the little cottage stood out in much more vivid detail than any of the other famous dwellings. \n",
            " Sabrina York must have been attracted to the place, for her footprints showed that she had turned in at the gate, walked up the little path and let herself in the door. \n",
            " They also showed that she had left by the same route, so there was no reason for Blake to linger. As a matter of fact, the fascination that had brought the place into being had been replaced by an illogical repugnance. But repugnance can sometimes be as compelling a force as fascination, and Blake not only lingered but went inside as well. \n",
            " He remembered the living room distinctly—the flagstone floor, the huge grill-fronted hearth, the deeply recessed window, the rack of cups and platters on the wall; the empty straight-backed chair standing sternly in a corner, the bare wooden table— \n",
            " He paused just within the doorway. The chair was no longer empty, the table no longer bare. \n",
            " A man sat on the former and a bottle of wine stood on the latter. Moreover, the room showed signs of having been lived in for a long time. The floor was covered with tracked-in dirt and the walls were blackened from smoke. The grill-work of the hearth was begrimed with grease.\n",
            "\n",
            "\n",
            "\n",
            "Passage 6:\n",
            "\n",
            " Warily he stepped inside, adjusting the temperature of his all-weather jacket to the remembered air-conditioning. His father was sitting in the living room, smoking, and watching 3V. He had no awareness of Blake. At Blake's entry he went right on smoking and watching as though the door had neither opened nor closed. He would go right on smoking and watching till Blake died and the conglomeration of place-times that constituted Blake's mind-world ceased to be. Ironically, he was watching nothing. The 3V program that had been in progress at the time of the unconscious materialization had failed to come through. The memory was a treasured one—the old man had perished in a 'copter crash several years ago—and for a long while Blake did not move. He had never been in his own mind before. Consequently he was more affected than he might otherwise have been. Finally, stirring himself, he walked out into the kitchen. On a shelf above the sink stood a gaily colored box of his mother's favorite detergent with a full-length drawing of Vera Velvetskin, the company's blond and chic visual symbol, on the front. His mother was standing before the huge automatic range, preparing a meal she had served twenty-three years ago. He regarded her with moist eyes. She had died a dozen years before his father, but the wound that her death had caused had never healed. He wanted to go up behind her and touch her shoulder and say, \"What's for supper, mom?\" but he knew it would do no good. For her he had no reality, not only because he was far in her future, but because in his mind-world she was a mortal and he, a god—a picayune god, perhaps, but a real one. \n",
            " As he was about to turn away, the name-plate on the range caught his eye, and thinking that he had read the two words wrong, he stepped closer so that he could see them more clearly. No, he had made no mistake: the first word was \"Sabrina\", and the second was \"York\". \n",
            "\n",
            "\n",
            "\n",
            "Passage 4:\n",
            "\n",
            " After resting for a few minutes, he descended the hill and started across the Deneb 1 wasteland. It was a remarkably detailed materialization, and his quarry's footprints stood out clearly in the duplicated sand. \n",
            " Sabrina York did not even know the rudiments of the art of throwing off a mind-tracker. It would have done her but little good if she had, for twelve years as a psycheye had taught Blake all the tricks. Probably she had taken it for granted that the mere act of hiding out in her tracker's mind was in itself a sufficient guarantee of her safety. After all, she had no way of knowing that he had discovered her presence. \n",
            " Mind-country was as temporally inconsecutive as it was topographically incongruous, so Blake was not surprised when the Deneb 1 wasteland gave way to an expanse of boyhood meadow. Near the meadow was the house where Blake had lived at a much later date. In reality, the places were as far apart in miles as they were in years, but here in the country of his mind they existed side by side, surrounded by heterogeneous landscapes from all over the civilized sector of the galaxy and by the sharply demarcated spectra of a hundred different suns. A few of the suns were in the patchwork sky—Sirius, for example, and its twinkling dwarf companion. Most of them, however, were present only in their remembered radiance. To add to the confusion, scattered night memories interrupted the hodge-podge horizon with columns of darkness, and here and there the gray column of a dawn or dusk memory showed. \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(questions[qid])\n",
        "print('\\n')\n",
        "for p, pid in retrieved_pages:\n",
        "    print(f'Passage {pid}:\\n')\n",
        "    print(p)\n",
        "    print('\\n\\n')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
