{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MYOnCMh83ZRE"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from nltk import sent_tokenize\n",
        "from transformers import AutoTokenizer\n",
        "import sys\n",
        "import seaborn as sb\n",
        "sys.path.append('../..')\n",
        "from spacy.tokens import Span, Doc\n",
        "\n",
        "from src import *\n",
        "from src.test_utils import *\n",
        "# os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "torch.backends.cuda.enable_flash_sdp(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# gritlm = GritLM(\"GritLM/GritLM-7B\", device_map=\"cuda:2\", torch_dtype=\"auto\")\n",
        "retriever = Retriever(device='cuda:1', syn_dist=0.1)\n",
        "doc_split = DocSplit(retriever.retriever_tokenizer)\n",
        "# llm = LLM()\n",
        "llm = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
        "# llm = None\n",
        "longdoc = LongDoc(retriever, llm)\n",
        "# dataset = NarrativeQADataset(llm)\n",
        "dataset = QualityDataset(llm, split='dev')\n",
        "# reading_agent = ReadingAgent(dataset, llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['What is the most likely meaning of the slang O.Q.? (in twentieth-century American English)',\n",
              " 'Why does the Skipper stop abruptly after he says \"when you\\'re running a blockade\"?',\n",
              " 'Who or what is Leo?',\n",
              " 'Why does the Skipper allow the new chef to use the heat-cannon as an incinerator?',\n",
              " ' Lieutenant Dugan brings up the examples of \"High G\" Gordon and \"Runt\" Hake in order to illustrates that...',\n",
              " \"Why didn't the Skipper follow the new cook's advice about avoiding Vesta?\",\n",
              " 'Why was the new cook so upset that the Skipper decided to surrender?',\n",
              " 'What does the Skipper mean by \"lady-logic\"?',\n",
              " \"What would've happened if the new cook had told the Skipper about the ekalastron deposits earlier?\"]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_i = 2\n",
        "sample = dataset.data[test_i]\n",
        "questions, answers = dataset.get_questions_and_answers(sample)\n",
        "article = dataset.get_article(sample)\n",
        "questions = [q.splitlines()[0] for q in questions]\n",
        "questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Index passages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pages = doc_split.split_paragraphs(article, 512 // 5)\n",
        "results, raw = longdoc.index_text_into_map(pages, 3)\n",
        "write_json('temp.json', [ci.to_json() for ci in results])\n",
        "write_json('raw.json', raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Navigation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "26\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "pages = doc_split.split_paragraphs(article, 300)\n",
        "all_summary = longdoc.lossless_index(pages, 1, 1, 1, 'relation')\n",
        "write_json('all_summary.json', all_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "pages = doc_split.split_paragraphs(article, 300)\n",
        "summary_prompts = [LongDocPrompt.summary(current_passage) for current_passage in pages]\n",
        "all_summary = [LongDocPrompt.parse_summary(response[0]) for response in longdoc.llm_server(summary_prompts)]\n",
        "write_json('all_summary.json', all_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "pages = doc_split.split_paragraphs(article, 300)\n",
        "all_summary = read_json('all_summary.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summarize: 6\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summarize: 2\n",
            "Summarize: 1\n"
          ]
        }
      ],
      "source": [
        "tree = longdoc.build_summary_pyramid(pages, all_summary)\n",
        "dump_tree('temp_tree.json', tree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "tree = load_tree('temp_tree.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tree[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_indices = retriever.dense_retrieval(\"Why didn't the Skipper follow the new cook's advice about avoiding Vesta?\", [node.children[0] for node in tree[0]], normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[6, 20, 19, 4, 8]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Dugan, call McMurtrie and tell him we lift gravs immediately— Slops! What are you doing at that table?\" For the little fellow had sidled across the control-room and now, eyes gleaming inquisitively, was peering at our trajectory charts. At the skipper\\'s roar he glanced up at us eagerly. \"Vesta!\" he piped in that curiously high-pitched and mellow voice. \"Loft trajectory for Vesta! Then we\\'re trying to run the Alliance blockade, Captain?\" \"None of your business!\" bellowed O\\'Hara in tones of thunderous outrage. \"Get below instantly, or by the lavendar lakes of Luna I\\'ll—\" \"If I were you,\" interrupted our diminutive new chef thoughtfully, \"I\\'d try to broach the blockade off Iris rather than Vesta. For one thing, their patrol line will be thinner there; for another, you can come in through the Meteor Bog, using it as a cover.\" \" Mr. Dugan! \" The Old Man\\'s voice had an ominous ring to it, one I had seldom heard. I sprang to attention and saluted smartly. \"Aye, sir?\"']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tree[0][6].children"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_parents(node:MyNode):\n",
        "    parents:List[MyNode] = []\n",
        "    cur_node = node\n",
        "    while cur_node.parent is not None:\n",
        "        parents.append(cur_node.parent)\n",
        "        cur_node = cur_node.parent\n",
        "    return parents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pring_branch_from_leaf(node:MyNode):\n",
        "    parents = get_parents(node)\n",
        "    infos = []\n",
        "    infos.append(('chunk', node.children[0]))\n",
        "    infos.append(('summary', node.summary))\n",
        "    for pid, p in enumerate(parents):\n",
        "        infos.append((f'summary_{pid+1}', p.summary))\n",
        "    return infos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tree[0][6].summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('chunk',\n",
              "  'Dugan, call McMurtrie and tell him we lift gravs immediately— Slops! What are you doing at that table?\" For the little fellow had sidled across the control-room and now, eyes gleaming inquisitively, was peering at our trajectory charts. At the skipper\\'s roar he glanced up at us eagerly. \"Vesta!\" he piped in that curiously high-pitched and mellow voice. \"Loft trajectory for Vesta! Then we\\'re trying to run the Alliance blockade, Captain?\" \"None of your business!\" bellowed O\\'Hara in tones of thunderous outrage. \"Get below instantly, or by the lavendar lakes of Luna I\\'ll—\" \"If I were you,\" interrupted our diminutive new chef thoughtfully, \"I\\'d try to broach the blockade off Iris rather than Vesta. For one thing, their patrol line will be thinner there; for another, you can come in through the Meteor Bog, using it as a cover.\" \" Mr. Dugan! \" The Old Man\\'s voice had an ominous ring to it, one I had seldom heard. I sprang to attention and saluted smartly. \"Aye, sir?\"'),\n",
              " ('summary',\n",
              "  \"In the control room of a spacecraft, Dugan, a crew member, is instructed by the captain, O'Hara, to contact McMurtrie and lift gravs immediately. Another crew member, Slops, who is a new chef, is found sneaking a look at the trajectory charts. O'Hara angrily orders Slops to get below deck. Slops suggests to O'Hara that they should attempt to bypass the Alliance blockade near Iris instead of Vesta, as the patrol line would be thinner and they could use the Meteor Bog as cover. O'Hara's voice becomes threatening, and Dugan, another crew member, springs to attention and salutes in response to the captain's authoritative tone.\"),\n",
              " ('summary_1',\n",
              "  \"In the passage, Dugan, a crew member, offers assistance to a struggling skipper and meets the cook, Slops. Dugan questions Slops about his willingness to sign for a blind journey and his possession of a space certificate. Slops becomes defensive, but Dugan hires him and directs him to the galley. On a spacecraft, Dugan receives orders from Captain O'Hara to contact McMurtrie and lift gravs. Slops, a new chef, is found looking at trajectory charts and suggests an alternative route to bypass the Alliance blockade. O'Hara becomes angry, and Dugan scolds Slops for offering unsolicited advice. Slops expresses his familiarity with the area, but Dugan insists that the Old Man, their space navigator, does not need his suggestions. The crew of the Leo, led by Captain Slops, sets sail for Vesta, with excitement from the crew, including McMurtrie, Wainwright, and Todd. During a meal preparation, Slops is absent as he prepares midday mess, and the crew enjoys a delicious meal prepared by him. Slops is known for his excellent cooking skills, and the meal is the finest feast the crew has had in a long time.\"),\n",
              " ('summary_2',\n",
              "  \"The spaceship Leo, led by Captain O'Hara, faces a challenge in finding a new cook after their previous one fell ill and was replaced. Dugan, a crew member, encounters Slops, a potential cook on Phobos, Mars' inner moon. Slops is hesitant to join due to the blind journey, but Dugan hires him and directs him to the galley. During their voyage to Callisto, Slops suggests an alternative route to bypass the Alliance blockade, but O'Hara becomes angry and dismisses his advice. Slops is known for his excellent cooking skills, and the crew enjoys a delicious meal prepared by him. However, during their journey from Mars to the asteroid belt, Slops interrupts Dugan while he was telling a story, revealing a lack of sense of humor. Slops is also described as bashful. Captain O'Hara promises to address the galley's need for an incinerator and considers using an old Nolan heat-cannon, despite safety regulations. Slops is found cleaning the cannon during the night watch, causing concern among the crew.\"),\n",
              " ('summary_3',\n",
              "  \"The spaceship Leo, under Captain O'Hara's leadership, searches for a new cook after their previous one falls ill. Dugan, a crew member, recruits Slops, a hesitant cook from Phobos, Mars' inner moon. Slops suggests an alternate route to bypass an Alliance blockade, but O'Hara dismisses his advice. The crew appreciates Slops' cooking skills, but finds him lacking a sense of humor and bashful. O'Hara considers using an old Nolan heat-cannon for the galley's incinerator, disregarding safety regulations. Slops is discovered cleaning the cannon during the night watch, causing concern.\")]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pring_branch_from_leaf(tree[0][6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tree[-2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"In the passage, Slops, the cook on Vesta, warns the skipper about the increased risk of pirate raids due to recent riches in ekalastron deposits and Vesta's orbit entering aphelion stage. The skipper is suspicious of Slops for not sharing this information earlier and becomes enraged. The crew's assessment of the risks was off, leading to significant consequences. Their next challenge was navigating past Callisto, as the discovery of their presence by the Callistans could lead to the occupation of Callisto by their adversaries, exposing hidden information. However, it was too late to change course as their lock had already been opened, and the sound of approaching Alliance soldiers was heard on the metal ramp. The Alliance commander stood before them, expressing satisfaction at their impending surrender.\""
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tree[-2][1].summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['CAPTAIN CHAOS By NELSON S. BOND The Callisto-bound Leo needed a cook. What it got was a piping-voiced Jonah who jinxed it straight into Chaos. [Transcriber\\'s Note: This etext was produced from Planet Stories Summer 1942. Extensive research did not uncover any evidence that the U.S. copyright on this publication was renewed.] We picked up our new cook on Phobos. Not Phoebus or Phoebe; I mean Phobos, Mars\\' inner moon. Our regular victual mangler came down with acute indigestion—tasted some of his own cooking, no doubt—when we were just one blast of a jet-tube out of Sand City spaceport. But since we were rocketing under sealed orders, we couldn\\'t turn back. So we laid the Leo down on Phobos\\' tiny cradle-field and bundled our ailing grub-hurler off to a hospital, and the skipper said to me, \"Mister Dugan,\" he said, \"go out and find us a cook!\" \"Aye, sir!\" I said, and went. Only it wasn\\'t that easy. In those days, Phobos had only a handful of settlers, and most of them had good-paying jobs.']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tree[0][0].children"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[\" Yes, the passage is helpful to answer the question. The Skipper, O'Hara, did not follow the new cook's advice about avoiding Vesta because he was angry that the cook, Dugan, had been snooping around the control room and looking at their trajectory charts without permission. O'Hara viewed this as a breach of trust and a potential security risk, so he ordered Dugan to get below deck. In the heat of the moment, O'Hara did not consider the strategic advantages that Dugan had suggested, such as a thinner patrol line and the use of the Meteor Bog as cover. Instead, he focused on punishing Dugan for his disobedience.\"]]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "longdoc.llm_server(f'Passage:\\n{tree[0][6].children[0]}\\n\\nQuestion:\\n{questions[5]}\\n\\nDo you think the above passage is helpful to answer the above question? Explain your answer.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[' The passage does not provide any information about where Bob spent his childhood. The only information given is that Bob died in Champaign. Therefore, the passage is not helpful in answering the question.']]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# passage = tree[0][6].children[0]\n",
        "# question = questions[5]\n",
        "passage = 'Bob died in Champaign.'\n",
        "question = 'Where did Bob spend his childhood?'\n",
        "longdoc.llm_server(f'Passage:\\n{passage}\\n\\nQuestion:\\n{question}\\n\\nDo you think the above passage is helpful to answer the above question? Explain your answer.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_node = tree[-2][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_node.children"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_node.summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TextGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_unimportant(doc:Span, additional_pos_labels:Set[str]=set()):\n",
        "    spans = []\n",
        "    temp_span_start = 0\n",
        "    tid = 0\n",
        "    while tid < len(doc):\n",
        "        t = doc[tid]\n",
        "        if t.pos_ in {'DET', 'PRON', 'CCONJ', 'PUNCT', 'AUX', 'PART'} or t.pos_ in additional_pos_labels:\n",
        "            if temp_span_start != tid:\n",
        "                spans.append((temp_span_start, tid))\n",
        "            temp_span_start = tid + 1\n",
        "        tid += 1\n",
        "    if temp_span_start < tid:\n",
        "        spans.append((temp_span_start, tid))\n",
        "    splitted_doc = [doc[span[0]:span[1]] for span in spans]\n",
        "    return splitted_doc\n",
        "\n",
        "def collect_keywords_from_text(doc:Doc):\n",
        "    ncs = list(doc.noun_chunks)\n",
        "    ents = doc.ents\n",
        "    nc_id, eid = 0, 0\n",
        "    spans:List[Span] = []\n",
        "    # Merge noun chunks with entities\n",
        "    while nc_id < len(ncs) and eid < len(ents):\n",
        "        nc, ent = ncs[nc_id], ents[eid]\n",
        "        if set(range(nc.start, nc.end)).intersection(range(ent.start, ent.end)):\n",
        "            spans.append(doc[min(nc.start, ent.start) : max(nc.end, ent.end)])\n",
        "            nc_id += 1\n",
        "            eid += 1\n",
        "        else:\n",
        "            if nc.start < ent.end:\n",
        "                spans.append(nc)\n",
        "                nc_id += 1\n",
        "            else:\n",
        "                spans.append(ent)\n",
        "                eid += 1\n",
        "    spans.extend(ncs[nc_id:])\n",
        "    spans.extend(ents[eid:])\n",
        "    # Update each noun chunks\n",
        "    updated_spans:List[Span] = []\n",
        "    for span in spans:\n",
        "        updated_spans.extend(remove_unimportant(span, {'ADJ', 'ADV'}))\n",
        "    ent_candidates = {' '.join([t.lemma_ for t in span]) for span in updated_spans}\n",
        "    return ent_candidates\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextGraph:\n",
        "    def __init__(self, docs:List[Doc]) -> None:\n",
        "        self.text_graph = nx.DiGraph()\n",
        "        self.ent_graph = nx.Graph()\n",
        "        self.tokenized_corpus:List[List[str]] = []\n",
        "        ent_pair_counter = Counter()\n",
        "        for pid, doc in enumerate(docs):\n",
        "            tokenized_page = [t.lemma_.lower() for t in doc]\n",
        "            nouns = collect_keywords_from_text(doc)\n",
        "            if len(nouns) >= 2:\n",
        "                ent_pair_counter.update(map(frozenset, itertools.combinations(nouns, 2)))\n",
        "            self.tokenized_corpus.append(tokenized_page)\n",
        "            self.text_graph.add_node(pid, tokenized_page=tokenized_page, nouns=nouns)\n",
        "        for (ent1, ent2), cnt in ent_pair_counter.items():\n",
        "            self.ent_graph.add_edge(ent1, ent2, log_freq=np.log(cnt+1))\n",
        "        self.ent_general_importance:Dict[str, float] = nx.pagerank(self.ent_graph, weight='log_freq')\n",
        "        self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
        "        for pid1 in range(len(docs)):\n",
        "            bm25_scores = self.bm25.get_scores(self.tokenized_corpus[pid1])\n",
        "            bm25_scores = bm25_scores / bm25_scores.sum()\n",
        "            nouns1:Set[str] = self.text_graph.nodes[pid1]['nouns']\n",
        "            for pid2 in range(len(docs)):\n",
        "                if pid1 != pid2:\n",
        "                    overlap = nouns1.intersection(self.text_graph.nodes[pid2]['nouns'])\n",
        "                    if overlap:\n",
        "                        ent_importance = sum([self.ent_general_importance[ent] for ent in overlap])\n",
        "                        dist = 1 / np.log(np.e + np.abs(pid2 - pid1))\n",
        "                        bm25_score = bm25_scores[pid2]\n",
        "                        weight = statistics.harmonic_mean([ent_importance, bm25_score]) * dist\n",
        "                        self.text_graph.add_edge(pid1, pid2, overlap=overlap, ent_importance=ent_importance, dist=dist, bm25_score=bm25_score, weight=weight)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tg = TextGraph([longdoc.nlp(p) for p in all_summary])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list(tg.text_graph.edges.data())[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Topic Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim import corpora\n",
        "from gensim.parsing.preprocessing import preprocess_string, DEFAULT_FILTERS\n",
        "from gensim.models import Phrases, CoherenceModel, LdaModel, EnsembleLda, LdaMulticore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pages = doc_split.split_paragraphs(article, 500)\n",
        "all_summary = read_json('all_summary.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "longdoc.llm_server(f'''\n",
        "Summarize the following passage.\n",
        "\n",
        "Passage:\n",
        "{pages[1]}\n",
        "''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "longdoc.llm_server(f'''\n",
        "Summarize the following passage.\n",
        "\n",
        "Passage:\n",
        "{pages[2]}\n",
        "''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "longdoc.llm_server(f'''\n",
        "What are the common information in the following 2 passages.\n",
        "\n",
        "Passage 1:\n",
        "{pages[1]}\n",
        "\n",
        "Passage 2:\n",
        "{pages[2]}\n",
        "''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "longdoc.llm_server(f'''\n",
        "What are the different information between the following 2 passages.\n",
        "\n",
        "Passage 1:\n",
        "{pages[1]}\n",
        "\n",
        "Passage 2:\n",
        "{pages[2]}\n",
        "''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pages[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(all_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocess_funcs = DEFAULT_FILTERS[:-1] # Remove the stemming\n",
        "preprocessed_summary = [preprocess_string(' '.join([t.lemma_ for t in longdoc.nlp(p, disable=['parser', 'ner'])]), preprocess_funcs) for p in all_summary]\n",
        "\n",
        "# bigram = Phrases(preprocessed_summary, min_count=2, threshold=1)\n",
        "\n",
        "# texts = [bigram[p] for p in preprocessed_summary]\n",
        "texts = preprocessed_summary\n",
        "\n",
        "# Create a dictionary from the corpus\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "\n",
        "# Remove low-frequency terms from the dictionary\n",
        "dictionary.filter_extremes(no_below=2)\n",
        "\n",
        "# Convert the corpus into a bag-of-words representation\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lda_model = EnsembleLda(\n",
        "    corpus=corpus, \n",
        "    id2word=dictionary, \n",
        "    passes=5, \n",
        "    iterations=100, \n",
        "    num_models=5, \n",
        "    # min_cores=10, \n",
        "    # min_samples=4,\n",
        "    epsilon=0.05\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lda_model.print_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic2p = defaultdict(list)\n",
        "for pid, p in enumerate(corpus):\n",
        "    topic_id = sorted(lda_model[p], key=lambda x: x[1])[-1][0]\n",
        "    topic2p[topic_id].append(all_summary[pid])\n",
        "print(lda_model.stable_topics.shape)\n",
        "print([(tid, len(topic2p[tid])) for tid in range(len(topic2p))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic2p[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topics = []\n",
        "score = []\n",
        "topic_models:Dict[int, LdaModel] = {}\n",
        "min_docs_per_topic = 4\n",
        "for topic_num in tqdm(range(4, len(all_summary) // min_docs_per_topic, 4)):\n",
        "    # Build the LDA model\n",
        "    lda_model = LdaMulticore(corpus, topic_num, dictionary, iterations=100, passes=5, workers=5)\n",
        "    cm = CoherenceModel(lda_model, texts = texts, corpus=corpus, dictionary=dictionary, coherence='c_v')\n",
        "    topics.append(topic_num)\n",
        "    score.append(cm.get_coherence())\n",
        "    topic_models[topic_num] = lda_model\n",
        "    \n",
        "plt.plot(topics, score)\n",
        "plt.xlabel('Number of Topics')\n",
        "plt.ylabel('Coherence Score')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lda_model = topic_models[44]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list(lda_model.get_document_topics(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
