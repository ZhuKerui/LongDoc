{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MYOnCMh83ZRE"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from nltk import sent_tokenize\n",
        "from transformers import AutoTokenizer\n",
        "import sys\n",
        "import seaborn as sb\n",
        "sys.path.append('../..')\n",
        "from rank_bm25 import BM25Okapi\n",
        "from spacy.tokens import Span, Doc\n",
        "\n",
        "from src import *\n",
        "from src.test_utils import *\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "torch.backends.cuda.enable_flash_sdp(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# gritlm = GritLM(\"GritLM/GritLM-7B\", device_map=\"cuda:2\", torch_dtype=\"auto\")\n",
        "retriever = Retriever(device='cuda:2', syn_dist=0.1)\n",
        "doc_split = DocSplit(retriever.retriever_tokenizer)\n",
        "# llm = LLM()\n",
        "llm = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
        "# llm = None\n",
        "longdoc = LongDoc(retriever, llm)\n",
        "# dataset = NarrativeQADataset(llm)\n",
        "dataset = QualityDataset(llm, split='dev')\n",
        "# reading_agent = ReadingAgent(dataset, llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['What is the most likely meaning of the slang O.Q.? (in twentieth-century American English)',\n",
              " 'Why does the Skipper stop abruptly after he says \"when you\\'re running a blockade\"?',\n",
              " 'Who or what is Leo?',\n",
              " 'Why does the Skipper allow the new chef to use the heat-cannon as an incinerator?',\n",
              " ' Lieutenant Dugan brings up the examples of \"High G\" Gordon and \"Runt\" Hake in order to illustrates that...',\n",
              " \"Why didn't the Skipper follow the new cook's advice about avoiding Vesta?\",\n",
              " 'Why was the new cook so upset that the Skipper decided to surrender?',\n",
              " 'What does the Skipper mean by \"lady-logic\"?',\n",
              " \"What would've happened if the new cook had told the Skipper about the ekalastron deposits earlier?\"]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_i = 2\n",
        "sample = dataset.data[test_i]\n",
        "questions, answers = dataset.get_questions_and_answers(sample)\n",
        "article = dataset.get_article(sample)\n",
        "questions = [q.splitlines()[0] for q in questions]\n",
        "questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Index passages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "paragraphs = read_json(os.path.join(dataset.data_dir, f'pages_{2}.json'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qid = 5\n",
        "question = questions[qid]\n",
        "print(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pages = doc_split.split_paragraphs(article, 512 // 5)\n",
        "results, raw = longdoc.index_text_into_map(pages, 3)\n",
        "write_json('temp.json', [ci.to_json() for ci in results])\n",
        "write_json('raw.json', raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Navigation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "37\n",
            "8\n"
          ]
        }
      ],
      "source": [
        "pages = doc_split.split_paragraphs(article, 50)\n",
        "all_summary = longdoc.lossless_index(pages, 5, 5, 5, 'relation')\n",
        "write_json('all_summary.json', all_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_summary_pyramid(\n",
        "    longdoc:LongDoc, \n",
        "    summaries:List[str], \n",
        "    summary_chunk_num: int = 5, \n",
        "    prev_chunk_num: int = 5, \n",
        "    post_chunk_num:int = 5):\n",
        "    batched_summary_ranges = []\n",
        "    for batch_start in range(0, len(summaries), summary_chunk_num):\n",
        "        prev_start = max(batch_start - prev_chunk_num, 0)\n",
        "        batch_end = min(batch_start + summary_chunk_num, len(summaries))\n",
        "        post_end = min(batch_end + post_chunk_num, len(summaries))\n",
        "        chunk_start = batch_start - prev_start\n",
        "        chunk_end = batch_end - prev_start\n",
        "        batched_summary_ranges.append((prev_start, chunk_start, chunk_end, post_end))\n",
        "    for prev_start, chunk_start, chunk_end, post_end in batched_summary_ranges:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pages = doc_split.split_paragraphs(article, 50)\n",
        "all_summary = read_json('all_summary.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TextGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_unimportant(doc:Span, additional_pos_labels:Set[str]=set()):\n",
        "    spans = []\n",
        "    temp_span_start = 0\n",
        "    tid = 0\n",
        "    while tid < len(doc):\n",
        "        t = doc[tid]\n",
        "        if t.pos_ in {'DET', 'PRON', 'CCONJ', 'PUNCT', 'AUX', 'PART'} or t.pos_ in additional_pos_labels:\n",
        "            if temp_span_start != tid:\n",
        "                spans.append((temp_span_start, tid))\n",
        "            temp_span_start = tid + 1\n",
        "        tid += 1\n",
        "    if temp_span_start < tid:\n",
        "        spans.append((temp_span_start, tid))\n",
        "    splitted_doc = [doc[span[0]:span[1]] for span in spans]\n",
        "    return splitted_doc\n",
        "\n",
        "def collect_keywords_from_text(doc:Doc):\n",
        "    ncs = list(doc.noun_chunks)\n",
        "    ents = doc.ents\n",
        "    nc_id, eid = 0, 0\n",
        "    spans:List[Span] = []\n",
        "    # Merge noun chunks with entities\n",
        "    while nc_id < len(ncs) and eid < len(ents):\n",
        "        nc, ent = ncs[nc_id], ents[eid]\n",
        "        if set(range(nc.start, nc.end)).intersection(range(ent.start, ent.end)):\n",
        "            spans.append(doc[min(nc.start, ent.start) : max(nc.end, ent.end)])\n",
        "            nc_id += 1\n",
        "            eid += 1\n",
        "        else:\n",
        "            if nc.start < ent.end:\n",
        "                spans.append(nc)\n",
        "                nc_id += 1\n",
        "            else:\n",
        "                spans.append(ent)\n",
        "                eid += 1\n",
        "    spans.extend(ncs[nc_id:])\n",
        "    spans.extend(ents[eid:])\n",
        "    # Update each noun chunks\n",
        "    updated_spans:List[Span] = []\n",
        "    for span in spans:\n",
        "        updated_spans.extend(remove_unimportant(span, {'ADJ', 'ADV'}))\n",
        "    ent_candidates = {' '.join([t.lemma_ for t in span]) for span in updated_spans}\n",
        "    return ent_candidates\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextGraph:\n",
        "    def __init__(self, docs:List[Doc]) -> None:\n",
        "        self.text_graph = nx.DiGraph()\n",
        "        self.ent_graph = nx.Graph()\n",
        "        self.tokenized_corpus:List[List[str]] = []\n",
        "        ent_pair_counter = Counter()\n",
        "        for pid, doc in enumerate(docs):\n",
        "            tokenized_page = [t.lemma_.lower() for t in doc]\n",
        "            nouns = collect_keywords_from_text(doc)\n",
        "            if len(nouns) >= 2:\n",
        "                ent_pair_counter.update(map(frozenset, itertools.combinations(nouns, 2)))\n",
        "            self.tokenized_corpus.append(tokenized_page)\n",
        "            self.text_graph.add_node(pid, tokenized_page=tokenized_page, nouns=nouns)\n",
        "        for (ent1, ent2), cnt in ent_pair_counter.items():\n",
        "            self.ent_graph.add_edge(ent1, ent2, log_freq=np.log(cnt+1))\n",
        "        self.ent_general_importance:Dict[str, float] = nx.pagerank(self.ent_graph, weight='log_freq')\n",
        "        self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
        "        for pid1 in range(len(docs)):\n",
        "            bm25_scores = self.bm25.get_scores(self.tokenized_corpus[pid1])\n",
        "            bm25_scores = bm25_scores / bm25_scores.sum()\n",
        "            nouns1:Set[str] = self.text_graph.nodes[pid1]['nouns']\n",
        "            for pid2 in range(len(docs)):\n",
        "                if pid1 != pid2:\n",
        "                    overlap = nouns1.intersection(self.text_graph.nodes[pid2]['nouns'])\n",
        "                    if overlap:\n",
        "                        ent_importance = sum([self.ent_general_importance[ent] for ent in overlap])\n",
        "                        dist = 1 / np.log(np.e + np.abs(pid2 - pid1))\n",
        "                        bm25_score = bm25_scores[pid2]\n",
        "                        weight = statistics.harmonic_mean([ent_importance, bm25_score]) * dist\n",
        "                        self.text_graph.add_edge(pid1, pid2, overlap=overlap, ent_importance=ent_importance, dist=dist, bm25_score=bm25_score, weight=weight)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tg = TextGraph([longdoc.nlp(p) for p in all_summary])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list(tg.text_graph.edges.data())[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Topic Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim import corpora\n",
        "from gensim.parsing.preprocessing import preprocess_string, DEFAULT_FILTERS\n",
        "from gensim.models import Phrases, CoherenceModel, LdaModel, EnsembleLda, LdaMulticore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pages = doc_split.split_paragraphs(article, 500)\n",
        "all_summary = read_json('all_summary.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "longdoc.llm_server(f'''\n",
        "Summarize the following passage.\n",
        "\n",
        "Passage:\n",
        "{pages[1]}\n",
        "''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "longdoc.llm_server(f'''\n",
        "Summarize the following passage.\n",
        "\n",
        "Passage:\n",
        "{pages[2]}\n",
        "''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "longdoc.llm_server(f'''\n",
        "What are the common information in the following 2 passages.\n",
        "\n",
        "Passage 1:\n",
        "{pages[1]}\n",
        "\n",
        "Passage 2:\n",
        "{pages[2]}\n",
        "''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "longdoc.llm_server(f'''\n",
        "What are the different information between the following 2 passages.\n",
        "\n",
        "Passage 1:\n",
        "{pages[1]}\n",
        "\n",
        "Passage 2:\n",
        "{pages[2]}\n",
        "''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pages[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(all_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocess_funcs = DEFAULT_FILTERS[:-1] # Remove the stemming\n",
        "preprocessed_summary = [preprocess_string(' '.join([t.lemma_ for t in longdoc.nlp(p, disable=['parser', 'ner'])]), preprocess_funcs) for p in all_summary]\n",
        "\n",
        "# bigram = Phrases(preprocessed_summary, min_count=2, threshold=1)\n",
        "\n",
        "# texts = [bigram[p] for p in preprocessed_summary]\n",
        "texts = preprocessed_summary\n",
        "\n",
        "# Create a dictionary from the corpus\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "\n",
        "# Remove low-frequency terms from the dictionary\n",
        "dictionary.filter_extremes(no_below=2)\n",
        "\n",
        "# Convert the corpus into a bag-of-words representation\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lda_model = EnsembleLda(\n",
        "    corpus=corpus, \n",
        "    id2word=dictionary, \n",
        "    passes=5, \n",
        "    iterations=100, \n",
        "    num_models=5, \n",
        "    # min_cores=10, \n",
        "    # min_samples=4,\n",
        "    epsilon=0.05\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lda_model.print_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic2p = defaultdict(list)\n",
        "for pid, p in enumerate(corpus):\n",
        "    topic_id = sorted(lda_model[p], key=lambda x: x[1])[-1][0]\n",
        "    topic2p[topic_id].append(all_summary[pid])\n",
        "print(lda_model.stable_topics.shape)\n",
        "print([(tid, len(topic2p[tid])) for tid in range(len(topic2p))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic2p[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topics = []\n",
        "score = []\n",
        "topic_models:Dict[int, LdaModel] = {}\n",
        "min_docs_per_topic = 4\n",
        "for topic_num in tqdm(range(4, len(all_summary) // min_docs_per_topic, 4)):\n",
        "    # Build the LDA model\n",
        "    lda_model = LdaMulticore(corpus, topic_num, dictionary, iterations=100, passes=5, workers=5)\n",
        "    cm = CoherenceModel(lda_model, texts = texts, corpus=corpus, dictionary=dictionary, coherence='c_v')\n",
        "    topics.append(topic_num)\n",
        "    score.append(cm.get_coherence())\n",
        "    topic_models[topic_num] = lda_model\n",
        "    \n",
        "plt.plot(topics, score)\n",
        "plt.xlabel('Number of Topics')\n",
        "plt.ylabel('Coherence Score')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lda_model = topic_models[44]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list(lda_model.get_document_topics(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw = read_json('raw.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = [ChunkInfo(**ci) for ci in read_json('temp.json')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "queries = [\"the Skipper\", \"the new cook\", \"advice\", \n",
        "        #    \"avoiding\", \n",
        "           \"Vesta\",\n",
        "           \"the cook\",\n",
        "        #    \"avoiding Vesta\"\n",
        "           ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_map(results, queries, retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode pages\n",
        "pages = doc_split.split_paragraphs(article, 512 // 5)\n",
        "p_emb = retriever.embed_paragraphs(pages, normalize=True, complete_return=True)\n",
        "p_strs, p_lhs = [], []\n",
        "for pid in range(len(p_emb.embeddings)):\n",
        "    word_spans = sent_split(p_emb.input_ids[pid], retriever.retriever_tokenizer, retriever.retriever_tokenizer.bos_token, retriever.retriever_tokenizer.eos_token)\n",
        "    temp_p_strs, temp_p_lhs = merge_words_and_embeddings(retriever.retriever_tokenizer, p_emb.input_ids[pid], p_emb.last_hidden_states[pid], word_spans, False)\n",
        "    p_strs.append(temp_p_strs)\n",
        "    p_lhs.append(temp_p_lhs)\n",
        "tsne_plot(p_emb.embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "score_mat = q_emb.embeddings @ p_emb.embeddings.T\n",
        "fig, ax = plt.subplots(figsize=(score_mat.shape[1], score_mat.shape[0]))\n",
        "sb.heatmap(score_mat, xticklabels=range(len(pages)), yticklabels=queries, annot=True, ax=ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "score_mat = q_emb.embeddings @ p_emb.embeddings.T\n",
        "score_mat_min = score_mat.min(1, keepdims=True)\n",
        "score_mat_max = score_mat.max(1, keepdims=True)\n",
        "score_mat = (score_mat - score_mat_min) / (score_mat_max - score_mat_min)\n",
        "fig, ax = plt.subplots(figsize=(score_mat.shape[1], score_mat.shape[0]))\n",
        "sb.heatmap(score_mat, xticklabels=range(len(pages)), yticklabels=queries, annot=True, ax=ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "score_mat_max.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_input_ids(p_strs, range(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_input_ids(p_strs, [57])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_pages(pages, range(80, 85))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Question-page token-sent matching\n",
        "xid, yid = 1, 0\n",
        "x_start, x_end = 0, None\n",
        "y_start, y_end = 0, None\n",
        "\n",
        "score_mat = (q_lhs[yid] / np.expand_dims(np.linalg.norm(q_lhs[yid], axis=1), axis=1)) @ (p_lhs[xid] / np.expand_dims(np.linalg.norm(p_lhs[xid], axis=1), axis=1)).T\n",
        "score_mat = score_mat[y_start:y_end, x_start:x_end]\n",
        "fig, ax = plt.subplots(figsize=(score_mat.shape[1], score_mat.shape[0]))\n",
        "sb.heatmap(score_mat, xticklabels=p_strs[xid][x_start:x_end], yticklabels=q_strs[yid][y_start:y_end], annot=True, ax=ax)\n",
        "fig.savefig('qp.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Page-page matching\n",
        "score_mat = p_emb.embeddings @ p_emb.embeddings.T\n",
        "fig, ax = plt.subplots(figsize=(score_mat.shape[1], score_mat.shape[0]))\n",
        "sb.heatmap(score_mat, xticklabels=range(score_mat.shape[1]), yticklabels=range(score_mat.shape[0]), annot=True, ax=ax)\n",
        "fig.savefig('pp_all.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Page-page sent-sent matching\n",
        "xid, yid = 9, 10\n",
        "x_start, x_end = 0, None\n",
        "y_start, y_end = 0, None\n",
        "\n",
        "score_mat = (p_lhs[yid] / np.expand_dims(np.linalg.norm(p_lhs[yid], axis=1), axis=1)) @ (p_lhs[xid] / np.expand_dims(np.linalg.norm(p_lhs[xid], axis=1), axis=1)).T\n",
        "score_mat = score_mat[y_start:y_end, x_start:x_end]\n",
        "fig, ax = plt.subplots(figsize=(score_mat.shape[1]/2, score_mat.shape[0]/2))\n",
        "sb.heatmap(score_mat, xticklabels=range(x_start, score_mat.shape[1] + x_start), yticklabels=range(y_start, score_mat.shape[0] + y_start), annot=True, ax=ax)\n",
        "fig.savefig('pp.pdf')\n",
        "print('x:\\n', pages[xid])\n",
        "print('y:\\n', pages[yid])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_x_sent = 2\n",
        "test_y_sent = 2\n",
        "print(score_mat[test_y_sent, test_x_sent])\n",
        "print(p_strs[xid][test_x_sent])\n",
        "print(p_strs[yid][test_y_sent])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p_input_ids, pid2embs_3, pid2lhs_3 = slide_encode(pages, retriever, 3)\n",
        "# pid2embs_5 = slide_encode(pages, retriever, 5)\n",
        "p_input_ids, pid2embs_1, pid2lhs_1 = slide_encode(pages, retriever, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# p_weight_5 = np.array([[0., 0., 0.3, 0., 0.]])\n",
        "p_weight_3 = np.array([0., 0., 0.])\n",
        "p_weight_1 = np.array([1.0])\n",
        "# p_embeddings = np.vstack([(p_weight_5 @ embs_5)[0] + (p_weight_1 @ embs_1)[0] for embs_5, embs_1 in zip(pid2embs_5, pid2embs_1)])\n",
        "p_embeddings = np.vstack([(np.expand_dims(p_weight_3, 0) @ embs_3)[0] + (np.expand_dims(p_weight_1, 0) @ embs_1)[0] for embs_3, embs_1 in zip(pid2embs_3, pid2embs_1)])\n",
        "p_lhs = [(lhs_3 * np.expand_dims(p_weight_3, (1,2))).mean(0) + (lhs_1 * np.expand_dims(p_weight_1, (1,2))).mean(0) for lhs_3, lhs_1 in zip(pid2lhs_3, pid2lhs_1)]\n",
        "tsne_plot(p_embeddings, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# p_weight_5 = np.array([[0., 0., 0.3, 0., 0.]])\n",
        "p_weight_3 = np.array([0., 0.5, 0.])\n",
        "p_weight_1 = np.array([0.5])\n",
        "# p_embeddings = np.vstack([(p_weight_5 @ embs_5)[0] + (p_weight_1 @ embs_1)[0] for embs_5, embs_1 in zip(pid2embs_5, pid2embs_1)])\n",
        "p_embeddings = np.vstack([(np.expand_dims(p_weight_3, 0) @ embs_3)[0] + (np.expand_dims(p_weight_1, 0) @ embs_1)[0] for embs_3, embs_1 in zip(pid2embs_3, pid2embs_1)])\n",
        "p_lhs = [(lhs_3 * np.expand_dims(p_weight_3, (1,2))).mean(0) + (lhs_1 * np.expand_dims(p_weight_1, (1,2))).mean(0) for lhs_3, lhs_1 in zip(pid2lhs_3, pid2lhs_1)]\n",
        "tsne_plot(p_embeddings, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Page-page matching\n",
        "normalized_p_embeddings = p_embeddings / np.expand_dims(np.linalg.norm(p_embeddings, axis=1), 1)\n",
        "score_mat = normalized_p_embeddings @ normalized_p_embeddings.T\n",
        "fig, ax = plt.subplots(figsize=(score_mat.shape[1], score_mat.shape[0]))\n",
        "sb.heatmap(score_mat, xticklabels=range(score_mat.shape[1]), yticklabels=range(score_mat.shape[0]), annot=True, ax=ax)\n",
        "fig.savefig('pp_all.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "score_mat[60, 20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_pages(pages, range(60, 70))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Page-page sent-sent matching\n",
        "xid, yid = 60, 20\n",
        "\n",
        "x_word_spans = sent_split(p_input_ids[xid], retriever.retriever_tokenizer)\n",
        "y_word_spans = sent_split(p_input_ids[yid], retriever.retriever_tokenizer)\n",
        "plot_score_matrix(retriever.retriever_tokenizer, p_input_ids[xid], p_lhs[xid], x_word_spans, p_input_ids[yid], p_lhs[yid], y_word_spans, False, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def norm(x):\n",
        "    return x / np.linalg.norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "norm(p_lhs[yid][0:21].mean(0)).dot(norm(p_lhs[xid][33:77].mean(0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Page-page sent-sent matching\n",
        "xid, yid = 60, 20\n",
        "# x_start, x_end = 33, 77\n",
        "# y_start, y_end = 0, 21\n",
        "\n",
        "x_start, x_end = 0, None\n",
        "y_start, y_end = 0, None\n",
        "\n",
        "x_word_spans = word_split(p_input_ids[xid], retriever.retriever_tokenizer)\n",
        "x_strs, x_lhs = merge_words_and_embeddings(retriever.retriever_tokenizer, p_input_ids[xid], p_lhs[xid], [], False, True)\n",
        "y_word_spans = word_split(p_input_ids[yid], retriever.retriever_tokenizer)\n",
        "y_strs, y_lhs = merge_words_and_embeddings(retriever.retriever_tokenizer, p_input_ids[yid], p_lhs[yid], [], False, True)\n",
        "\n",
        "score_mat = (y_lhs) @ (x_lhs).T\n",
        "score_mat = score_mat[y_start:y_end, x_start:x_end]\n",
        "fig, ax = plt.subplots(figsize=(score_mat.shape[1], score_mat.shape[0]))\n",
        "sb.heatmap(score_mat, xticklabels=x_strs[x_start:score_mat.shape[1] + x_start], yticklabels=y_strs[y_start:score_mat.shape[0] + y_start], annot=True, ax=ax)\n",
        "fig.savefig('pp.pdf')\n",
        "print('x:\\n', x_strs)\n",
        "print('y:\\n', y_strs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p_lhs = [np.array(pid2embs[pid]).mean(0) for pid in range(len(pid2embs))]\n",
        "p_embeddings = np.array([lhs.mean(0) for lhs in p_lhs])\n",
        "p_norm = np.linalg.norm(p_embeddings, axis=1)\n",
        "p_embeddings = p_embeddings / np.expand_dims(p_norm, 1)\n",
        "p_lhs = [lhs / n for lhs, n in zip(p_lhs, p_norm)]\n",
        "pids, scores = retriever.dense_retrieval(q_emb.embeddings, p_embeddings, None, normalize=False, return_score=True)\n",
        "pids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query_distribution(retriever.retriever_tokenizer, q_emb.last_hidden_states[0], q_emb.input_ids[0], p_lhs, 5, q_spans=word_spans[3:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query_indicatiors(retriever.retriever_tokenizer, question, [f'passage: {p}' for p in pages], q_emb.last_hidden_states[0], q_emb.input_ids[0], p_lhs, p_input_ids, pids[:10], scores, 5, q_spans=word_spans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p_emb = retriever.embed_paragraphs([f'passage: {p}' for p in pages], normalize=True, complete_return=True)\n",
        "pids, scores = retriever.dense_retrieval(q_emb.embeddings, p_emb.embeddings, None, normalize=False, return_score=True)\n",
        "pids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query_indicatiors(retriever.retriever_tokenizer, question, [f'passage: {p}' for p in pages], q_emb.last_hidden_states[0], q_emb.input_ids[0], p_emb.last_hidden_states, p_emb.input_ids, pids, scores, q_spans=word_spans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib widget\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib.widgets import Cursor\n",
        "\n",
        "# Fixing random state for reproducibility\n",
        "np.random.seed(19680801)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "x, y = 4*(np.random.rand(2, 100) - .5)\n",
        "ax.plot(x, y, 'o')\n",
        "ax.set_xlim(-2, 2)\n",
        "ax.set_ylim(-2, 2)\n",
        "\n",
        "# Set useblit=True on most backends for enhanced performance.\n",
        "cursor = Cursor(ax, useblit=True, color='red', linewidth=2)\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
