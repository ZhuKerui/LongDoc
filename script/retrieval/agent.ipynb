{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import things that are needed generically\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool, StructuredTool, tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchNextStep(BaseModel):\n",
    "    summary_level: str = Field(description='the summary level under which the sub-tree will be explored.', default='summary_0')\n",
    "    query: str = Field(description='a query for the information you expect to find in the sub-tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional, Type\n",
    "\n",
    "\n",
    "    \n",
    "class SummaryTree(BaseTool):\n",
    "    name = 'branch retrieval'\n",
    "    description = ' '.join('''\n",
    "        This tool organizes the document in a summary tree. \n",
    "        The leaf nodes are the chunks from the document and the non-leaf nodes are the summaries of their children. \n",
    "        Higher-level nodes contain more general but less reliable information. \n",
    "        In the initial call, \n",
    "        Given a query, if  and a summary level, the tool will return the relevant chunk and all its ancestors as a branch in the summary tree. provide the multi-granularity context. \n",
    "        This context is useful in connecting the current relevant node with the remaining parts in the document.\n",
    "    '''.split())\n",
    "    args_schema: Type[BaseModel] = SearchNextStep\n",
    "    return_direct: bool = False\n",
    "    \n",
    "    def __init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import TreeIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter, SemanticSplitterNodeParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SemanticSplitterNodeParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NavigateAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import sys\n",
    "import seaborn as sb\n",
    "sys.path.append('../..')\n",
    "\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"EMPTY\"\n",
    "\n",
    "\n",
    "from src import *\n",
    "from src.test_utils import *\n",
    "from src.summary_tree import *\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "article = open('article.txt').read()\n",
    "question = \"Why didn't the skipper follow the new cook's advice about avoiding Vesta?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NavigateState(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    queries: List[str] = []\n",
    "    retriever: Literal['dpr', 'tree'] = 'dpr'\n",
    "    retrieve_root: int = -1\n",
    "    # generation: str\n",
    "    documents: List[Document] = []\n",
    "    aggretate_info: List[str] = []\n",
    "    retrieve_cnts: int = 0\n",
    "    answer:str = ''\n",
    "\n",
    "class NavigateAgent:\n",
    "    class Nodes:\n",
    "        GENERATE_QUERY = 'generate_query'\n",
    "        RETRIEVE_DOC = 'retrieve_doc'\n",
    "        GRADE_DOC = 'grade_doc'\n",
    "        ANALYZE_DOC = 'analyze_doc'\n",
    "        GENERATE_ANSWER = 'generate_answer'\n",
    "    \n",
    "    def __init__(self, llm:BaseChatModel, retrievers:Dict[str, MyStructure], max_retrieve_turn:int=5) -> None:\n",
    "        self.llm = llm\n",
    "        self.retrievers = retrievers\n",
    "        self.generate_query_chain = GenerateQuery(self.llm)\n",
    "        self.grade_doc_chain = GradeDocument(self.llm)\n",
    "        self.analyze_doc_chain = AnalyzeDocument(self.llm)\n",
    "        self.eval_complete_info_chain = EvalCompleteInfo(self.llm)\n",
    "        self.generate_answer_chain = GenerateAnswer(self.llm)\n",
    "        self.max_retrieve_turn = max_retrieve_turn\n",
    "    \n",
    "    def generate_query(self, state:NavigateState):\n",
    "        print('generate_query')\n",
    "        if state.aggretate_info:\n",
    "            result = self.generate_query_chain(questions=[state.question], contexts=['\\n'.join(state.aggretate_info)])[0]\n",
    "        else:\n",
    "            result = self.generate_query_chain(questions=[state.question])[0]\n",
    "        state.queries = result.queries\n",
    "        return state\n",
    "    \n",
    "    def retrieve_doc(self, state:NavigateState):\n",
    "        print('retrieve_doc')\n",
    "        retriever = self.retrievers[state.retriever]\n",
    "        state.documents = []\n",
    "        document_ids = set()\n",
    "        if state.retriever == 'dpr':\n",
    "            for query in state.queries:\n",
    "                for doc in retriever.vectorstore.similarity_search(query):\n",
    "                    if doc.metadata['i'] not in document_ids:\n",
    "                        document_ids.add(doc.metadata['i'])\n",
    "                        state.documents.append(doc)\n",
    "        state.retrieve_cnts += 1\n",
    "        return state\n",
    "    \n",
    "    def grade_doc(self, state:NavigateState):\n",
    "        print('grade_doc')\n",
    "        documents = [doc.page_content for doc in state.documents]\n",
    "        questions = [state.question] * len(documents)\n",
    "        if state.aggretate_info:\n",
    "            context = '\\n'.join(state.aggretate_info)\n",
    "            contexts = [context] * len(documents)\n",
    "            batch_results = self.grade_doc_chain(documents=documents, questions=questions, contexts=contexts)\n",
    "        else:\n",
    "            batch_results = self.grade_doc_chain(documents=documents, questions=questions)\n",
    "        state.documents = [doc for doc, result in zip(state.documents, batch_results) if 'yes' in result.binary_score.lower()]\n",
    "        return state\n",
    "    \n",
    "    def check_non_empty_retrieval(self, state:NavigateState):\n",
    "        print('check_non_empty_retrieval')\n",
    "        return 'Not empty' if state.documents else 'Empty'\n",
    "    \n",
    "    def analyze_doc(self, state:NavigateState):\n",
    "        print('analyze_doc')\n",
    "        documents = [doc.page_content for doc in state.documents]\n",
    "        questions = [state.question] * len(documents)\n",
    "        if state.aggretate_info:\n",
    "            context = '\\n'.join(state.aggretate_info)\n",
    "            contexts = [context] * len(documents)\n",
    "            batch_results = self.analyze_doc_chain(documents=documents, questions=questions, contexts=contexts)\n",
    "        else:\n",
    "            batch_results = self.analyze_doc_chain(documents=documents, questions=questions)\n",
    "        state.aggretate_info.extend(batch_results)\n",
    "        return state\n",
    "    \n",
    "    def eval_complete_info(self, state:NavigateState):\n",
    "        print('eval_complete_info')\n",
    "        result = self.eval_complete_info_chain(contexts=['\\n'.join(state.aggretate_info)], questions=[state.question])[0]\n",
    "        if 'yes' in result.binary_score.lower() or state.retrieve_cnts >= self.max_retrieve_turn:\n",
    "            return 'generate_answer'\n",
    "        else:\n",
    "            return 'update_query'\n",
    "    \n",
    "    def generate_answer(self, state:NavigateState):\n",
    "        print('generate_answer')\n",
    "        state.answer = self.generate_answer_chain(contexts=['\\n'.join(state.aggretate_info)], questions=[state.question])[0]\n",
    "        return state\n",
    "    \n",
    "    def create_workflow(self):\n",
    "        workflow = StateGraph(NavigateState)\n",
    "        for attr_name, attr_value in vars(self.Nodes).items():\n",
    "            if not attr_name.startswith('_'):\n",
    "                workflow.add_node(attr_value, getattr(self, attr_value))\n",
    "        \n",
    "        workflow.set_entry_point(self.Nodes.GENERATE_QUERY)\n",
    "        workflow.add_edge(self.Nodes.GENERATE_QUERY, self.Nodes.RETRIEVE_DOC)\n",
    "        workflow.add_edge(self.Nodes.RETRIEVE_DOC, self.Nodes.GRADE_DOC)\n",
    "        workflow.add_conditional_edges(\n",
    "            self.Nodes.GRADE_DOC,\n",
    "            self.check_non_empty_retrieval,\n",
    "            {\n",
    "                'Not empty': self.Nodes.ANALYZE_DOC,\n",
    "                'Empty': self.Nodes.GENERATE_QUERY,\n",
    "            },\n",
    "        )\n",
    "        workflow.add_conditional_edges(\n",
    "            self.Nodes.ANALYZE_DOC,\n",
    "            self.eval_complete_info,\n",
    "            {\n",
    "                \"update_query\": self.Nodes.GENERATE_QUERY,\n",
    "                \"generate_answer\": self.Nodes.GENERATE_ANSWER,\n",
    "            },\n",
    "        )\n",
    "        workflow.add_edge(self.Nodes.GENERATE_ANSWER, END)\n",
    "        app = workflow.compile()\n",
    "        \n",
    "        return app\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Factory()\n",
    "retrievers = f.build_corpus(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = NavigateAgent(f.llm, retrievers)\n",
    "app = agent.create_workflow()\n",
    "app.invoke(NavigateState(question=question))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longdoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
