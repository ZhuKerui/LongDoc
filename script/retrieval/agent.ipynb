{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import things that are needed generically\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool, StructuredTool, tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchNextStep(BaseModel):\n",
    "    summary_level: str = Field(description='the summary level under which the sub-tree will be explored.', default='summary_0')\n",
    "    query: str = Field(description='a query for the information you expect to find in the sub-tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional, Type\n",
    "\n",
    "\n",
    "    \n",
    "class SummaryTree(BaseTool):\n",
    "    name = 'branch retrieval'\n",
    "    description = ' '.join('''\n",
    "        This tool organizes the document in a summary tree. \n",
    "        The leaf nodes are the chunks from the document and the non-leaf nodes are the summaries of their children. \n",
    "        Higher-level nodes contain more general but less reliable information. \n",
    "        In the initial call, \n",
    "        Given a query, if  and a summary level, the tool will return the relevant chunk and all its ancestors as a branch in the summary tree. provide the multi-granularity context. \n",
    "        This context is useful in connecting the current relevant node with the remaining parts in the document.\n",
    "    '''.split())\n",
    "    args_schema: Type[BaseModel] = SearchNextStep\n",
    "    return_direct: bool = False\n",
    "    \n",
    "    def __init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import TreeIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter, SemanticSplitterNodeParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SemanticSplitterNodeParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NavigateAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from src.summary_tree import *\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from langsmith import Client\n",
    "from langsmith.schemas import Run\n",
    "from uuid import UUID\n",
    "import pickle\n",
    "from wikipediaapi import Wikipedia\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Factory(llm_name='microsoft/Phi-3-mini-128k-instruct')\n",
    "wiki_wiki = Wikipedia('MyProjectName (merlin@example.com)', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_py = wiki_wiki.page('Python_(programming_language)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages:List[str] = []\n",
    "for sec_text in page_py.sections:\n",
    "    pages.extend(f.split_text(sec_text.full_text()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_pages_text = '\\n\\n'.join([f'Passage {i}: {\" \".join(s.split())}' for i, s in enumerate(pages)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = '{context}\\n\\n\\n\\nAbove are the passages from a document in their original sequential order. Please suggest passages for the task below. To suggest passages, only return the passage ids like \"Passage 1\" or \"Passage 2 and 3\" and a very brief description about the task-relevant information in each passage as your reason for suggestion.\\n\\nTask: {task}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = f.llm.invoke([HumanMessage(content=test_prompt.format(context=labeled_pages_text, task='Introduce the python syntax and semantics.'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labeled_pages_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpr_corpus = MyDPR.build_dpr(pages)\n",
    "dpr_corpus.create_vector_retriever(f.embeder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpr_corpus.vectorstore.similarity_search(\"In what ways does Python aim for simplicity in its design?\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpr_corpus.vectorstore.similarity_search(\"Introduce the python design philosophy.\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate queries for the following task to retrieve relevant information from a document.\n",
    "\n",
    "# Task: Introduce the python design philosophy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What are the key principles of Python's design philosophy?\", \n",
    "    \"How does Python's design philosophy influence its syntax and grammar?\", \n",
    "    \"What is meant by Python's 'one obvious way to do it' philosophy?\", \n",
    "    \"In what ways does Python aim for simplicity in its design?\"\n",
    "]\n",
    "retrieval = []\n",
    "for q in queries:\n",
    "    retrieval.extend(dpr_corpus.vectorstore.similarity_search_with_relevance_scores(q))\n",
    "retrieval.sort(key=lambda x: x[1], reverse=True)\n",
    "uni_retrieval = []\n",
    "for doc, score in retrieval:\n",
    "    if doc.page_content not in uni_retrieval:\n",
    "        uni_retrieval.append(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_retrieval[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpr_corpus.vectorstore.similarity_search('What languages are influenced by Python?', 10) # Incomplete retrieval and noisy information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.llm.invoke([HumanMessage(content='Hello')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "fineweb = load_dataset('HuggingFaceFW/fineweb', 'sample-10BT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = fineweb['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from urllib.parse import quote_plus\n",
    "import io\n",
    "import gzip\n",
    "\n",
    "# Please note: f-strings require Python 3.6+\n",
    "\n",
    "# The URL of the Common Crawl Index server\n",
    "CC_INDEX_SERVER = 'http://index.commoncrawl.org/'\n",
    "\n",
    "# The Common Crawl index you want to query\n",
    "INDEX_NAME = 'CC-MAIN-2013-20'      # Replace with the latest index name\n",
    "\n",
    "# The URL you want to look up in the Common Crawl index\n",
    "target_url = 'http://daytimeroyaltyonline.com/single/?p=8906650&t=8780053'  # Replace with your target URL\n",
    "\n",
    "# Function to search the Common Crawl Index\n",
    "def search_cc_index(url):\n",
    "    encoded_url = quote_plus(url)\n",
    "    index_url = f'{CC_INDEX_SERVER}{INDEX_NAME}-index?url={encoded_url}&output=json'\n",
    "    response = requests.get(index_url)\n",
    "    print(\"Response from CCI:\", response.text)  # Output the response from the server\n",
    "    if response.status_code == 200:\n",
    "        records = response.text.strip().split('\\n')\n",
    "        return [json.loads(record) for record in records]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to fetch the content from Common Crawl\n",
    "def fetch_page_from_cc(records):\n",
    "    for record in records:\n",
    "        offset, length = int(record['offset']), int(record['length'])\n",
    "        prefix = record['filename'].split('/')[0]\n",
    "        s3_url = f'https://data.commoncrawl.org/{record[\"filename\"]}'\n",
    "        response = requests.get(s3_url, headers={'Range': f'bytes={offset}-{offset+length-1}'})\n",
    "        if response.status_code == 206:\n",
    "            # Process the response content if necessary\n",
    "            # For example, you can use warcio to parse the WARC record\n",
    "            zipped_file = io.BytesIO(response.content)\n",
    "            unzipped_file = gzip.GzipFile(fileobj=zipped_file)\n",
    "\n",
    "            raw_data: bytes = unzipped_file.read()\n",
    "            return raw_data.decode('utf-8')\n",
    "        else:\n",
    "            print(f\"Failed to fetch data: {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "# Search the index for the target URL\n",
    "records = search_cc_index(target_url)\n",
    "if records:\n",
    "    print(f\"Found {len(records)} records for {target_url}\")\n",
    "\n",
    "    # Fetch the page content from the first record\n",
    "    content = fetch_page_from_cc(records)\n",
    "    if content:\n",
    "        print(f\"Successfully fetched content for {target_url}\")\n",
    "        # You can now process the 'content' variable as needed\n",
    "else:\n",
    "    print(f\"No records found for {target_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(content.strip().split('\\r\\n\\r\\n', 2)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = chardet.universaldetector.UniversalDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.feed(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comcrawl import IndexClient\n",
    "\n",
    "client = IndexClient()\n",
    "\n",
    "client.search('http://daytimeroyaltyonline.com/single/?p=8906650&t=8780053')\n",
    "client.download()\n",
    "\n",
    "first_page_html = client.results[0][\"html\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_map = defaultdict(list)\n",
    "trace2runs: Dict[UUID, Dict[int, List[Run]]] = {}\n",
    "for project in tqdm(client.list_projects(), total=38):\n",
    "    traces = list(client.list_runs(project_name=project.name, is_root=True))\n",
    "    trace_ids = [t.trace_id for t in traces]\n",
    "    if 'tree' in project.name:\n",
    "        project_map['tree'].extend(trace_ids)\n",
    "    elif 'dpr' in project.name:\n",
    "        project_map['dpr'].extend(trace_ids)\n",
    "    for trace in traces:\n",
    "        runs = [d for d in client.list_runs(run_ids=trace.child_run_ids) if 'langgraph_node' in d.extra['metadata']][::-1]\n",
    "        step2runs = defaultdict(list)\n",
    "        for run in runs:\n",
    "            step2runs[run.extra['metadata']['langgraph_step']].append({'metadata': run.extra['metadata'], 'inputs': run.inputs, 'outputs': run.outputs})\n",
    "        trace2runs[trace.trace_id] = step2runs\n",
    "\n",
    "with open('result.pickle', 'wb') as f_out:\n",
    "    pickle.dump(trace2runs, f_out)\n",
    "    \n",
    "with open('project_map.pickle', 'wb') as f_out:\n",
    "    pickle.dump(project_map, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result.pickle', 'rb') as f_in:\n",
    "    trace2runs = pickle.load(f_in)\n",
    "    \n",
    "with open('project_map.pickle', 'rb') as f_in:\n",
    "    project_map = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_steps(step2runs:Dict[int, Any], node:str):\n",
    "    return [step for step, runs in step2runs.items() if runs and runs[0]['metadata']['langgraph_node'] == node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_keys = [trace_key for trace_key, step2runs in trace2runs.items() if get_steps(step2runs, NavigateAgent.Nodes.REFORM_QUERY) and get_steps(step2runs, NavigateAgent.Nodes.GENERATE_ANSWER)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trace_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = defaultdict(list)\n",
    "proposes = defaultdict(list)\n",
    "for trace_key in trace_keys:\n",
    "    step2runs = trace2runs[trace_key]\n",
    "    answer_steps = get_steps(step2runs, NavigateAgent.Nodes.GENERATE_ANSWER)\n",
    "    reform_steps = get_steps(step2runs, NavigateAgent.Nodes.REFORM_QUERY)\n",
    "    propose_num = 0\n",
    "    accept_num = 0\n",
    "    temp_proposes = []\n",
    "    for s in reform_steps:\n",
    "        propose_num += len(step2runs[s+1][0]['outputs']['output']['new_document_ids'])\n",
    "        if len(step2runs[s+1][0]['outputs']['output']['new_document_ids']):\n",
    "            temp_proposes.append(len(step2runs[s+1][0]['outputs']['output']['new_document_ids']))\n",
    "        accept_num += len(step2runs[s+2][0]['outputs']['output']['new_document_ids'])\n",
    "        if len(step2runs[s+2][0]['outputs']['output']['new_document_ids']) == 0:\n",
    "            break\n",
    "    \n",
    "    if propose_num > 0:\n",
    "        if trace_key in project_map['dpr']:\n",
    "            scores['dpr'].append(accept_num * 1. / propose_num)\n",
    "            proposes['dpr'].extend(temp_proposes)\n",
    "        if trace_key in project_map['tree']:\n",
    "            scores['tree'].append(accept_num * 1. / propose_num)\n",
    "            proposes['tree'].extend(temp_proposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scores['dpr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(proposes['dpr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scores['dpr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scores['tree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(proposes['tree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scores['tree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "propose_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = [run['outputs']['output']['score'] for grade_step in grade_steps for run in step2runs[grade_step] if 'output' in run['outputs'] and 'score' in run['outputs']['output']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[run['outputs'] for run in step2runs[grade_steps[1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[run['outputs'] for run in step2runs[retrieve_steps[1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_steps(step2runs, NavigateAgent.Nodes.REFORM_QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step2runs[4][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_steps(step2runs, NavigateAgent.Nodes.GENERATE_ANSWER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[run.inputs for run in step2runs[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step2runs[6][0].extra['metadata']['langgraph_node']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 4\n",
    "print(step2runs[step][0].extra['metadata']['langgraph_node'])\n",
    "print(step2runs[step][0].inputs['input'])\n",
    "print(step2runs[step][0].outputs['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"EMPTY\"\n",
    "f = Factory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = QualityDataset(None, split='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = 19\n",
    "article = dataset.get_article(dataset.data[test_id])\n",
    "questions, answers = dataset.get_questions_and_answers(dataset.data[test_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpr_retriever, tree_retriever, documents = f.build_corpus(article, dpr_file=os.path.join(dataset.data_dir, f'dpr_{test_id}.json'), tree_file=os.path.join(dataset.data_dir, f'tree_{test_id}.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_retriever.retrieve_children(tree_retriever.docs[14])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longdoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
