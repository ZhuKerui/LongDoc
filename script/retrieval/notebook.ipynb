{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from index_files import LongDoc, read_jsonline, read_json, write_json\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "longdoc = LongDoc(llm_name=\"mistralai/Mistral-7B-Instruct-v0.2\", device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_i = 0\n",
    "question_i = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LongBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    \"narrativeqa\", \n",
    "    # \"qasper\", \n",
    "    # \"multifieldqa_en\", \n",
    "    # \"multifieldqa_zh\", \n",
    "    # \"hotpotqa\", \n",
    "    # \"2wikimqa\", \n",
    "    # \"musique\", \n",
    "    # \"dureader\", \n",
    "    # \"gov_report\", \n",
    "    # \"qmsum\", \n",
    "    # \"multi_news\", \n",
    "    # \"vcsum\", \n",
    "    # \"trec\", \n",
    "    # \"triviaqa\", \n",
    "    # \"samsum\", \n",
    "    # \"lsht\", \n",
    "    # \"passage_count\", \n",
    "    # \"passage_retrieval_en\", \n",
    "    # \"passage_retrieval_zh\", \n",
    "    # \"lcc\", \n",
    "    # \"repobench-p\"\n",
    "]\n",
    "task_name = datasets[0]\n",
    "\n",
    "dataset_dict = {task_name: load_dataset('THUDM/LongBench', task_name, split='test')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = dataset_dict[task_name][task_i]['context']\n",
    "query = dataset_dict[task_name][task_i]['input']\n",
    "answer = dataset_dict[task_name][task_i]['answers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QuALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_qa = read_jsonline('../../data/QuALITY/QuALITY.v1.0.1.htmlstripped.train')\n",
    "task_name = 'quality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = quality_qa[task_i]['article']\n",
    "query = quality_qa[task_i]['questions'][question_i]['question']\n",
    "answer = quality_qa[task_i]['questions'][question_i]['options'][quality_qa[task_i]['questions'][question_i]['gold_label'] - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "for eid in range(10):\n",
    "    with open(f'quality/pages_{eid}.json') as f_in:\n",
    "        pages = json.load(f_in)\n",
    "        index_file = f'quality/response_{eid}.json'\n",
    "        write_json(index_file, longdoc.index_text(['\\n'.join(page) for page in pages]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coreference Resolution (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coref_batch = 4\n",
    "coref_resolver:fastcoref.spacy_component.FastCorefResolver = longdoc.nlp.get_pipe('fastcoref')\n",
    "coref_resolved_paragraphs = []\n",
    "for bid in tqdm(range((len(paragraphs[:10]) - 1) // coref_batch + 1)):\n",
    "    if bid == 0:\n",
    "        frozen_paragraphs = []\n",
    "        update_paragraphs = paragraphs[:coref_batch]\n",
    "    else:\n",
    "        frozen_paragraphs = coref_resolved_paragraphs[-1:]\n",
    "        update_paragraphs = paragraphs[bid * coref_batch : (bid + 1) * coref_batch]\n",
    "    \n",
    "    batch_paragraphs = frozen_paragraphs + update_paragraphs\n",
    "    doc = longdoc.nlp(''.join(batch_paragraphs))\n",
    "    prev_char_num = 0\n",
    "    paragraph_char_seps = []\n",
    "    for paragraph in batch_paragraphs:\n",
    "        paragraph_char_seps.append((prev_char_num, prev_char_num + len(paragraph)))\n",
    "        prev_char_num = paragraph_char_seps[-1][1]\n",
    "    \n",
    "    clusters:List[List[Tuple[int, int]]] = doc._.coref_clusters\n",
    "    \n",
    "    # Normalize the referred entities\n",
    "    idx2nc = {nc.root.i: nc for nc in doc.ents if '\\n\\n' not in nc.text}\n",
    "    new_clusters = []\n",
    "    for cluster in clusters:\n",
    "        indices = coref_resolver._get_span_noun_indices(doc, cluster)\n",
    "        if indices:\n",
    "            new_cluster = []\n",
    "            for sid, span in enumerate(cluster):\n",
    "                if sid in indices:\n",
    "                    doc_span = doc.char_span(span[0], span[1])\n",
    "                    if ',' in doc_span.text:\n",
    "                        root_i = doc_span.root.i\n",
    "                        if root_i in idx2nc:\n",
    "                            new_cluster.append((idx2nc[root_i].start_char, idx2nc[root_i].end_char))\n",
    "                        continue\n",
    "                new_cluster.append(span)\n",
    "            new_clusters.append(new_cluster)\n",
    "    clusters = new_clusters\n",
    "    \n",
    "    # Resolve part of prons\n",
    "    resolved = list(tok.text_with_ws for tok in doc)\n",
    "    all_spans = [span for cluster in clusters for span in cluster]\n",
    "    for cluster in clusters:\n",
    "        indices = coref_resolver._get_span_noun_indices(doc, cluster)\n",
    "        if indices and doc.char_span(cluster[indices[0]][0], cluster[indices[0]][1]).root.i in idx2nc:\n",
    "            mention_span, mention = coref_resolver._get_cluster_head(doc, cluster, indices)\n",
    "            marked = ([True] * len(frozen_paragraphs)) + ([False] * len(update_paragraphs))\n",
    "            pid = 0\n",
    "            for pid, (p_start, p_end) in enumerate(paragraph_char_seps):\n",
    "                if mention[0] >= p_start and mention[0] < p_end:\n",
    "                    marked[pid] = True\n",
    "                    break\n",
    "            for coref in cluster:\n",
    "                if coref != mention and not coref_resolver._is_containing_other_spans(coref, all_spans):\n",
    "                    while pid < len(marked) and marked[pid]:\n",
    "                        pid += 1\n",
    "                    if pid == len(marked):\n",
    "                        break\n",
    "                    if coref[0] >= paragraph_char_seps[pid][0] and coref[0] < paragraph_char_seps[pid][1]:\n",
    "                        marked[pid] = True\n",
    "                        coref_resolver._core_logic_part(doc, coref, resolved, mention_span)\n",
    "    coref_resolved_paragraphs.append(\"\".join(resolved)[sum([len(p) for p in frozen_paragraphs]):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(''.join(coref_resolved_paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(''.join(paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r_tool in ['dpr', 'index', 'gist']:\n",
    "    for task_i in range(10):\n",
    "        index_results = []\n",
    "        temp_result = {}\n",
    "        for line in read_jsonline(f'quality/response_{r_tool}_{task_i}_log.jsonl'):\n",
    "            temp_result[line[0]] = line[1]\n",
    "            if line[0] == 'current_summary':\n",
    "                index_results.append(temp_result)\n",
    "                temp_result = {}\n",
    "        results = []\n",
    "        for qid in range(len(quality_qa[task_i]['questions'])):\n",
    "            current_summary = index_results[qid]['current_summary']\n",
    "            query = quality_qa[task_i]['questions'][qid]['question']\n",
    "            options = '\\n'.join([f'{oid + 1}: {option}' for oid, option in enumerate(quality_qa[task_i]['questions'][qid]['options'])])\n",
    "            answer = quality_qa[task_i]['questions'][qid]['gold_label']\n",
    "            writer_answer = quality_qa[task_i]['questions'][qid]['writer_label']\n",
    "            prompt = f'''Answer the question based on a given summary.\\n\\n{current_summary}\\n\\nQuestion: {query}\\n{options}\\n\\nChoose the correct option above and return the option number. Generate your answer in the following format:\"Answer: the option number\".'''\n",
    "            gen = longdoc._call_llm(prompt).choices[0].message.content\n",
    "            fail_cnt = 0\n",
    "            while not gen.strip().lower().startswith('answer: ') or not gen.strip()[8].isnumeric():\n",
    "                fail_cnt += 1\n",
    "                if fail_cnt >= 5:\n",
    "                    break\n",
    "                gen = longdoc._call_llm(prompt).choices[0].message.content\n",
    "            results.append({'prompt': prompt, 'gen': gen, 'gold': answer, 'writer': writer_answer})\n",
    "            print(task_i, qid)\n",
    "        write_json(f'quality/generation_{r_tool}_{task_i}.json', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = {}\n",
    "results = {}\n",
    "retrieved_passages = {}\n",
    "for r_tool in ['index', 'dpr', 'gist']:\n",
    "    accuracy[r_tool] = []\n",
    "    results[r_tool] = []\n",
    "    retrieved_passages[r_tool] = []\n",
    "    for task_i in range(0, 10):\n",
    "        results[r_tool].extend(read_json(f'quality/generation_{r_tool}_{task_i}.json'))\n",
    "        retrieved_passages[r_tool].extend([(task_i, line[1]) for line in read_jsonline(f'quality/response_{r_tool}_{task_i}_log.jsonl') if line[0] == 'retrieval_result'])\n",
    "    for result in results[r_tool]:\n",
    "        try:\n",
    "            answer_start = result['gen'].lower().index('answer: ')\n",
    "            accuracy[r_tool].append(int(result['gen'][answer_start + 8]) == result['writer'])\n",
    "        except:\n",
    "            accuracy[r_tool].append(False)\n",
    "    print(r_tool, sum(accuracy[r_tool]) * 1. / len(accuracy[r_tool]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "for i in range(len(retrieved_passages['index'])):\n",
    "    if accuracy['index'][i] and not accuracy['dpr'][i]:\n",
    "        examples.append({'index': results['index'][i], 'gist': results['gist'][i], 'dpr': results['dpr'][i], 'task_id': retrieved_passages['index'][i][0], 'index_ret': retrieved_passages['index'][i][1], 'gist_ret': retrieved_passages['gist'][i][1], 'dpr_ret': retrieved_passages['dpr'][i][1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eid = 0\n",
    "print(f'''Task id: {examples[eid]['task_id']}\n",
    "\n",
    "Index result:\n",
    "\n",
    "{examples[eid]['index']['prompt']}\n",
    "\n",
    "{examples[eid]['index']['gen']}\n",
    "\n",
    "\n",
    "\n",
    "DPR result:\n",
    "\n",
    "{examples[eid]['dpr']['prompt']}\n",
    "\n",
    "{examples[eid]['dpr']['gen']}\n",
    "\n",
    "\n",
    "\n",
    "GIST result:\n",
    "\n",
    "{examples[eid]['gist']['prompt']}\n",
    "\n",
    "{examples[eid]['gist']['gen']}\n",
    "\n",
    "\n",
    "\n",
    "Index Retrieved:\n",
    "\n",
    "{examples[eid]['index_ret']}\n",
    "\n",
    "\n",
    "\n",
    "DPR Retrieved:\n",
    "\n",
    "{examples[eid]['dpr_ret']}\n",
    "\n",
    "\n",
    "\n",
    "GIST Retrieved:\n",
    "\n",
    "{examples[eid]['gist_ret']}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(quality_qa[3]['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(quality_qa[2]['questions'][1]['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_qa[2]['questions'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lid = 4\n",
    "print(f'''{index_results[lid]['menu']}\\n\\n\\n\\n{index_results[lid]['retrieval_command']}\\n\\n\\n\\n{index_results[lid]['retrieval_result']}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_results[lid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load index\n",
    "results = longdoc.main(query, f'{task_name}/response_{task_i}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: identity entities of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_entity_prompt = f'''Question: {query}\\nYou need to answer the above question based on a given story. Before reading the story, identify some entities and keywords in the question you want to query from the story for useful information. Don't give any explanation. Generate your response in the following format:\\n\"Query entities:\\nthe first entity, the second entity, ...\\n\\nQuery keywords:\\nthe first keyword, the second keyword, ...\".'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response = longdoc.llm_server.chat.completions.create(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": query_entity_prompt},\n",
    "    ]\n",
    ")\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities, keywords = longdoc.identify_entity_keyword(query)\n",
    "entities + keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_sets = longdoc.retrieve_node(doc_index.graph, entities + keywords, 10)\n",
    "# ent_sets = longdoc.retrieve_node(all_graph, ['the witch', \"the character's residence in the story\"])\n",
    "mention_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: retrieve summary/original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "menu, pair2sids = longdoc.retrieve_menu(mention_sets, doc_index)\n",
    "print(menu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: analyze retrieved info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: continue searching or start answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_prompt = f'''Question: {query}\\n\\nYou need to answer the above question based on a given story.\\nBelow is a list of related entities and entity pairs contained in each passage from the story. The passage numbers are assigned based on the original order of the passages in the text.\\n\\n'''\n",
    "decision_prompt += menu\n",
    "retrieved_passage_idx_str = ', '.join(map(str, passage_indices))\n",
    "decision_prompt += f'''Below is the summary of useful information from passage {retrieved_passage_idx_str}.\\n\\n'''\n",
    "decision_prompt += current_summary\n",
    "decision_prompt += '''Now, you need to choose whether to continue searching for more information or to start answering the question.\n",
    "\n",
    "If the information is not adequate, you may choose to continue searching. Select a retrieval type and the passage numbers. For the retrieval type, you may choose \"original text\" to retrieve the original passages, or \"summary\" to retrieve the summary of the entities in the passage. For passage selection, you may select passage numbers that do not exist in the above list to obtain continuous contextual information. You may retrieve either 5 passages for \"original text\" or 10 passages for \"summary\". Generate your response in the following format:\\n\"Retrieval type: summary/original text\\nPassage numbers: first passage number, second passage number, ...\".\n",
    "\n",
    "Otherwise, if the information is adequate, you may choose to start answering the question. Generate your answer to the question in the following format:\\n\"Answer: your answer here\".\n",
    "\n",
    "For either choice, don't give any explanation.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long context and multi-hop reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HotpotQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotpot_qa = load_dataset('hotpot_qa', 'distractor', split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotpot_qa[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NarrativeQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narrative_qa = load_dataset('narrativeqa', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narrative_qa[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QASPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qasper = load_dataset('allenai/qasper', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qasper[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QuALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_qa = [json.loads(l) for l in open('../../QuALITY.v1.0.1.train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_qa[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### openbookqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openbookqa = load_dataset('openbookqa', 'main', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openbookqa[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LongBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    # \"narrativeqa\", \n",
    "    # \"qasper\", \n",
    "    # \"multifieldqa_en\", \n",
    "    # \"multifieldqa_zh\", \n",
    "    # \"hotpotqa\", \n",
    "    # \"2wikimqa\", \n",
    "    \"musique\", \n",
    "    # \"dureader\", \n",
    "    # \"gov_report\", \n",
    "    # \"qmsum\", \n",
    "    # \"multi_news\", \n",
    "    # \"vcsum\", \n",
    "    # \"trec\", \n",
    "    # \"triviaqa\", \n",
    "    # \"samsum\", \n",
    "    # \"lsht\", \n",
    "    # \"passage_count\", \n",
    "    # \"passage_retrieval_en\", \n",
    "    # \"passage_retrieval_zh\", \n",
    "    # \"lcc\", \n",
    "    # \"repobench-p\"\n",
    "]\n",
    "task_name = datasets[0]\n",
    "\n",
    "dataset_dict = {task_name: load_dataset('THUDM/LongBench', task_name, split='test')}\n",
    "print(dataset_dict[task_name][1]['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict[task_name][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LooGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    # \"shortdep_qa\", \n",
    "    # \"shortdep_cloze\", \n",
    "    \"longdep_qa\", \n",
    "    # \"longdep_summarization\"\n",
    "]\n",
    "\n",
    "for testset in datasets:\n",
    "    data = load_dataset('bigainlco/LooGLE', testset, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
