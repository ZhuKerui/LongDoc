{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from src.index_files import *\n",
    "\n",
    "import seaborn as sb\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from wikipediaapi import Wikipedia\n",
    "\n",
    "dataset = QualityDataset(split='dev')\n",
    "wiki_wiki = Wikipedia('MyProjectName (merlin@example.com)', 'en')\n",
    "page_py = wiki_wiki.page('Python_(programming_language)')\n",
    "python_page = '\\n\\n'.join([sec_text.full_text() for sec_text in page_py.sections])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config, device_map=\"cuda:0\").eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = dataset.get_article(dataset.data[2])\n",
    "questions, answers = dataset.get_questions_and_answers(dataset.data[2])\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_article = tokenizer.encode(article, return_tensors='pt')[:, :3102]\n",
    "# chunked_article = tokenizer.encode(python_page, return_tensors='pt')[:, :3116]\n",
    "result = tokenizer.decode(chunked_article[0].tolist())\n",
    "print(len(result), result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(article[:10040])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''You are given a story and a question. Answer the question as concisely as you can, using a single phrase if possible. Do not provide any explanation.\n",
    "\n",
    "Story: {context}\n",
    "\n",
    "Now, answer the question based on the story as concisely as you can, using a single phrase if possible. Do not provide any explanation.\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Answer:'''\n",
    "\n",
    "text = template.format(context=article[:10040], input='Why does the Skipper stop abruptly after he says \"when you\\'re running a blockade\"?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # chunked_article = tokenizer.encode(text, return_tensors='pt').to(model.device)\n",
    "    chunked_article = tokenizer.encode(python_page[:13129], return_tensors='pt').to(model.device)\n",
    "    # chunked_article = model.generate(chunked_article, do_sample=True, stop_strings='\\n', tokenizer=tokenizer)\n",
    "    output = model(chunked_article, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = [a.squeeze(0).cpu() for a in output.attentions]\n",
    "avg_attn = [a.mean(0) for a in attn]\n",
    "avg_attn[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.concat([a.unsqueeze(0) for a in avg_attn]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(torch.concat([a.unsqueeze(0) for a in avg_attn]), 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.heatmap(torch.mean(torch.concat([a.unsqueeze(0) for a in avg_attn]), 0)[1:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "start_id, length = attn[0].shape[-1] - 1000, 1000\n",
    "for layer in tqdm(range(len(attn))):\n",
    "    axes = sb.heatmap(avg_attn[layer][start_id:start_id+length, start_id:start_id+length], cbar=False)\n",
    "    plt.title(f'layer {layer}')\n",
    "    plt.savefig(f'figs/{layer}.png')\n",
    "\n",
    "frame = cv2.imread(os.path.join('figs', '0.png')) \n",
    "\n",
    "# setting the frame width, height width \n",
    "# the width, height of first image \n",
    "height, width, layers = frame.shape   \n",
    "\n",
    "video = cv2.VideoWriter('video.avi', 0, 1, (width, height))  \n",
    "\n",
    "# Appending the images to the video one by one \n",
    "for image in [f'{l}.png' for l in range(len(attn))]:  \n",
    "    video.write(cv2.imread(os.path.join('figs', image)))  \n",
    "    \n",
    "# Deallocating memories taken for window creation \n",
    "cv2.destroyAllWindows()  \n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_id, length, layer = attn[0].shape[-1] - 1000, 50, -1\n",
    "# sb.heatmap(attn[-1][3][start_id:start_id+length, start_id:start_id+length])\n",
    "sb.heatmap(avg_attn[layer][start_id:start_id+length, start_id:start_id+length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_focus = defaultdict(list)\n",
    "used_tokens = defaultdict(set)\n",
    "for layer in tqdm(range(len(attn))):\n",
    "    for tid in range(3000, 3116):#attn[0].shape[-1]):\n",
    "        token_attn = avg_attn[layer][tid, :tid+1]\n",
    "        threshold = token_attn.mean() * 10\n",
    "        temp_used_tokens = np.arange(len(token_attn))[token_attn > threshold]\n",
    "        token_focus[tid].append(temp_used_tokens)\n",
    "        used_tokens[tid].update(temp_used_tokens.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_tokens[3173]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_token_id = 3173\n",
    "check_token_id = 3113\n",
    "used_tokens_mat = np.zeros((len(attn), len(used_tokens[check_token_id])))\n",
    "for layer in range(len(attn)):\n",
    "    for tid, temp_token_id in enumerate(sorted(used_tokens[check_token_id])):\n",
    "        if temp_token_id in [0, 2]:\n",
    "            continue\n",
    "        used_tokens_mat[layer, tid] = avg_attn[layer][check_token_id, temp_token_id]\n",
    "print(tokenizer.decode(chunked_article[0, check_token_id:check_token_id+20]))\n",
    "fig, ax = plt.subplots(figsize=(used_tokens_mat.shape[1]/2, used_tokens_mat.shape[0]/2))\n",
    "sb.heatmap(used_tokens_mat, xticklabels=sorted(used_tokens[check_token_id]))\n",
    "plt.xlabel('token id')\n",
    "plt.ylabel('layer id')\n",
    "plt.title('Attention distribution for \"expression\" at pos 3113 over important tokens')\n",
    "plt.savefig('attn.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_used_tokens = sorted(used_tokens[check_token_id])\n",
    "start_sent_id, end_sent_id = 0, 0\n",
    "for tid, temp_token_id in enumerate(sorted_used_tokens):\n",
    "    if tid == 0:\n",
    "        start_sent_id = temp_token_id\n",
    "    elif tid == len(sorted_used_tokens) - 1:\n",
    "        end_sent_id = temp_token_id\n",
    "        print(start_sent_id, '---', end_sent_id, ':', tokenizer.decode(chunked_article[0, start_sent_id:end_sent_id+1]))\n",
    "    else:\n",
    "        if temp_token_id - sorted_used_tokens[tid-1] <= 10:\n",
    "            end_sent_id = temp_token_id\n",
    "        else:\n",
    "            if end_sent_id == start_sent_id:\n",
    "                print(start_sent_id, ':', tokenizer.decode(chunked_article[0, start_sent_id:end_sent_id+1]))\n",
    "            else:\n",
    "                print(start_sent_id, '---', end_sent_id, ':', tokenizer.decode(chunked_article[0, start_sent_id:end_sent_id+1]))\n",
    "            start_sent_id = temp_token_id\n",
    "            end_sent_id = temp_token_id\n",
    "# for temp_token_id in sorted(used_tokens[check_token_id]):\n",
    "#     print(temp_token_id, tokenizer.decode(article_w_answer[0, temp_token_id]), '---', tokenizer.decode(article_w_answer[0, max(temp_token_id-1, 0):temp_token_id+5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_focus.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_focus[3221]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id = 12 + start_id\n",
    "print(token_id)\n",
    "# attn[layer][3][token_id:, token_id][:10]\n",
    "avg_attn[layer][token_id:, token_id][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_attn[layer][token_id, :token_id+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode(chunked_article[0, token_id])\n",
    "tokenizer.decode(chunked_article[0, 1876:3200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_token_id = 8 + start_id\n",
    "tokenizer.decode(article_w_answer[0, last_token_id:token_id+100].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(article_w_answer[0, token_id+10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_attn[layer][token_id+10, :token_id+11][-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(article_w_answer[0, token_id+10-25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(article_w_answer[0, 808:1200].tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longdoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
