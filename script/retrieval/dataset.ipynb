{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from src.index_files import *\n",
    "from src.corenlp_base import Doc, Mention, Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = QualityDataset(split='dev')\n",
    "# f = Factory(chunk_size=100, llm_name=None)\n",
    "# article = dataset.get_article(dataset.data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphrag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from rank_bm25 import BM25Okapi\n",
    "import spacy.tokens\n",
    "\n",
    "class ChunkInfo(BaseModel):\n",
    "    i: int\n",
    "    chunk_text: str\n",
    "    statements: List[str] = []\n",
    "    entities: List[List[str]] = []\n",
    "    ent_modifiers: list = []\n",
    "        \n",
    "\n",
    "class LongDoc:\n",
    "    \n",
    "    def __init__(self, factory:Factory, chunk_info_file:str=None) -> None:\n",
    "        self.factory = factory\n",
    "        self.nlp = spacy.load('en_core_web_lg')\n",
    "        if chunk_info_file:\n",
    "            self.chunk_infos = [ChunkInfo.parse_obj(line) for line in read_json(chunk_info_file)]\n",
    "            self.enrich_index()\n",
    "        \n",
    "    # Index functions\n",
    "    def build_index(self, article:str, chunk_info_file:str=None):\n",
    "        pieces = self.factory.split_text(article)\n",
    "        chunks, statements = self.generate_statements(pieces, chunk_size=3, summary_size=12)\n",
    "        self.chunk_infos = [ChunkInfo(i=i, chunk_text=chunk, statements=statements[i]) for i, chunk in enumerate(chunks)]\n",
    "        # missing_chunk_ids = [ci.i for ci in self.chunk_infos if not ci.statements]\n",
    "        # temp_stm_groups = []\n",
    "        # split_nums:List[int] = []\n",
    "        # for missing_ci in missing_chunk_ids:\n",
    "        #     temp_stms = statements[missing_ci]\n",
    "        #     split_num = (len(temp_stms) + 1) // 5\n",
    "        #     split_nums.append(split_num)\n",
    "        #     for split in range(split_num):\n",
    "        #         temp_stm_groups.append(temp_stms[split * 5 : (split + 1) * 5])\n",
    "        # entities = self.extract_entities(temp_stm_groups)\n",
    "        # start_stm_idx = 0\n",
    "        # for cid, split_num in enumerate(split_nums):\n",
    "        #     for sid in range(split_num):\n",
    "        #         self.chunk_infos[missing_chunk_ids[cid]].statements.extend(temp_stm_groups[start_stm_idx + sid])\n",
    "        #         if len(temp_stm_groups[start_stm_idx + sid]) == len(entities[start_stm_idx + sid]):\n",
    "        #             self.chunk_infos[missing_chunk_ids[cid]].entities.extend(entities[start_stm_idx + sid])\n",
    "        #         else:\n",
    "        #             self.chunk_infos[missing_chunk_ids[cid]].entities.extend([[] for _ in range(len(temp_stm_groups[start_stm_idx + sid]))])\n",
    "        #     start_stm_idx += split_num\n",
    "        # missing_chunk_ids = [ci.i for ci in self.chunk_infos if not ci.statements]\n",
    "\n",
    "        if chunk_info_file:\n",
    "            write_json(chunk_info_file, [ci.dict() for ci in self.chunk_infos])\n",
    "    \n",
    "    def enrich_index(self):\n",
    "        for ci in self.chunk_infos:\n",
    "            for statement in ci.statements:\n",
    "                addition_ents, ent_modifiers = self.collect_keywords_from_text(statement)\n",
    "                ent_map = {}\n",
    "                ci.entities.append([])\n",
    "                for addition_ent in addition_ents:\n",
    "                    # for ent in ci.entities[sid]:\n",
    "                    #     if addition_ent.lower() in ent.lower():\n",
    "                    #         ent_map[addition_ent] = ent\n",
    "                    # if addition_ent not in ent_map:\n",
    "                    ent_map[addition_ent] = addition_ent\n",
    "                    ci.entities[-1].append(addition_ent)\n",
    "                updated_ent_modifiers = []\n",
    "                for ent, modifiers in ent_modifiers:\n",
    "                    if isinstance(ent, str):\n",
    "                        updated_ent_modifiers.append(json.dumps((ent_map[ent], modifiers)))\n",
    "                    else:\n",
    "                        ent, modifiers = modifiers, ent\n",
    "                        updated_ent_modifiers.append(json.dumps((modifiers, ent_map[ent])))\n",
    "                ci.ent_modifiers.append([json.loads(s) for s in set(updated_ent_modifiers)])\n",
    "\n",
    "        self.build_ent_graph()\n",
    "        self.build_semantic_graph()\n",
    "        self.build_lexical_store()\n",
    "        \n",
    "    def build_ent_graph(self):\n",
    "        self.ent_graph = nx.Graph()\n",
    "        # semantic edges\n",
    "        for ci in self.chunk_infos:\n",
    "            for sid, related_ents in enumerate(ci.entities):\n",
    "                loc = (ci.i, sid)\n",
    "                # Insert node locs\n",
    "                for e in related_ents:\n",
    "                    if not self.ent_graph.has_node(e):\n",
    "                        self.ent_graph.add_node(e, locs=[], norm=' '.join(self.normalize_entity(e)))\n",
    "                    ent_locs:list = self.ent_graph.nodes[e]['locs']\n",
    "                    if loc not in ent_locs:\n",
    "                        ent_locs.insert(0, loc)\n",
    "                for ent1, ent2 in itertools.combinations(related_ents, 2):\n",
    "                    if not self.ent_graph.has_edge(ent1, ent2):\n",
    "                        self.ent_graph.add_edge(ent1, ent2, locs=[])\n",
    "                    edge_locs:list = self.ent_graph[ent1][ent2]['locs']\n",
    "                    edge_locs.append(loc)\n",
    "        \n",
    "        self.normal2ents:Dict[str, List[str]] = defaultdict(list)\n",
    "        for ent, normal in self.ent_graph.nodes(data='norm'):\n",
    "            self.normal2ents[normal].append(ent)\n",
    "        normals = list(self.normal2ents)\n",
    "        normals.sort()\n",
    "        self.ent_corpus = [ent.split() for ent in normals]\n",
    "        self.ent_bm25 = BM25Okapi(self.ent_corpus)\n",
    "        \n",
    "        for normal in self.normal2ents:\n",
    "            # Add edges between entities that have the same norm\n",
    "            for ent1, ent2 in itertools.combinations(self.normal2ents[normal], 2):\n",
    "                if not self.ent_graph.has_edge(ent1, ent2):\n",
    "                    self.ent_graph.add_edge(ent1, ent2, locs=[])\n",
    "                edge_locs:list = self.ent_graph[ent1][ent2]['locs']\n",
    "                edge_locs.append(None)\n",
    "            # Add edges between entities that have similar norms\n",
    "            scores:List[float] = self.ent_bm25.get_scores(normal.split()).tolist()\n",
    "            for score, similar_normal in zip(scores, self.ent_corpus):\n",
    "                if score > 0:\n",
    "                    similar_normal = ' '.join(similar_normal)\n",
    "                    if normal != similar_normal:\n",
    "                        for ent1, ent2 in itertools.product(self.normal2ents[normal], self.normal2ents[similar_normal]):\n",
    "                            ent1, ent2 = (ent1, ent2) if len(ent2) < len(ent1) else (ent2, ent1)\n",
    "                            if not self.ent_graph.has_edge(ent1, ent2):\n",
    "                                self.ent_graph.add_edge(ent1, ent2, locs=[])\n",
    "                            edge_locs:list = self.ent_graph[ent1][ent2]['locs']\n",
    "                            edge_locs.append(None)\n",
    "        \n",
    "        for _, _, edge_data in self.ent_graph.edges.data():\n",
    "            edge_data['weight'] = np.log(len([loc for loc in edge_data['locs'] if loc]) + 1)\n",
    "                            \n",
    "    def build_semantic_graph(self):\n",
    "        self.semantic_graph = nx.DiGraph()\n",
    "        ent1:str\n",
    "        ent2:str\n",
    "        for ci in self.chunk_infos:\n",
    "            for sid, entities in enumerate(ci.entities):\n",
    "                loc = (ci.i, sid)\n",
    "                for ent1, ent2 in itertools.combinations(entities, 2):\n",
    "                    ent1, ent2 = f'{ent1}_{ci.i}', f'{ent2}_{ci.i}'\n",
    "                    if not self.semantic_graph.has_edge(ent1, ent2):\n",
    "                        self.semantic_graph.add_edge(ent1, ent2, locs=[])\n",
    "                    edge_locs:List[Tuple[int, int]] = self.semantic_graph[ent1][ent2]['locs']\n",
    "                    edge_locs.append(loc)\n",
    "        \n",
    "        for ent, locs in self.ent_graph.nodes(data='locs'):\n",
    "            chunk_ids:Set[int] = {loc[0] for loc in locs}\n",
    "            for cid1, cid2 in itertools.combinations(chunk_ids, 2):\n",
    "                cid1, cid2 = sorted([cid1, cid2])\n",
    "                ent1, ent2 = f'{ent}_{cid1}', f'{ent}_{cid2}'\n",
    "                self.semantic_graph.add_edge(ent1, ent2, locs=[None])\n",
    "        for ent1, ent2, edge_locs in self.ent_graph.edges.data('locs'):\n",
    "            if None in edge_locs:\n",
    "                for ent1_loc, ent2_loc in itertools.product(self.ent_graph.nodes[ent1]['locs'], self.ent_graph.nodes[ent2]['locs']):\n",
    "                    if ent1_loc == ent2_loc:\n",
    "                        continue\n",
    "                    temp_ent1, temp_ent2 = f'{ent1}_{ent1_loc[0]}', f'{ent2}_{ent2_loc[0]}'\n",
    "                    # Entities from different chunks, must be similar entities\n",
    "                    if ent1_loc[0] < ent2_loc[0]:\n",
    "                        self.semantic_graph.add_edge(temp_ent1, temp_ent2, locs=[None])\n",
    "                    elif ent1_loc[0] > ent2_loc[0]:\n",
    "                        self.semantic_graph.add_edge(temp_ent2, temp_ent1, locs=[None])\n",
    "                    else:\n",
    "                        # Entities from same chunks, but are similar entities\n",
    "                        if ent1_loc[1] < ent2_loc[1]:\n",
    "                            if not self.semantic_graph.has_edge(temp_ent1, temp_ent2):\n",
    "                                self.semantic_graph.add_edge(temp_ent1, temp_ent2, locs=[])\n",
    "                            self.semantic_graph[temp_ent1][temp_ent2]['locs'].append(None)\n",
    "                        elif ent1_loc[1] > ent2_loc[1]:\n",
    "                            if not self.semantic_graph.has_edge(temp_ent2, temp_ent1):\n",
    "                                self.semantic_graph.add_edge(temp_ent2, temp_ent1, locs=[])\n",
    "                            self.semantic_graph[temp_ent2][temp_ent1]['locs'].append(None)\n",
    "        \n",
    "        for ent1, ent2, edge_data in self.semantic_graph.edges.data():\n",
    "            # edge_data['explore_weight'] = 0 if ent1.rsplit('_', 1)[1] != ent2.rsplit('_', 1)[1] else 1\n",
    "            edge_data['explore_weight'] = 0 if None in edge_data['locs'] else 1\n",
    "                            \n",
    "    def build_lexical_store(self):\n",
    "        self.raw_corpus = [self.normalize_text(ci.chunk_text) for ci in self.chunk_infos]\n",
    "        self.raw_bm25 = BM25Okapi(self.raw_corpus)\n",
    "        \n",
    "    # Retrieve functions\n",
    "    def lexical_retrieval_chunks(self, query:str, n:int=5):\n",
    "        chunk_idxs = self.bm25_retrieve(self.normalize_text(query), self.raw_bm25)\n",
    "        return [(self.chunk_infos[idx].chunk_text, idx) for idx in chunk_idxs][:n]\n",
    "\n",
    "    def lexical_retrieval_entities(self, query:str, n:int=5):\n",
    "        tokenized_query = self.normalize_entity(query)\n",
    "        normal_idxs = self.bm25_retrieve(tokenized_query, self.ent_bm25)\n",
    "        candidate_normals = [' '.join(self.ent_corpus[idx]) for idx in normal_idxs]\n",
    "        temp_ents = []\n",
    "        for normal in candidate_normals:\n",
    "            temp_ents.extend(self.normal2ents[normal])\n",
    "        temp_ent_refs = [' '.join(self.split_lower_text(ent)) for ent in temp_ents]\n",
    "        rouge_l = self.factory.rouge.compute(predictions=[' '.join(tokenized_query)] * len(temp_ent_refs), references=temp_ent_refs, use_aggregator=False)['rougeL']\n",
    "        return [temp_ents[idx] for idx in np.argsort(rouge_l)[::-1]][:n]\n",
    "\n",
    "    def exact_match_chunks(self, query:str):\n",
    "        normalized_query = ' '.join(self.normalize_text(query))\n",
    "        return [(self.chunk_infos[idx].chunk_text, idx) for idx, normalized_chunk in enumerate(self.raw_corpus) if normalized_query in ' '.join(normalized_chunk)]\n",
    "\n",
    "    # LLM call and parser functions\n",
    "    def generate_statements(self, pieces:List[str], chunk_size:int=5, summary_size:int=25, overlap:int=1):\n",
    "        summary_chunks = concate_with_overlap(pieces, summary_size, overlap=overlap)\n",
    "        summaries = self.factory.llm.generate([[HumanMessage(content=summary_prompt.format(chunk=' '.join(summary_chunk)))] for summary_chunk in summary_chunks])\n",
    "        prompts = []\n",
    "        chunks:List[str] = []\n",
    "        for summary_chunk, summary in zip(summary_chunks, summaries.generations):\n",
    "            temp_summary_size = min(summary_size, len(summary_chunk))\n",
    "            summary_chunk = summary_chunk[:temp_summary_size]\n",
    "            for batch_start in range((temp_summary_size + 1) // chunk_size):\n",
    "                chunk = ' '.join(summary_chunk[batch_start * chunk_size : (batch_start + 1) * chunk_size])\n",
    "                prompts.append(statement_prompt.format(summary=summary[0].text, chunk=chunk))\n",
    "                chunks.append(chunk)\n",
    "        return chunks, [self.parse_statements(gen[0].text) for gen in self.factory.llm.generate([[HumanMessage(content=prompt)] for prompt in prompts]).generations]\n",
    "\n",
    "    def parse_statements(self, text:str):\n",
    "        i = 1\n",
    "        statements:List[str] = []\n",
    "        for line in text.strip().splitlines():\n",
    "            if line.startswith(f'{i}. '):\n",
    "                statements.append(line.split(' ', 1)[1].strip())\n",
    "                i += 1\n",
    "        return statements\n",
    "\n",
    "    def extract_entities(self, statements:List[List[str]]):\n",
    "        return [self.parse_entities(gen[0].text) for gen in self.factory.llm.generate([[HumanMessage(content='List the entities in each line of the following statements.\\nAvoid resolving the pronoun unless you are absolutely certain.\\n\\nStatements:\\n' + '\\n'.join([f'{sid+1}. {s}' for sid, s in enumerate(statement)]))] for statement in statements]).generations]\n",
    "\n",
    "    def parse_entities(self, text:str):\n",
    "        i = 1\n",
    "        list_of_ent_list:List[List[str]] = []\n",
    "        for line in text.strip().splitlines():\n",
    "            line = line.strip()\n",
    "            if line.startswith(f'{i}. '):\n",
    "                temp_ent_list = line.split(' ', 1)[1].strip().split(',')\n",
    "                ent_list:List[str] = []\n",
    "                incomplete_ent = []\n",
    "                for ent in temp_ent_list:\n",
    "                    if '(' in ent and ')' not in ent:\n",
    "                        incomplete_ent.append(ent)\n",
    "                    elif '(' not in ent and ')' in ent:\n",
    "                        incomplete_ent.append(ent)\n",
    "                        ent_list.append(','.join(incomplete_ent).strip().strip('.'))\n",
    "                        incomplete_ent.clear()\n",
    "                    elif incomplete_ent:\n",
    "                        incomplete_ent.append(ent)\n",
    "                    else:\n",
    "                        ent_list.append(ent.strip().strip('.'))\n",
    "                ent_list = [ent.split(':', 1)[1].strip() if ent.startswith('Entities:') else ent for ent in ent_list]\n",
    "                ent_list = [self.clean_entity(ent) for ent in ent_list]\n",
    "                ent_list = [ent for ent in ent_list if ent]\n",
    "                list_of_ent_list.append(ent_list)\n",
    "                i += 1\n",
    "        return list_of_ent_list\n",
    "\n",
    "    # Helper functions\n",
    "    def collect_keywords_from_text(self, text:str):\n",
    "        \n",
    "        def trim_det(noun_chunk:spacy.tokens.Span):\n",
    "            for tid, t in enumerate(noun_chunk):\n",
    "                if t.pos_ not in ['DET']:\n",
    "                    return noun_chunk[tid:]\n",
    "                \n",
    "        doc = self.nlp(text)\n",
    "        ncs = [trim_det(nc) for nc in doc.noun_chunks if nc.root.pos_ not in ['NUM', 'PRON']]\n",
    "        ents = [trim_det(ent) for ent in doc.ents if ent.root.pos_ not in ['NUM', 'PRON']]\n",
    "        \n",
    "        ncs_spans = [(nc.start, nc.end) for nc in ncs if nc]\n",
    "        ents_spans = [(ent.start, ent.end) for ent in ents if ent]\n",
    "        nc_id, eid = 0, 0\n",
    "        spans = []\n",
    "        while nc_id < len(ncs_spans) and eid < len(ents_spans):\n",
    "            nc_span, ent_span = ncs_spans[nc_id], ents_spans[eid]\n",
    "            if set(range(*nc_span)).intersection(range(*ent_span)):\n",
    "                merged_span = (min(nc_span[0], ent_span[0]), max(nc_span[1], ent_span[1]))\n",
    "                spans.append(merged_span)\n",
    "                nc_id += 1\n",
    "                eid += 1\n",
    "            else:\n",
    "                if nc_span[0] < ent_span[0]:\n",
    "                    spans.append(nc_span)\n",
    "                    nc_id += 1\n",
    "                else:\n",
    "                    spans.append(ent_span)\n",
    "                    eid += 1\n",
    "        spans.extend(ncs_spans[nc_id:])\n",
    "        spans.extend(ents_spans[eid:])\n",
    "        updated_spans:List[Tuple[int, int]] = []\n",
    "        for span in spans:\n",
    "            doc_span = doc[span[0]:span[1]]\n",
    "            if ',' in doc_span.text:\n",
    "                start = doc_span.start\n",
    "                for t in doc_span:\n",
    "                    if t.text == ',':\n",
    "                        if t.i != start:\n",
    "                            updated_spans.append((start, t.i))\n",
    "                        start = t.i + 1\n",
    "                if start < span[1]:\n",
    "                    updated_spans.append((start, span[1]))\n",
    "            else:\n",
    "                updated_spans.append(span)\n",
    "        updated_spans = [span for span in updated_spans if any([t.pos_ in ['NOUN', 'PROPN'] for t in doc[span[0]:span[1]]])]\n",
    "        updated_spans = sorted([span if doc[span[0]].pos_ != 'PRON' else (span[0]+1, span[1]) for span in updated_spans])\n",
    "        ent_candidates:List[str] = []\n",
    "        ent_mask = -np.ones(len(doc), dtype=np.int32)\n",
    "        for span in updated_spans:\n",
    "            ent = doc[span[0]:span[1]].text.strip('\"\\'')\n",
    "            if len(ent) >= 2 and ent not in ent_candidates:\n",
    "                ent_mask[span[0]:span[1]] = len(ent_candidates)\n",
    "                ent_candidates.append(ent)\n",
    "        \n",
    "        ent_modifiers:List[Tuple[str, List[str]] | Tuple[List[str], str]] = []\n",
    "        for t in doc:\n",
    "            if t.pos_ in ['VERB', 'ADJ', 'AUX', 'ADP'] and ent_mask[t.i] < 0:\n",
    "                modifiers = []\n",
    "                # if t.pos_ in ['VERB', 'AUX']:\n",
    "                if t.pos_ == 'VERB':\n",
    "                    # if t.pos_ == 'AUX':\n",
    "                    #     if t.dep_ == 'auxpass':\n",
    "                    #         continue\n",
    "                    #     else:\n",
    "                    #         modifiers.append(f'{t.lemma_}_{t.i}')\n",
    "                    # elif t.pos_ == 'VERB':\n",
    "                    is_passive = False\n",
    "                    for child in t.children:\n",
    "                        if child.dep_ == 'auxpass':\n",
    "                            modifiers.extend([f'{child.lemma_}_{child.i}', f'{t.text}_{t.i}'])\n",
    "                            is_passive = True\n",
    "                    if not is_passive:\n",
    "                        modifiers.append(f'{t.lemma_}_{t.i}')\n",
    "                    \n",
    "                    subj_found = False\n",
    "                    for child in t.children:\n",
    "                        if 'subj' in child.dep_:\n",
    "                            ent_modifiers.extend([(ent_candidates[ent_mask[subj.i]], modifiers) for subj in self.collect_parallel_ents(child, ent_mask)])\n",
    "                            subj_found = True\n",
    "                        elif 'obj' in child.dep_:\n",
    "                            ent_modifiers.extend([(modifiers, ent_candidates[ent_mask[obj.i]]) for obj in self.collect_parallel_ents(child, ent_mask)])\n",
    "                        elif 'advmod' == child.dep_ and ent_mask[child.i] < 0 and not child.is_stop:\n",
    "                            if child.i < t.i:\n",
    "                                modifiers.insert(0, f'{child.text}_{child.i}')\n",
    "                            else:\n",
    "                                modifiers.append(f'{child.text}_{child.i}')\n",
    "                        elif 'prep' == child.dep_:\n",
    "                            ent_modifiers.extend([(modifiers + new_modifiers, obj) for new_modifiers, obj in self.collect_prep_pobj(child, ent_mask, ent_candidates)])\n",
    "                    if not subj_found:\n",
    "                        for ancestor in t.ancestors:\n",
    "                            for child in ancestor.children:\n",
    "                                if 'subj' in child.dep_:\n",
    "                                    ent_modifiers.extend([(ent_candidates[ent_mask[subj.i]], modifiers) for subj in self.collect_parallel_ents(child, ent_mask)])\n",
    "                                    subj_found = True\n",
    "                            if subj_found:\n",
    "                                break\n",
    "                            \n",
    "                elif t.pos_ == 'ADJ':\n",
    "                    modifiers.append(f'{t.text}_{t.i}')\n",
    "                    for ancestor in t.ancestors:\n",
    "                        if ent_mask[ancestor.i] >= 0:\n",
    "                            ent_modifiers.extend([(ent_candidates[ent_mask[subj.i]], modifiers) for subj in self.collect_parallel_ents(ancestor, ent_mask)])\n",
    "                            break\n",
    "                    if t.dep_ == 'acomp':\n",
    "                        for child in list(t.ancestors)[0].children:\n",
    "                            if 'subj' in child.dep_:\n",
    "                                ent_modifiers.extend([(ent_candidates[ent_mask[subj.i]], modifiers) for subj in self.collect_parallel_ents(child, ent_mask)])\n",
    "                    for child in t.children:\n",
    "                        if child.dep_ == 'prep':\n",
    "                            ent_modifiers.extend([(modifiers + new_modifiers, obj) for new_modifiers, obj in self.collect_prep_pobj(child, ent_mask, ent_candidates)])\n",
    "                        \n",
    "                elif t.pos_ == 'ADP':\n",
    "                    modifiers.append(f'{t.text.lower()}_{t.i}')\n",
    "                    subjs:List[spacy.tokens.Token] = []\n",
    "                    for ancestor in t.ancestors:\n",
    "                        if ent_mask[ancestor.i] >= 0:\n",
    "                            subjs.extend(self.collect_parallel_ents(ancestor, ent_mask))\n",
    "                            break\n",
    "                        if ancestor.pos_ == 'ADP':\n",
    "                            modifiers.insert(0, f'{ancestor.text.lower()}_{ancestor.i}')\n",
    "                        elif ancestor.pos_ == 'AUX':\n",
    "                            for child in ancestor.children:\n",
    "                                if 'subj' in child.dep_:\n",
    "                                    subjs.extend(self.collect_parallel_ents(child, ent_mask))\n",
    "                            break\n",
    "                        else:\n",
    "                            break\n",
    "                    objs:List[spacy.tokens.Token] = []\n",
    "                    for child in t.children:\n",
    "                        if 'obj' in child.dep_:\n",
    "                            objs.extend(self.collect_parallel_ents(child, ent_mask))\n",
    "                    if subjs and objs:\n",
    "                        ent_modifiers.extend([(ent_candidates[ent_mask[s.i]], modifiers) for s in subjs])\n",
    "                        ent_modifiers.extend([(modifiers, ent_candidates[ent_mask[o.i]]) for o in objs])\n",
    "                        \n",
    "                elif t.pos_ == 'AUX':\n",
    "                    modifiers.append(f'{t.lemma_}_{t.i}')\n",
    "                    subjs:List[spacy.tokens.Token] = []\n",
    "                    objs:List[spacy.tokens.Token] = []\n",
    "                    for child in t.children:\n",
    "                        if 'subj' in child.dep_:\n",
    "                            subjs.extend(self.collect_parallel_ents(child, ent_mask))\n",
    "                        if 'obj' in child.dep_:\n",
    "                            objs.extend(self.collect_parallel_ents(child, ent_mask))\n",
    "                    if subjs and objs:\n",
    "                        ent_modifiers.extend([(ent_candidates[ent_mask[s.i]], modifiers) for s in subjs])\n",
    "                        ent_modifiers.extend([(modifiers, ent_candidates[ent_mask[o.i]]) for o in objs])\n",
    "        \n",
    "        return ent_candidates, ent_modifiers\n",
    "\n",
    "    def clean_entity(self, ent_text:str):\n",
    "        ent_doc = self.nlp(ent_text, disable=['parser', 'ner'])\n",
    "        for tid, t in enumerate(ent_doc):\n",
    "            if t.pos_ not in ['DET', 'CCONJ', 'PRON']:\n",
    "                return ent_doc[tid:].text\n",
    "        \n",
    "    def normalize_entity(self, ent_text:str):\n",
    "        # return [t.text.lower() if t.pos_ != 'NOUN' else t.lemma_.lower() for t in self.nlp(ent_text, disable=['parser', 'ner']) if t.pos_ not in ['DET', 'PUNCT', 'ADP', 'SCONJ', 'PRON', 'CCONJ', 'PART', 'AUX']]\n",
    "        return [t.text.lower() if t.pos_ != 'NOUN' else t.lemma_.lower() for t in self.nlp(ent_text, disable=['parser', 'ner']) if not (t.is_stop or t.pos_ == \"PUNCT\")]\n",
    "\n",
    "    def normalize_text(self, text:str):\n",
    "        return [t.lemma_.lower() if t.pos_ in ['NOUN', 'VERB'] else t.text.lower() for t in self.nlp(text, disable=['ner', 'parser']) if not t.is_stop]\n",
    "\n",
    "    def split_lower_text(self, text:str) -> List[str]:\n",
    "        return [t.text.lower() for t in self.nlp(text, disable=['ner', 'parser'])]\n",
    "\n",
    "    def bm25_retrieve(self, tokenized_query:List[str], bm25:BM25Okapi):\n",
    "        index_score_pairs = [(idx, score) for idx, score in enumerate(bm25.get_scores(tokenized_query)) if score > 0]\n",
    "        index_score_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [idx for idx, _ in index_score_pairs]\n",
    "        \n",
    "    def collect_parallel_ents(self, ent:spacy.tokens.Token, ent_mask:np.ndarray):\n",
    "        ret_list:List[spacy.tokens.Token] = []\n",
    "        parallel_ents:List[spacy.tokens.Token] = [ent]\n",
    "        while parallel_ents:\n",
    "            temp_ent = parallel_ents[0]\n",
    "            if ent_mask[temp_ent.i] >= 0:\n",
    "                ret_list.append(temp_ent)\n",
    "        \n",
    "            for child in temp_ent.children:\n",
    "                if child.dep_ in ['conj', 'appos']:\n",
    "                    parallel_ents.append(child)\n",
    "            parallel_ents.pop(0)\n",
    "        return ret_list\n",
    "    \n",
    "    def collect_prep_pobj(self, child:spacy.tokens.Token, ent_mask:np.ndarray, ent_candidates:List[str]):\n",
    "        ent_modifiers:List[Tuple[List[str], str]] = []\n",
    "        if ent_mask[child.i] < 0:\n",
    "            for grand_child in child.children:\n",
    "                if 'obj' in grand_child.dep_:\n",
    "                    ent_modifiers.extend([([f'{child.text}_{child.i}'], ent_candidates[ent_mask[obj.i]]) for obj in self.collect_parallel_ents(grand_child, ent_mask)])\n",
    "                elif 'prep' == grand_child.dep_:\n",
    "                    for grand_grand_child in grand_child.children:\n",
    "                        if 'obj' in grand_grand_child.dep_:\n",
    "                            ent_modifiers.extend([([f'{child.text}_{child.i}', f'{grand_child.text}_{grand_child.i}'], ent_candidates[ent_mask[obj.i]]) for obj in self.collect_parallel_ents(grand_grand_child, ent_mask)])\n",
    "        return ent_modifiers\n",
    "\n",
    "# longdoc = LongDoc(f)#, 'atomic_facts.json')\n",
    "longdoc = LongDoc(f, 'atomic_facts2.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.build_index(article, 'atomic_facts2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.collect_keywords_from_text(longdoc.chunk_infos[1].statements[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cid = 0\n",
    "longdoc.chunk_infos[cid].statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = 2\n",
    "print(longdoc.chunk_infos[cid].entities[sid])\n",
    "print()\n",
    "print(longdoc.chunk_infos[cid].ent_modifiers[sid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.displacy import render\n",
    "render(longdoc.nlp(\"The spaceship Leo is stranded on Mars' moon Phobos.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = longdoc.nlp('Finding a cook on Phobos was difficult because it had only a handful of settlers and most of them had good-paying jobs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_compression(source_text:str, output_text:str):\n",
    "    return float(len(word_tokenize(output_text))) / len(word_tokenize(source_text))\n",
    "\n",
    "for chunk_size in tqdm((5, 10, 15, 20)):\n",
    "    chunks = concate_with_overlap(pieces, chunk_size=chunk_size)\n",
    "    results = f.llm.generate([[HumanMessage(content=f'Rewrite the following passage into a list of statements.\\nEach statement should tell an atomic fact in the passage.\\nAll the statements together should cover all the information in the passage.\\nTry to use the original words from the passage.\\n\\nPassage:\\n{chunk}')] for chunk in chunks])\n",
    "    compression_ratios = []\n",
    "    for cid, (output, source) in enumerate(zip(results.generations, chunks)):\n",
    "        if cid != len(chunks) - 1:\n",
    "            compression_ratios.append(cal_compression(source, output[0].text))\n",
    "    print(chunk_size, np.mean(compression_ratios))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = dataset.get_questions_and_answers(dataset.data[2])\n",
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does the Skipper stop abruptly after he says \"when you\\'re running a blockade\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.exact_match_chunks('''when you\\'re running a blockade''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.exact_match_chunks('''stop abruptly''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(longdoc.ent_graph.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does the Skipper allow the new chef to use the heat-cannon as an incinerator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.lexical_retrieval_entities('heat-cannon', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.lexical_retrieval_entities('incinerator', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.lexical_retrieval_entities('skipper', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.lexical_retrieval_entities('new chef', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = nx.pagerank(longdoc.ent_graph.to_undirected(), personalization={'new cook': 1.0}, weight='weight')\n",
    "sorted(list(pr.items()), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.shortest_path(longdoc.ent_graph.to_undirected(), 'new cook', 'Old Man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.ent_graph.to_undirected()['Old Man']['meal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.chunk_infos[6].statements[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_collection = defaultdict(Counter)\n",
    "target_ents = ['heat-cannon', 'incinerator', 'skipper', 'new chef', 'new cook', 'old man']\n",
    "for target_ent in target_ents:\n",
    "    for real_ent in longdoc.lexical_retrieval_entities(target_ent, 20):\n",
    "        if (target_ent, real_ent) in [('new chef', 'new incinerator shipshape'), ('new chef', 'new age'), ('new chef', 'new course'), ('new chef', 'new incinerator')]:\n",
    "            continue\n",
    "        for pid, sid in longdoc.ent_graph.nodes[real_ent]['locs']:\n",
    "            info_collection[pid][target_ent] += 1\n",
    "df = pd.DataFrame({target_ent: [info_collection[i][target_ent] for i in range(len(longdoc.chunk_infos))] for target_ent in target_ents}, index=range(len(longdoc.chunk_infos)))\n",
    "df.plot(kind='bar', stacked=True)\n",
    "plt.xlabel('Chunks')\n",
    "plt.ylabel('Entity occurrence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.has_path(longdoc.semantic_graph, 'skipper_0', 'Old Man_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nx.all_shortest_paths(longdoc.semantic_graph, 'skipper_0', 'Old Man_2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would've happened if the new cook had told the Skipper about the ekalastron deposits earlier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To effectively filter passages from the story that might answer the question \"What would've happened if the new cook had told the Skipper about the ekalastron deposits earlier?\", here are some key pieces of information to look for:\n",
    "\n",
    "1. **Information about the new cook**:\n",
    "   - The role and significance of the new cook in the story.\n",
    "   - Any specific interactions the new cook has with the Skipper or other characters.\n",
    "\n",
    "2. **Details about the ekalastron deposits**:\n",
    "   - What the ekalastron deposits are and why they are important.\n",
    "   - Any known impact or potential impact of discovering these deposits.\n",
    "\n",
    "3. **The Skipper's role and decision-making**:\n",
    "   - The Skipper's authority and responsibilities.\n",
    "   - How the Skipper typically responds to important information.\n",
    "\n",
    "4. **Consequences of the timing of information**:\n",
    "   - Any events or outcomes directly influenced by the timing of discovering or sharing information about the ekalastron deposits.\n",
    "   - Hypothetical scenarios or speculations within the story about different timings of revealing information.\n",
    "\n",
    "5. **Reactions and outcomes**:\n",
    "   - Charactersâ€™ reactions to discovering the ekalastron deposits.\n",
    "   - Any explicit or implicit suggestions of what could have happened if the information was revealed earlier.\n",
    "\n",
    "These points can help narrow down relevant passages that provide context, character motivations, and possible outcomes related to the timing of sharing information about the ekalastron deposits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = longdoc.lexical_retrieval_entities('new cook', 20)\n",
    "ent_cnts = [(ent, len(longdoc.ent_graph.nodes[ent]['locs'])) for ent in ents]\n",
    "ent_cnts.sort(key=lambda x: x[1], reverse=True)\n",
    "for eid, (ent, cnt) in enumerate(ent_cnts):\n",
    "    print(f'{eid}. {ent}: {cnt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = nx.pagerank(longdoc.ent_graph, personalization={'new cook': 1.0}, weight='weight')\n",
    "sorted(list(pr.items()), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.shortest_path(longdoc.ent_graph, 'cook', 'Captain Slops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.ent_graph['cook']['Mister Dugan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.ent_graph['Mister Dugan']['Captain Slops']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.chunk_infos[4].statements[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.chunk_infos[4].statements[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.shortest_path(longdoc.ent_graph, 'new cook', 'Leo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.ent_graph['new cook']['Leo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.chunk_infos[9].statements[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = longdoc.lexical_retrieval_entities('ekalastron deposits', 20)\n",
    "ent_cnts = [(ent, len(longdoc.ent_graph.nodes[ent]['locs'])) for ent in ents]\n",
    "ent_cnts.sort(key=lambda x: x[1], reverse=True)\n",
    "for eid, (ent, cnt) in enumerate(ent_cnts):\n",
    "    print(f'{eid}. {ent}: {cnt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.ent_graph.nodes['ekalastron deposits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.ent_graph.nodes['rich ekalastron deposits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.chunk_infos[20].statements[17:18] + longdoc.chunk_infos[21].statements[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(longdoc.chunk_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = nx.pagerank(longdoc.semantic_graph.to_undirected(), personalization={'ekalastron deposits_21': 1}, alpha=0.3)\n",
    "sorted(list(pr.items()), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist, path = nx.single_source_dijkstra(longdoc.semantic_graph.reverse(), 'rich ekalastron deposits_20', cutoff=5, weight='explore_weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nx.all_shortest_paths(longdoc.semantic_graph, 'Leo_3', 'rich ekalastron deposits_20', weight='explore_weight'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.semantic_graph.has_edge(\"Captain O'Hara_21\", \"Captain O'Hara_22\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, dist = nx.dijkstra_predecessor_and_distance(longdoc.semantic_graph.reverse(), 'ekalastron deposits_21', cutoff=5, weight='explore_weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(longdoc.semantic_graph.predecessors('ekalastron deposits_21'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.lexical_retrieval_entities('advice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = nx.pagerank(longdoc.semantic_graph.reverse(), personalization={'rich ekalastron deposits_20': 1}, weight='explore_weight', alpha=0.3)\n",
    "sorted(list(pr.items()), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.chunk_infos[21].chunk_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.chunk_infos[22].statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = longdoc.lexical_retrieval_entities('skipper')\n",
    "for eid, ent in enumerate(ents):\n",
    "    print(f'{eid}. {ent}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_qa = read_json('../../data/strategyqa/strategyqa_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_qa[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TempQuestions = read_json('../../data/TempQuestions/TempQuestions.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TempQuestions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonsense_qa = load_dataset('tau/commonsense_qa', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kilt_eli5 = load_dataset('facebook/kilt_tasks', 'eli5', split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asqa = load_dataset('din0s/asqa', split='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asqa[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kilt_eli5[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narrativeqa = load_dataset('THUDM/LongBench', 'narrativeqa', split='test', trust_remote_code=True)\n",
    "qasper = load_dataset('THUDM/LongBench', 'qasper', split='test', trust_remote_code=True)\n",
    "gov_report = load_dataset('THUDM/LongBench', 'gov_report', split='test', trust_remote_code=True)\n",
    "qmsum = load_dataset('THUDM/LongBench', 'qmsum', split='test', trust_remote_code=True)\n",
    "multifieldqa_zh = load_dataset('THUDM/LongBench', 'multifieldqa_zh', split='test', trust_remote_code=True)\n",
    "vcsum = load_dataset('THUDM/LongBench', 'vcsum', split='test', trust_remote_code=True)\n",
    "\n",
    "quality = QualityDataset(split='dev')\n",
    "squality = []\n",
    "for sample in read_jsonline('../../data/squality/test.jsonl'):\n",
    "    context, input = sample['input'].split('Question:\\n')\n",
    "    context = context.split('\\n', 1)[1].strip()\n",
    "    input = input.split('\\n', 1)[0]\n",
    "    squality.append({'input': input, 'context': context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcsum[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vcsum[7]['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_tasks = [\n",
    "    \"passage_retrieval_en\", # Match passage with summary\n",
    "    # \"trec\", # Match question type\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "needs = []\n",
    "inputs = []\n",
    "for task in [\"narrativeqa\", \"qasper\", \"multifieldqa_en\", \"hotpotqa\", \"2wikimqa\", \"musique\", \\\n",
    "            # \"gov_report\",\n",
    "            \"qmsum\",\n",
    "            # \"multi_news\", \"vcsum\", \n",
    "            # \"triviaqa\", # Few Shot QA\n",
    "            # \"samsum\", # Few Shot Summary\n",
    "            # \"lsht\", # Few Shot Summary\n",
    "            # \"passage_count\", \n",
    "            # \"lcc\", # Code task\n",
    "            # \"repobench-p\", # Code task\n",
    "            ]:\n",
    "    dataset = load_dataset('THUDM/LongBench', task, split='test')\n",
    "    for i in range(20):\n",
    "        inputs.append((dataset[i]['input'], task))\n",
    "\n",
    "for i in range(20):\n",
    "    inputs.append((squality[i]['input'], 'squality'))\n",
    "\n",
    "needs = [[inputs[qid], needs[0].text] for qid, needs in enumerate(f.llm.generate([[HumanMessage(\n",
    "    content=f'''You need to answer the following question based on a given document. Before reading the document, what information would you like to know from the document to answer this question?\\n\\nQuestion: {input_text[0]}''')] for input_text in inputs]).generations)]\n",
    "\n",
    "write_json('needs.json', needs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(needs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in needs:\n",
    "    if n[0][1] == 'trec':\n",
    "        print(n[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nid = 20\n",
    "print(needs[nid][0])\n",
    "print(needs[nid][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('questions.txt') as f_in:\n",
    "#     questions = f_in.read().splitlines()\n",
    "random.shuffle(questions)\n",
    "print('\\n'.join([f'{qid+1}. {q}' for qid, q in enumerate(questions[:20])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qmsum[0]['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qmsum[0]['answers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.build_index(qmsum[0]['context'], 'qmsum.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.enrich_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.lexical_retrieval_entities('remote control', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.lexical_retrieval_entities('working design', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5 = load_dataset('defunct-datasets/eli5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/nyu-mll/SQuALITY/blob/main/data/v1-3/txt/dev.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dev.jsonl') as f_in:\n",
    "    squality = f_in.read()\n",
    "    # squality = [json.loads(l) for l in f_in]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squality[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squality = load_dataset('pszemraj/SQuALITY-v1.3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coreference resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanza.server import CoreNLPClient, StartServer\n",
    "\n",
    "def get_coref_pair(mentions:List[Mention]):\n",
    "    rep, corefs = None, list[Mention]()\n",
    "    for mention in mentions:\n",
    "        if mention.isRepresentativeMention:\n",
    "            if mention.type != 'PRONOMINAL':\n",
    "                rep = mention\n",
    "        else:\n",
    "            corefs.append(mention)\n",
    "    if not rep:\n",
    "        return\n",
    "    return [(*sorted([rep.sentNum, mention.sentNum]), rep.text, mention.text) for mention in corefs if mention.sentNum != rep.sentNum and mention.text != rep.text]\n",
    "\n",
    "def plot_sents_by_coref(article:str, doc:Doc, sent_span:Tuple[int, int]):\n",
    "    print(article[doc.sentences[sent_span[0] - 1].tokens[0].characterOffsetBegin : doc.sentences[sent_span[1] - 1].tokens[-1].characterOffsetEnd], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Chris Manning is a nice person. Chris wrote a simple sentence. It was a joke.\"\n",
    "text = ' '.join(dataset.get_article(dataset.data[2]).split())\n",
    "with CoreNLPClient(\n",
    "    start_server=StartServer.DONT_START,\n",
    "    annotators=['tokenize','ssplit','pos','lemma','ner','depparse','coref'],\n",
    "    endpoint='http://172.22.224.150:9000') as client:\n",
    "    ann = client.annotate(text, output_format='json')\n",
    "    doc = Doc(**ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc.corefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.corefs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.corefs['285']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = []\n",
    "for last_id, mentions in doc.corefs.items():\n",
    "    coref_pairs = get_coref_pair(mentions)\n",
    "    if coref_pairs:\n",
    "        dists.extend([(sum([len(doc.sentences[sid].tokens) for sid in range(s1, s2+1)]), s1, s2, rep, men) for s1, s2, rep, men in coref_pairs])\n",
    "sorted(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dkg = list[Sentence]()\n",
    "dkg_miss = list[Sentence]()\n",
    "for sent in doc.sentences:\n",
    "    deps = {dep.dep for dep in sent.basicDependencies}\n",
    "    if 'nsubj' in deps and deps.intersection({'obj', 'obl', 'nmod:poss'}):\n",
    "        dkg.append(sent)\n",
    "    else:\n",
    "        dkg_miss.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dkg) * 1. / len(doc.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([0.6272965879265092, 0.7296222664015904, 0.6307977736549165])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 6\n",
    "plot_sents_by_coref(text, doc, (dkg[i].index + 1, dkg[i].index + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 11\n",
    "plot_sents_by_coref(text, doc, (dkg_miss[i].index + 1, dkg_miss[i].index + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in dkg:\n",
    "    plot_sents_by_coref(text, doc, (sent.index + 1, sent.index + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dkg_miss[11].basicDependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sents_by_coref(text, doc, (297,\n",
    "  304))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'PRONOMINAL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.sentences[368].tokens[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join([t.word for t in doc.sentences[368].tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join([t.word for t in doc.sentences[369].tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.sentences[47].tokens[0].characterOffsetBegin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.sentences[47].tokens[-1].characterOffsetEnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[3631:3678]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_coref_graph = nx.Graph()\n",
    "for last_id, mentions in doc.corefs.items():\n",
    "    sents = list({mention.sentNum for mention in mentions})\n",
    "    if len(sents) > 1 and mentions[0].type != 'PRONOMINAL':\n",
    "        sent_coref_graph.add_edges_from(zip(sents[:-1], sents[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nx.connected_components(sent_coref_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coref_spans = list[tuple[int, int]]()\n",
    "for last_id, mentions in doc.corefs.items():\n",
    "    sents = list({mention.sentNum for mention in mentions})\n",
    "    if len(sents) > 1 and mentions[0].type != 'PRONOMINAL':\n",
    "        for mention in mentions[1:]:\n",
    "            if mention.sentNum - mentions[0].sentNum <= 30:\n",
    "                coref_spans.append((mentions[0].sentNum, mention.sentNum))\n",
    "coref_spans.sort()\n",
    "\n",
    "merged_coref_spans = list[tuple[int, int]]()\n",
    "for coref_span in coref_spans:\n",
    "    if not merged_coref_spans:\n",
    "        merged_coref_spans.append(coref_span)\n",
    "    else:\n",
    "        if coref_span[1] <= merged_coref_spans[-1][1]:\n",
    "            continue\n",
    "        elif coref_span[1] - merged_coref_spans[-1][0] <= 30:\n",
    "            merged_coref_spans[-1] = (merged_coref_spans[-1][0], coref_span[1])\n",
    "        else:\n",
    "            merged_coref_spans.append(coref_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coref_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_coref_spans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = QualityDataset(split='dev')\n",
    "f = Factory(chunk_size=50, llm_name=None, embeder_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "article = dataset.get_article(dataset.data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = f.split_text(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_embs = np.array(f.embeder.embed_documents(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "clustering = DBSCAN(eps=0.4, min_samples=2, metric='cosine').fit(chunk_embs)\n",
    "# clustering.labels_\n",
    "# clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cluster_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = perform_clustering(np.array(chunk_embs), dim=20, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster2chunks = defaultdict(list)\n",
    "for c, chunk in zip(clustering.labels_, chunks):\n",
    "    cluster2chunks[int(c)].append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster2chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longdoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
