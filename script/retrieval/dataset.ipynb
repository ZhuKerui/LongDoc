{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from src.index_files import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = QualityDataset(split='dev')\n",
    "f = Factory(chunk_size=100, llm_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = dataset.get_article(dataset.data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from rank_bm25 import BM25Okapi\n",
    "import spacy.tokens\n",
    "\n",
    "class ChunkInfo(BaseModel):\n",
    "    i: int\n",
    "    chunk_text: str\n",
    "    statements: List[str] = []\n",
    "    entities: List[List[str]] = []\n",
    "    keywords: List[List[str]] = []\n",
    "        \n",
    "\n",
    "class LongDoc:\n",
    "    \n",
    "    def __init__(self, factory:Factory, chunk_info_file:str=None) -> None:\n",
    "        self.factory = factory\n",
    "        self.nlp = spacy.load('en_core_web_lg')\n",
    "        if chunk_info_file:\n",
    "            self.chunk_infos = [ChunkInfo.parse_obj(line) for line in read_json(chunk_info_file)]\n",
    "            self.build_relation_graph()\n",
    "            self.build_lexical_store()\n",
    "        \n",
    "    def build_index(self, article:str, chunk_info_file:str=None):\n",
    "        pieces = self.factory.split_text(article)\n",
    "        print('generate_statements')\n",
    "        chunks, statements = self.generate_statements(pieces)\n",
    "        # statements = [ci.statements for ci in chunk_infos]\n",
    "        print('end')\n",
    "        chunk_infos = [ChunkInfo(i=i, chunk_text=chunk) for i, chunk in enumerate(chunks)]\n",
    "        missing_chunk_ids = [ci.i for ci in chunk_infos if not ci.statements]\n",
    "        attempt = 0\n",
    "        while missing_chunk_ids:\n",
    "            print(attempt, len(missing_chunk_ids))\n",
    "            attempt += 1\n",
    "            if len(missing_chunk_ids) == len(statements):\n",
    "                entities = self.extract_entities(statements)\n",
    "                for cid, (statement_list, entity_list) in enumerate(zip(statements, entities)):\n",
    "                    if len(statement_list) == len(entity_list):\n",
    "                        chunk_infos[cid].statements, chunk_infos[cid].entities = statement_list, entity_list\n",
    "            else:\n",
    "                temp_stm_groups = []\n",
    "                for missing_ci in missing_chunk_ids:\n",
    "                    temp_stms = statements[missing_ci]\n",
    "                    split = np.random.randint(len(statements) // 3, len(statements) * 2 // 3)\n",
    "                    temp_stm_groups.append(temp_stms[:split])\n",
    "                    temp_stm_groups.append(temp_stms[split:])\n",
    "                entities = self.extract_entities(temp_stm_groups)\n",
    "                for sid in range(len(temp_stm_groups)//2):\n",
    "                    if len(temp_stm_groups[sid * 2]) == len(entities[sid * 2]) and len(temp_stm_groups[sid * 2 + 1]) == len(entities[sid * 2 + 1]):\n",
    "                        chunk_infos[missing_chunk_ids[sid]].statements, chunk_infos[missing_chunk_ids[sid]].entities = temp_stm_groups[sid * 2] + temp_stm_groups[sid * 2 + 1], entities[sid * 2] + entities[sid * 2 + 1]\n",
    "            missing_chunk_ids = [ci.i for ci in chunk_infos if not ci.statements]\n",
    "\n",
    "        for ci in chunk_infos:\n",
    "            for sid, statement in enumerate(ci.statements):\n",
    "                addition_ents, keywords = self.collect_keywords_from_text(statement)\n",
    "                ci.keywords.append(keywords)\n",
    "                for addition_ent in addition_ents:\n",
    "                    if all([addition_ent.lower() not in ent.lower() for ent in ci.entities[sid]]):\n",
    "                        ci.entities[sid].append(addition_ent)\n",
    "        \n",
    "        self.chunk_infos = chunk_infos\n",
    "        self.build_relation_graph()\n",
    "        self.build_lexical_store()\n",
    "        if chunk_info_file:\n",
    "            write_json(chunk_info_file, [ci.dict() for ci in chunk_infos])\n",
    "\n",
    "    def collect_keywords_from_text(self, text:str):\n",
    "        \n",
    "        def trim_det_adv(noun_chunk:spacy.tokens.Span):\n",
    "            for tid, t in enumerate(noun_chunk):\n",
    "                if t.pos_ not in ['DET', 'ADV']:\n",
    "                    return noun_chunk[tid:]\n",
    "                \n",
    "        doc = self.nlp(text)\n",
    "        ncs = [trim_det_adv(nc) for nc in doc.noun_chunks if nc.root.pos_ not in ['NUM', 'PRON']]\n",
    "        ents = [trim_det_adv(ent) for ent in doc.ents if ent.root.pos_ not in ['NUM', 'PRON']]   \n",
    "        keywords = [(t.i, t.i+1) for t in doc if t.pos_ in ['VERB', 'ADJ', 'ADV']]\n",
    "        ncs_spans = [(nc.start, nc.end) for nc in ncs]\n",
    "        ents_spans = [(ent.start, ent.end) for ent in ents]\n",
    "        nc_id, eid = 0, 0\n",
    "        spans = []\n",
    "        while nc_id < len(ncs_spans) and eid < len(ents_spans):\n",
    "            nc_span, ent_span = ncs_spans[nc_id], ents_spans[eid]\n",
    "            if set(range(*nc_span)).intersection(range(*ent_span)):\n",
    "                merged_span = (min(nc_span[0], ent_span[0]), max(nc_span[1], ent_span[1]))\n",
    "                spans.append(merged_span)\n",
    "                nc_id += 1\n",
    "                eid += 1\n",
    "            else:\n",
    "                if nc_span[0] < ent_span[0]:\n",
    "                    spans.append(nc_span)\n",
    "                    nc_id += 1\n",
    "                else:\n",
    "                    spans.append(ent_span)\n",
    "                    eid += 1\n",
    "        spans.extend(ncs_spans[nc_id:])\n",
    "        spans.extend(ents_spans[eid:])\n",
    "        updated_spans:List[Tuple[int, int]] = []\n",
    "        for span in spans:\n",
    "            doc_span = doc[span[0]:span[1]]\n",
    "            if ',' in doc_span.text:\n",
    "                start = doc_span.start\n",
    "                for t in doc_span:\n",
    "                    if t.text == ',':\n",
    "                        if t.i != start:\n",
    "                            updated_spans.append((start, t.i))\n",
    "                        start = t.i + 1\n",
    "                if start < span[1]:\n",
    "                    updated_spans.append((start, span[1]))\n",
    "            else:\n",
    "                updated_spans.append(span)\n",
    "        updated_spans = [span for span in updated_spans if any([t.pos_ in ['NOUN', 'PROPN'] for t in doc[span[0]:span[1]]])]\n",
    "        updated_spans = [span if doc[span[0]].pos_ != 'PRON' else (span[0]+1, span[1]) for span in updated_spans]\n",
    "        ent_candidates = [doc[span[0]:span[1]].text for span in updated_spans]\n",
    "        ent_candidates = [ent.strip('\"') for ent in ent_candidates]\n",
    "        ent_candidates = [ent for ent in ent_candidates if len(ent) >= 2]\n",
    "        \n",
    "        keywords = updated_spans + keywords\n",
    "        keywords.sort(key=lambda x: x[0])\n",
    "        kw_candidates = [doc[span[0]:span[1]].text for span in keywords]\n",
    "        kw_candidates = [ent.strip('\"') for ent in kw_candidates]\n",
    "        return ent_candidates, kw_candidates\n",
    "    \n",
    "    def parse_statements(self, text:str):\n",
    "        i = 1\n",
    "        statements:List[str] = []\n",
    "        for line in text.strip().splitlines():\n",
    "            if line.startswith(f'{i}. '):\n",
    "                statements.append(line.split(' ', 1)[1].strip())\n",
    "                i += 1\n",
    "        return statements\n",
    "\n",
    "    def parse_entities(self, text:str):\n",
    "        i = 1\n",
    "        list_of_ent_list:List[List[str]] = []\n",
    "        for line in text.strip().splitlines():\n",
    "            line = line.strip()\n",
    "            if line.startswith(f'{i}. '):\n",
    "                temp_ent_list = line.split(' ', 1)[1].strip().split(',')\n",
    "                ent_list:List[str] = []\n",
    "                incomplete_ent = []\n",
    "                for ent in temp_ent_list:\n",
    "                    if '(' in ent and ')' not in ent:\n",
    "                        incomplete_ent.append(ent)\n",
    "                    elif '(' not in ent and ')' in ent:\n",
    "                        incomplete_ent.append(ent)\n",
    "                        ent_list.append(','.join(incomplete_ent).strip().strip('.'))\n",
    "                        incomplete_ent.clear()\n",
    "                    elif incomplete_ent:\n",
    "                        incomplete_ent.append(ent)\n",
    "                    else:\n",
    "                        ent_list.append(ent.strip().strip('.'))\n",
    "                ent_list = [ent.split(':', 1)[1].strip() if ent.startswith('Entities:') else ent for ent in ent_list]\n",
    "                ent_list = [self.clean_entity(ent) for ent in ent_list]\n",
    "                ent_list = [ent for ent in ent_list if ent]\n",
    "                list_of_ent_list.append(ent_list)\n",
    "                i += 1\n",
    "        return list_of_ent_list\n",
    "    \n",
    "    def build_relation_graph(self):\n",
    "        relation_graph = nx.Graph()\n",
    "        # semantic edges\n",
    "        for ci in self.chunk_infos:\n",
    "            for sid, related_ents in enumerate(ci.entities):\n",
    "                loc = (ci.i, sid)\n",
    "                # Insert node locs\n",
    "                for e in related_ents:\n",
    "                    if not relation_graph.has_node(e):\n",
    "                        relation_graph.add_node(e, locs=[], norm=' '.join(self.normalize_entity(e)))\n",
    "                    ent_locs:list = relation_graph.nodes[e]['locs']\n",
    "                    if loc not in ent_locs:\n",
    "                        ent_locs.insert(0, loc)\n",
    "                for ent1, ent2 in itertools.combinations(related_ents, 2):\n",
    "                    if not relation_graph.has_edge(ent1, ent2):\n",
    "                        relation_graph.add_edge(ent1, ent2, locs=[])\n",
    "                    edge_locs:list = relation_graph[ent1][ent2]['locs']\n",
    "                    edge_locs.append((loc, loc))\n",
    "        \n",
    "        self.normal2ents:Dict[str, List[str]] = defaultdict(list)\n",
    "        for ent, normal in relation_graph.nodes(data='norm'):\n",
    "            self.normal2ents[normal].append(ent)\n",
    "        normals = list(self.normal2ents)\n",
    "        normals.sort()\n",
    "        self.ent_corpus = [ent.split() for ent in normals]\n",
    "        self.ent_bm25 = BM25Okapi(self.ent_corpus)\n",
    "        \n",
    "        for normal in self.normal2ents:\n",
    "            # Add edges between entities that have the same norm\n",
    "            for ent1, ent2 in itertools.combinations(self.normal2ents[normal], 2):\n",
    "                if not relation_graph.has_edge(ent1, ent2):\n",
    "                    relation_graph.add_edge(ent1, ent2, locs=[])\n",
    "                edge_locs:list = relation_graph[ent1][ent2]['locs']\n",
    "                edge_locs.extend(itertools.product(relation_graph.nodes[ent1]['locs'], relation_graph.nodes[ent2]['locs']))\n",
    "            # Add edges between entities that have similar norms\n",
    "            scores:List[float] = self.ent_bm25.get_scores(normal.split()).tolist()\n",
    "            for score, similar_normal in zip(scores, self.ent_corpus):\n",
    "                if score > 0:\n",
    "                    similar_normal = ' '.join(similar_normal)\n",
    "                    if normal != similar_normal:\n",
    "                        for ent1, ent2 in itertools.product(self.normal2ents[normal], self.normal2ents[similar_normal]):\n",
    "                            if not relation_graph.has_edge(ent1, ent2):\n",
    "                                relation_graph.add_edge(ent1, ent2, locs=[])\n",
    "                            edge_locs:list = relation_graph[ent1][ent2]['locs']\n",
    "                            edge_locs.extend(itertools.product(relation_graph.nodes[ent1]['locs'], relation_graph.nodes[ent2]['locs']))\n",
    "        \n",
    "        self.graph = relation_graph\n",
    "    \n",
    "    def build_lexical_store(self):\n",
    "        self.raw_corpus = [self.normalize_text(ci.chunk_text) for ci in self.chunk_infos]\n",
    "        self.raw_bm25 = BM25Okapi(self.raw_corpus)\n",
    "        \n",
    "    def lexical_retrieval_chunks(self, query:str, n:int=5):\n",
    "        chunk_idxs = self.bm25_retrieve(self.normalize_text(query), self.raw_bm25)\n",
    "        return [(self.chunk_infos[idx].chunk_text, idx) for idx in chunk_idxs][:n]\n",
    "    \n",
    "    def lexical_retrieval_entities(self, query:str, n:int=5):\n",
    "        tokenized_query = self.normalize_entity(query)\n",
    "        normal_idxs = self.bm25_retrieve(tokenized_query, self.ent_bm25)\n",
    "        candidate_normals = [' '.join(self.ent_corpus[idx]) for idx in normal_idxs]\n",
    "        temp_ents = []\n",
    "        for normal in candidate_normals:\n",
    "            temp_ents.extend(self.normal2ents[normal])\n",
    "        temp_ent_corpus = [self.split_lower_text(ent) for ent in temp_ents]\n",
    "        temp_bm25 = BM25Okapi(temp_ent_corpus)\n",
    "        ent_idxs = self.bm25_retrieve(tokenized_query, temp_bm25)\n",
    "        return [temp_ents[idx] for idx in ent_idxs][:n]\n",
    "    \n",
    "    def exact_match_chunks(self, query:str):\n",
    "        normalized_query = ' '.join(self.normalize_text(query))\n",
    "        return [(self.chunk_infos[idx].chunk_text, idx) for idx, normalized_chunk in enumerate(self.raw_corpus) if normalized_query in ' '.join(normalized_chunk)]\n",
    "        \n",
    "    def generate_statements(self, pieces:List[str], chunk_size:int=5, summary_size:int=25, overlap:int=1):\n",
    "        summary_chunks = concate_with_overlap(pieces, summary_size, overlap=overlap)\n",
    "        summaries = self.factory.llm.generate([[HumanMessage(content=summary_prompt.format(chunk=' '.join(summary_chunk)))] for summary_chunk in summary_chunks])\n",
    "        prompts = []\n",
    "        chunks:List[str] = []\n",
    "        for summary_chunk, summary in zip(summary_chunks, summaries.generations):\n",
    "            temp_summary_size = min(summary_size, len(summary_chunk))\n",
    "            summary_chunk = summary_chunk[:temp_summary_size]\n",
    "            for batch_start in range((temp_summary_size + 1) // chunk_size):\n",
    "                chunk = ' '.join(summary_chunk[batch_start * chunk_size : (batch_start + 1) * chunk_size])\n",
    "                prompts.append(statement_prompt.format(summary=summary[0].text, chunk=chunk))\n",
    "                chunks.append(chunk)\n",
    "        return chunks, [self.parse_statements(gen[0].text) for gen in self.factory.llm.generate([[HumanMessage(content=prompt)] for prompt in prompts]).generations]\n",
    "        \n",
    "    def clean_entity(self, ent_text:str):\n",
    "        ent_doc = self.nlp(ent_text, disable=['parser', 'ner'])\n",
    "        for tid, t in enumerate(ent_doc):\n",
    "            if t.pos_ not in ['DET', 'CCONJ', 'PRON']:\n",
    "                return ent_doc[tid:].text\n",
    "            \n",
    "    def normalize_entity(self, ent_text:str):\n",
    "        # return [t.text.lower() if t.pos_ != 'NOUN' else t.lemma_.lower() for t in self.nlp(ent_text, disable=['parser', 'ner']) if t.pos_ not in ['DET', 'PUNCT', 'ADP', 'SCONJ', 'PRON', 'CCONJ', 'PART', 'AUX']]\n",
    "        return [t.text.lower() if t.pos_ != 'NOUN' else t.lemma_.lower() for t in self.nlp(ent_text, disable=['parser', 'ner']) if not (t.is_stop or t.pos_ == \"PUNCT\")]\n",
    "    \n",
    "    def normalize_text(self, text:str):\n",
    "        return [t.lemma_.lower() if t.pos_ in ['NOUN', 'VERB'] else t.text.lower() for t in self.nlp(text, disable=['ner', 'parser']) if not t.is_stop]\n",
    "    \n",
    "    def split_lower_text(self, text:str) -> List[str]:\n",
    "        return [t.text.lower() for t in self.nlp(text, disable=['ner', 'parser'])]\n",
    "    \n",
    "    def bm25_retrieve(self, tokenized_query:List[str], bm25:BM25Okapi):\n",
    "        index_score_pairs = [(idx, score) for idx, score in enumerate(bm25.get_scores(tokenized_query)) if score > 0]\n",
    "        index_score_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [idx for idx, _ in index_score_pairs]\n",
    "        \n",
    "    def extract_entities(self, statements:List[List[str]]):\n",
    "        return [self.parse_entities(gen[0].text) for gen in self.factory.llm.generate([[HumanMessage(content='List the entities in each line of the following statements.\\n\\nStatements:\\n' + '\\n'.join([f'{sid+1}. {s}' for sid, s in enumerate(statement)]))] for statement in statements]).generations]\n",
    "\n",
    "\n",
    "longdoc = LongDoc(f, 'atomic_facts.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = dataset.get_questions_and_answers(dataset.data[2])\n",
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does the Skipper stop abruptly after he says \"when you\\'re running a blockade\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.exact_match_chunks('''when you\\'re running a blockade''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.exact_match_chunks('''stop abruptly''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.lexical_retrieval_entities('Skipper', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does the Skipper allow the new chef to use the heat-cannon as an incinerator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.lexical_retrieval_entities('heat-cannon', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.lexical_retrieval_entities('incinerator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.lexical_retrieval_entities('skipper', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.lexical_retrieval_entities('new chef', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_collection = defaultdict(Counter)\n",
    "target_ents = ['heat-cannon', 'incinerator', 'skipper', 'new chef']\n",
    "for target_ent in target_ents:\n",
    "    for real_ent in longdoc.lexical_retrieval_entities(target_ent, 20):\n",
    "        if (target_ent, real_ent) in [('new chef', 'new incinerator shipshape'), ('new chef', 'new incinerator (the old Nolan heat cannon)')]:\n",
    "            continue\n",
    "        for pid, sid in longdoc.graph.nodes[real_ent]['locs']:\n",
    "            info_collection[pid][target_ent] += 1\n",
    "df = pd.DataFrame({target_ent: [info_collection[i][target_ent] for i in range(len(longdoc.chunk_infos))] for target_ent in target_ents}, index=range(len(longdoc.chunk_infos)))\n",
    "df.plot(kind='bar', stacked=True)\n",
    "plt.xlabel('Chunks')\n",
    "plt.ylabel('Entity occurrence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.chunk_infos[6].statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.chunk_infos[5].chunk_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.graph.nodes['new cook']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.graph.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = nx.pagerank(longdoc.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(pr.items()), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(longdoc.graph.neighbors('Spaceship Leo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.graph['Spaceship Leo']['Leo (spaceship)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[t.pos_ for t in longdoc.nlp(\"he is good at it\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = f.llm.generate([[HumanMessage(content='''Below is a list of entity mentions. Group the mentions that refer to the same entity and give an entity name for each group.\\n\\nEntity mentions:\\n''' + '\\n'.join(ent_set) + '''\\n\\nIf there are duplicated mentions, generate your response in the following format:\\n1. Entity 1's name: list of mentions\\n2. Entity 2's name: list of mentions\\n...\\n\\nIf there is no duplicate, simply generate \"None\".''')]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(chunk_infos[1].json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longdoc.collect_keywords_from_text(chunk_infos[1].statements[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_texts(texts=pieces, embedding=f.embeder)\n",
    "# retriever = vectorstore.similarity_search_with_relevance_scores(pieces[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_compression(source_text:str, output_text:str):\n",
    "    return float(len(word_tokenize(output_text))) / len(word_tokenize(source_text))\n",
    "\n",
    "for chunk_size in tqdm((5, 10, 15, 20)):\n",
    "    chunks = concate_with_overlap(pieces, chunk_size=chunk_size)\n",
    "    results = f.llm.generate([[HumanMessage(content=f'Rewrite the following passage into a list of statements.\\nEach statement should tell an atomic fact in the passage.\\nAll the statements together should cover all the information in the passage.\\nTry to use the original words from the passage.\\n\\nPassage:\\n{chunk}')] for chunk in chunks])\n",
    "    compression_ratios = []\n",
    "    for cid, (output, source) in enumerate(zip(results.generations, chunks)):\n",
    "        if cid != len(chunks) - 1:\n",
    "            compression_ratios.append(cal_compression(source, output[0].text))\n",
    "    print(chunk_size, np.mean(compression_ratios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.parsing.preprocessing import preprocess_string, DEFAULT_FILTERS\n",
    "from gensim.models import Phrases, CoherenceModel, LdaModel, EnsembleLda, LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_funcs = DEFAULT_FILTERS[:-1] # Remove the stemming\n",
    "preprocessed_summary = [preprocess_string(' '.join([t.lemma_ for t in longdoc.nlp(p, disable=['parser', 'ner'])]), preprocess_funcs) for p in all_summary]\n",
    "\n",
    "# bigram = Phrases(preprocessed_summary, min_count=2, threshold=1)\n",
    "\n",
    "# texts = [bigram[p] for p in preprocessed_summary]\n",
    "texts = preprocessed_summary\n",
    "\n",
    "# Create a dictionary from the corpus\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Remove low-frequency terms from the dictionary\n",
    "dictionary.filter_extremes(no_below=2)\n",
    "\n",
    "# Convert the corpus into a bag-of-words representation\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_questions_and_answers(dataset.data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5 = load_dataset('defunct-datasets/eli5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/nyu-mll/SQuALITY/blob/main/data/v1-3/txt/dev.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dev.jsonl') as f_in:\n",
    "    squality = f_in.read()\n",
    "    # squality = [json.loads(l) for l in f_in]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squality[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squality = load_dataset('pszemraj/SQuALITY-v1.3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longdoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
