{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../../data/systematic_review_papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from src.index_files import *\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import pymupdf4llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr2question = {\n",
    "    \"base_model\": \"Which foundation models are used in the proposed method? A foundation model is a machine learning or deep learning model that is trained on broad data such that it can be applied across a wide range of use cases through further training or direct use. Only generate the models that are used in the proposed method and do not generate the baseline models that are used to compare with the proposed method. Also, generate the specific models instead of general model types.\", \n",
    "    \n",
    "    \"adaption\": '''Extract all the functionality adaptations applied to the foundation models in the proposed method. Each adaptation enables the foundation model for a specific functionality in the system. Multiple adaptations could be applied to the same foundation model. For each adaptation, classify the adaptation type following the below steps:\n",
    "\n",
    "1.  Identify the Type of Foundation Model:\n",
    "    •\tDiscriminative Model (e.g., BERT): Focuses on language representation.\n",
    "    •\tGenerative Model (e.g., GPT): Focuses on natural language generation.\n",
    "2.\tClassify Adaptation Based on Model Type:\n",
    "    •\tFor Discriminative Models:\n",
    "        •\tIf the model is adapted to downstream tasks by designing specific objective functions, classify as \"fine-tuning\".\n",
    "        •\tIf the adaptation aligns downstream tasks' objectives with pre-trained loss using hard/soft prompts and a label word verbalizer, classify as \"prompt-tuning\".\n",
    "    •\tFor Generative Models:\n",
    "        •\tCheck if the adaptation method updates the model parameters:\n",
    "            •\tTuning Method:\n",
    "                •\tIf the model serves primarily as an encoder for extracting user or item representations and the parameters are fine-tuned using specific downstream task loss functions, classify as \"fine-tuning.\"\n",
    "\t            •\tIf the output is textual and parameters are trained focusing on a specific task using language modeling loss, classify as \"prompt-tuning.\"\n",
    "\t            •\tIf the parameters are trained for multiple tasks with varying instructions, classify as \"instruction-tuning.\"\n",
    "\t        •\tNon-Tuning Method:\n",
    "\t            •\tIf the adaptation involves designing suitable instructions and prompts for task understanding and solving, classify as \"prompting.\"\n",
    "\t            •\tIf it involves adding demonstration examples in the prompt to enhance task understanding, classify as \"in-context learning.\"\n",
    "\n",
    "This decision tree should help you effectively categorize the adaptation discussed in the given paper. You must extract the necessary implementation details at each step before making any decision to ensure accurate classification.''', \n",
    "    \n",
    "    \"task\": \"On which tasks is the proposed method evaluated? Only return the general tasks and do not return the datasets.\", \n",
    "    \n",
    "    \"paradigm\": '''Extract all the modeling paradigms used in the proposed method. For each pre-trained language model (PLM) that provides a specific functionality in the proposed system, classify its modeling paradigm type following the below steps:\n",
    "\n",
    "    1.\tCheck the Role of PLM: Is the PLM primarily used as a feature extractor?\n",
    "        •\tIf yes, move to step 2.\n",
    "        •\tIf no, go to step 3.\n",
    "    2.\tFeature Extraction Details: Does the PLM output embeddings or tokens?\n",
    "        •\tIf embeddings, classify as \"PLM Embeddings + RS\".\n",
    "        •\tIf tokens, classify as \"PLM Tokens + RS\".\n",
    "    3.\tPLM Integration: Is the PLM itself configured to function directly as a recommendation system?\n",
    "        •\tIf yes, classify as \"PLM as RS\".\n",
    "        •\tIf no, further analysis of the paper is required to determine the specific paradigm. Classify as \"Unknown\".\n",
    "\n",
    "This decision tree should help you effectively categorize the paradigms discussed in a given paper. Extract the necessary details at each step to ensure accurate classification. The paradigm type must be \"PLM Embeddings + RS\", \"PLM Tokens + RS\", \"PLM as RS\" or \"Unknown\".''', \n",
    "}\n",
    "\n",
    "# attr2question = {\n",
    "#     \"base_model\": \"Which foundation models are used in the proposed method? A foundation model is a machine learning or deep learning model that is trained on broad data such that it can be applied across a wide range of use cases through further training or direct use.\", \n",
    "#     \"adaption\": '''What adaptations are applied to the base models in the proposed method? The adaptation includes\n",
    "    \n",
    "#     1. Fine-tuning: The models mainly serve as encoders to extract representations of users or items, and the parameters of the models are subsequently fine-tuned on the specific loss functions of downstream recommendation tasks.\n",
    "#     2. Prompt tuning with discriminative models: Prompt tuning with discriminative models aims to align the representations of pre-trained models like BERT with the domain-specific data through pre-trained loss, using hard/soft prompts and a label word verbalizer.\n",
    "#     3. Prompt tuning with generative models: In the prompt tuning with generative models, the output of the models is consistently textual, and their parameters are trained using the loss of language modeling predominantly on a specific task.\n",
    "#     4. Instruction tuning: In the instruction tuning, the output of the models is consistently textual, and their parameters are trained using the loss of language modeling predominantly on multiple tasks with different types of instructions.\n",
    "#     5. Prompting: Prompting aims to design more suitable instructions and prompts to help models better understand and solve the tasks without training.\n",
    "#     6. In-context learning: In-context learning is a technique using a few demonstration input-label pairs to quickly adapt the models to new tasks and information and predicting the label for an unseen input without additional parameter updates.\n",
    "    \n",
    "#     Extract the adaptations used in the proposed method and generate your response in the following format: \"[Adaptation type]: [Detailed description of the adaptation in the proposed method].\"''', \n",
    "#     \"task\": \"On which tasks is the proposed method evaluated? Only return the general tasks and do not return the datasets.\", \n",
    "#     \"paradigm\": '''What modeling paradigms are used in the proposed method? Modeling paradigms describe the components in a system and their interaction with one another. The modeling paradigm includes\n",
    "    \n",
    "#     1. LLM Embeddings + RS. This modeling paradigm views the language model as a feature extractor, which feeds the features of items and users into LLMs and outputs corresponding embeddings. A traditional RS model can utilize knowledgeaware embeddings for various recommendation tasks.\n",
    "#     2. LLM Tokens + RS. Similar to the former method, this approach generates tokens based on the inputted items' and users' features. The generated tokens capture potential preferences through semantic mining, which can be integrated into the decision-making process of a recommendation system.\n",
    "#     3. LLM as RS. Different from (1) and (2), this paradigm aims to directly transfer pre-trained LLM into a powerful recommendation system. The input sequence usually consists of the profile description, behavior prompt, and task instruction. The output sequence is expected to offer a reasonable recommendation result.\n",
    "    \n",
    "#     Extract the modeling paradigms used in the proposed method and generate your response in the following format: \"[Modeling paradigm type]: [Detailed description of the modeling paradigm in the proposed method].\"''', \n",
    "# }\n",
    "\n",
    "# attr2format = {\n",
    "#     \"adaption\": \"Separate each extracted piece with '\\n\\n'.\", \n",
    "#     \"base_model\": \"Separate each extracted piece with '\\n\\n'.\", \n",
    "#     \"task\": \"Separate each extracted piece with '\\n\\n'.\", \n",
    "#     \"paradigm\": \"Separate each extracted piece with '\\n\\n'.\", \n",
    "# }\n",
    "\n",
    "prompts = {\n",
    "    # attr: 'Answer the question based only on the following context:\\n\\nContext:\\n\\n{context}\\n\\n' + f'Answer the question based on the above context: {question}' for attr, question in attr2question.items()\n",
    "    attr: f'Answer the question based on the above context: {question}' for attr, question in attr2question.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join(attr2question['paradigm'].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr, prompt in prompts.items():\n",
    "    print(attr)\n",
    "    print(prompt)\n",
    "    print('----------------\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_json('dataset.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", model_kwargs={'device': 'cuda:1'}, encode_kwargs={'normalize_embeddings': False})\n",
    "# embeder = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\", model_kwargs={'device': 'cuda:1'}, encode_kwargs={'normalize_embeddings': True})\n",
    "# embeder = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-large\", model_kwargs={'device': 'cuda:1'}, encode_kwargs={'normalize_embeddings': True})\n",
    "# embeder = HuggingFaceBgeEmbeddings(model_name=\"intfloat/e5-mistral-7b-instruct\", model_kwargs={'device': 'cuda:1'}, query_instruction='Instruct: Given a search query, retrieve relevant passages that answer the query\\nQuery: ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "\n",
    "llm_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# llm_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# llm = ChatHuggingFace(\n",
    "#     llm=HuggingFacePipeline(pipeline = pipeline(\n",
    "#         \"text-generation\", model=llm_name, device_map=\"auto\", max_new_tokens=2000\n",
    "#     )),\n",
    "#     tokenizer=AutoTokenizer.from_pretrained(llm_name),\n",
    "#     model_id=llm_name)\n",
    "# f = Factory(llm_name=None)\n",
    "# f.llm_name = llm_name\n",
    "# f.llm = llm\n",
    "# f.llm_tokenizer = llm.tokenizer\n",
    "\n",
    "f = Factory(llm_name=llm_name, base_url='128.174.136.27')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = pymupdf4llm.to_markdown(os.path.join('../../data/systematic_review_papers/', 'Agent4Rec.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(dict)\n",
    "for test_data in tqdm(dataset):\n",
    "    test_file = test_data['file']\n",
    "    article = pymupdf4llm.to_markdown(os.path.join('../../data/systematic_review_papers/', test_file))#, page_chunks=True)\n",
    "    concated_article = '\\n\\n'.join([' '.join(passage.split()) for passage in article.replace('\\n\\n\\n-----\\n\\n', ' ').split('\\n\\n')])\n",
    "    cropped_article = f.llm_tokenizer.decode(f.llm_tokenizer.encode(concated_article, add_special_tokens=False)[:28000])\n",
    "    \n",
    "    prompt2attr = dict[str, str]()\n",
    "    for attr, question in attr2question.items():\n",
    "        prompt2attr[prompts[attr].format(context=cropped_article)] = attr\n",
    "    \n",
    "    test_prompts = list(prompt2attr)\n",
    "    for prompt, gen in zip(test_prompts, f.llm.generate([[HumanMessage(content=full_prompt)] for full_prompt in test_prompts], max_tokens=2000).generations):\n",
    "        results[test_file][f'{prompt2attr[prompt]}_gen'] = gen[0].text\n",
    "        results[test_file][f'{prompt2attr[prompt]}_prompt'] = prompt\n",
    "        \n",
    "write_json(f'sys_review_openllm_{f.llm_name.split(\"/\")[-1]}.json', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeders = [\n",
    "    HuggingFaceBgeEmbeddings(model_name=\"intfloat/e5-mistral-7b-instruct\", model_kwargs={'device': 'cuda:1'}, query_instruction='Instruct: Given a search query, retrieve relevant passages that answer the query\\nQuery: '),\n",
    "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", model_kwargs={'device': 'cuda:1'}, encode_kwargs={'normalize_embeddings': False}),\n",
    "    HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\", model_kwargs={'device': 'cuda:1'}, encode_kwargs={'normalize_embeddings': True})\n",
    "]\n",
    "for embeder in embeders:\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=20,\n",
    "        length_function=lambda x: len(embeder.client.tokenizer.encode(x, add_special_tokens=False)),\n",
    "        separators=[\n",
    "            \"\\n\\n\",\n",
    "            \"\\n\",\n",
    "            \".\",\n",
    "            \",\",\n",
    "            \"\\u200b\",  # Zero-width space\n",
    "            \"\\uff0c\",  # Fullwidth comma\n",
    "            \"\\u3001\",  # Ideographic comma\n",
    "            \"\\uff0e\",  # Fullwidth full stop\n",
    "            \"\\u3002\",  # Ideographic full stop\n",
    "            \" \",\n",
    "            \"\",\n",
    "        ],\n",
    "        # Existing args\n",
    "    )\n",
    "    \n",
    "    results = defaultdict(dict)\n",
    "    for ret_num in [10]:\n",
    "        print('ret_num:', ret_num)\n",
    "        for test_data in tqdm(dataset):\n",
    "            test_file = test_data['file']\n",
    "            print(test_file)\n",
    "            article = pymupdf4llm.to_markdown(os.path.join('../../data/systematic_review_papers/', test_file))#, page_chunks=True)\n",
    "            concated_article = '\\n\\n'.join([' '.join(passage.split()) for passage in article.replace('\\n\\n\\n-----\\n\\n', ' ').split('\\n\\n')])\n",
    "            chunks = text_splitter.create_documents([concated_article])\n",
    "            # for cid, chunk in enumerate(chunks):\n",
    "            #     chunk.metadata['cid'] = str(cid)\n",
    "            print(len(chunks))\n",
    "\n",
    "            db_chroma = Chroma.from_documents(chunks, embeder, ids=[str(i) for i in range(len(chunks))])\n",
    "            \n",
    "            prompt2attr = dict[str, str]()\n",
    "            for attr, question in attr2question.items():\n",
    "                # docs_chroma = db_chroma.similarity_search_with_score('Instruct: Given a search query, retrieve relevant passages that answer the query.\\nQuery: ' + question, k=ret_num)\n",
    "                docs_chroma = db_chroma.similarity_search_with_score(question, k=len(chunks))[:ret_num]\n",
    "                # docs_chroma.sort(key=lambda x: x[0].metadata['cid'])\n",
    "                results[test_file][f'{attr}_retrieve_context'] = [(doc.page_content, _score) for doc, _score in docs_chroma]\n",
    "                context_text = \"\\n\\n\".join([doc.page_content for doc, _score in docs_chroma])\n",
    "                prompt2attr[prompts[attr].format(context=context_text)] = attr\n",
    "            \n",
    "            db_chroma.delete_collection()\n",
    "            del db_chroma\n",
    "            test_prompts = list(prompt2attr)\n",
    "            for prompt, gen in zip(test_prompts, f.llm.generate([[HumanMessage(content=full_prompt)] for full_prompt in test_prompts], max_tokens=2000).generations):\n",
    "                results[test_file][f'{prompt2attr[prompt]}_gen'] = gen[0].text\n",
    "                \n",
    "        write_json(f'sys_review_ret_{ret_num}_{f.llm_name.split(\"/\")[-1]}_{embeder.model_name.split(\"/\")[-1]}.json', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {sample['file']:sample for sample in dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys_review_chatgpt_pdf = read_json('sys_review_chatgpt_pdf.json')\n",
    "# sys_review_chatgpt = read_json('sys_review_chatgpt.json')\n",
    "# sys_review_ret_mistral_mpnet = read_json('sys_review_ret_10_Mistral-7B-Instruct-v0.3_all-mpnet-base-v2.json')\n",
    "# sys_review_ret_mistral_bge = read_json('sys_review_ret_10_Mistral-7B-Instruct-v0.3_bge-large-en-v1.5.json')\n",
    "# sys_review_ret_mistral_mistral = read_json('sys_review_ret_10_Mistral-7B-Instruct-v0.3_e5-mistral-7b-instruct.json')\n",
    "# sys_review_ret_llama3_mpnet = read_json('sys_review_ret_10_Meta-Llama-3.1-8B-Instruct_all-mpnet-base-v2.json')\n",
    "# sys_review_ret_llama3_bge = read_json('sys_review_ret_10_Meta-Llama-3.1-8B-Instruct_bge-large-en-v1.5.json')\n",
    "# sys_review_ret_llama3_mistral = read_json('sys_review_ret_10_Meta-Llama-3.1-8B-Instruct_e5-mistral-7b-instruct.json')\n",
    "# sys_review_openllm_mistral = read_json('sys_review_openllm_Mistral-7B-Instruct-v0.3.json')\n",
    "sys_review_openllm_llama3 = read_json('sys_review_openllm_Meta-Llama-3.1-8B-Instruct.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sys_review_openllm_llama3['RankGPT.pdf']['base_model_prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = sys_review_openllm_mistral_eval\n",
    "# test_data = sys_review_openllm_llama3_eval\n",
    "test_data = sys_review_openllm_llama3\n",
    "# test_data = sys_review_chatgpt_pdf_eval\n",
    "for sample in samples:\n",
    "    print(sample)\n",
    "    for attr in attr2question:\n",
    "        print(attr)\n",
    "        print('Question:', attr)\n",
    "        print('Gold Standard:', samples[sample]['qa'][attr])\n",
    "        if f'{attr}_cls' in test_data[sample]:\n",
    "            print('Generation:', test_data[sample][f'{attr}_cls'], '\\n\\n')\n",
    "        print('Generation:', test_data[sample][f'{attr}_gen'])\n",
    "        print('-----\\n')\n",
    "    print('-------------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = {\n",
    "    'chatgpt' : {\n",
    "        'ReprBERT.pdf': {\n",
    "            \"base_model\": (1/2, 1),\n",
    "            \"adaption\": (1, 1),\n",
    "            \"task\": (1, 1),\n",
    "            \"paradigm\": (1, 1),\n",
    "        },\n",
    "        'MESE.pdf': {\n",
    "            \"base_model\": (1, 1),\n",
    "            \"adaption\": (1, 1),\n",
    "            \"task\": (1, 1),\n",
    "            \"paradigm\": (1/2, 1/2),\n",
    "        },\n",
    "        'ChatGPT.pdf': {\n",
    "            \"base_model\": (1, 1),\n",
    "            \"adaption\": (1, 1),\n",
    "            \"task\": (1, 1),\n",
    "            \"paradigm\": (1, 1),\n",
    "        },\n",
    "        'LLM-Rate.pdf': {\n",
    "            \"base_model\": (1, 1),\n",
    "            \"adaption\": (1, 1),\n",
    "            \"task\": (1, 1),\n",
    "            \"paradigm\": (1, 1),\n",
    "        },\n",
    "        'PEPLER.pdf': {\n",
    "            \"base_model\": (1, 1),\n",
    "            \"adaption\": (1, 1),\n",
    "            \"task\": (1, 1),\n",
    "            \"paradigm\": (0, 0),\n",
    "        },\n",
    "        'Agent4Rec.pdf': {\n",
    "            \"base_model\": (1, 1),\n",
    "            \"adaption\": (1, 1),\n",
    "            \"task\": (1, 7/8),\n",
    "            \"paradigm\": (0, 0),\n",
    "        },\n",
    "        'GLRec.pdf': {\n",
    "            \"base_model\": (1, 1),\n",
    "            \"adaption\": (1/3, 1),\n",
    "            \"task\": (1, 1),\n",
    "            \"paradigm\": (1, 1),\n",
    "        },\n",
    "        'ONCE.pdf': {\n",
    "            \"base_model\": (1, 1),\n",
    "            \"adaption\": (1, 1),\n",
    "            \"task\": (1, 1),\n",
    "            \"paradigm\": (1, 1),\n",
    "        },\n",
    "        'UniCRS.pdf': {\n",
    "            \"base_model\": (1, 1),\n",
    "            \"adaption\": (0, 0),\n",
    "            \"task\": (1, 1),\n",
    "            \"paradigm\": (1/2, 1/2),\n",
    "        },\n",
    "        'SpeedyFeed.pdf': {\n",
    "            \"base_model\": (1/2, 1),\n",
    "            \"adaption\": (1, 1),\n",
    "            \"task\": (1, 1),\n",
    "            \"paradigm\": (1, 1),\n",
    "        },\n",
    "        'RankGPT.pdf': {\n",
    "            \"base_model\": (3/4, 1),\n",
    "            \"adaption\": (2/3, 2/3),\n",
    "            \"task\": (1, 1),\n",
    "            \"paradigm\": (1/2, 1/2),\n",
    "        },\n",
    "    },\n",
    "    'openllm_llama3' : {\n",
    "        'ReprBERT.pdf': {\n",
    "            \"base_model\": (0, 0),\n",
    "            \"adaption\": (0, 0),\n",
    "            \"task\": (0, 0),\n",
    "            \"paradigm\": (0, 0),\n",
    "        },\n",
    "        'MESE.pdf': {\n",
    "            \"base_model\": (0, 0),\n",
    "            \"adaption\": (0, 0),\n",
    "            \"task\": (0, 0),\n",
    "            \"paradigm\": (0, 0),\n",
    "        },\n",
    "        'ChatGPT.pdf': {\n",
    "            \"base_model\": (0, 0),\n",
    "            \"adaption\": (0, 0),\n",
    "            \"task\": (0, 0),\n",
    "            \"paradigm\": (0, 0),\n",
    "        },\n",
    "        'LLM-Rate.pdf': {\n",
    "            \"base_model\": (0, 0),\n",
    "            \"adaption\": (0, 0),\n",
    "            \"task\": (0, 0),\n",
    "            \"paradigm\": (0, 0),\n",
    "        },\n",
    "        'PEPLER.pdf': {\n",
    "            \"base_model\": (0, 0),\n",
    "            \"adaption\": (0, 0),\n",
    "            \"task\": (0, 0),\n",
    "            \"paradigm\": (0, 0),\n",
    "        },\n",
    "        'Agent4Rec.pdf': {\n",
    "            \"base_model\": (0, 0),\n",
    "            \"adaption\": (0, 0),\n",
    "            \"task\": (0, 0),\n",
    "            \"paradigm\": (0, 0),\n",
    "        },\n",
    "        'GLRec.pdf': {\n",
    "            \"base_model\": (0, 0),\n",
    "            \"adaption\": (0, 0),\n",
    "            \"task\": (0, 0),\n",
    "            \"paradigm\": (0, 0),\n",
    "        },\n",
    "        'ONCE.pdf': {\n",
    "            \"base_model\": (0, 0),\n",
    "            \"adaption\": (0, 0),\n",
    "            \"task\": (0, 0),\n",
    "            \"paradigm\": (0, 0),\n",
    "        },\n",
    "        'UniCRS.pdf': {\n",
    "            \"base_model\": (0, 0),\n",
    "            \"adaption\": (0, 0),\n",
    "            \"task\": (0, 0),\n",
    "            \"paradigm\": (0, 0),\n",
    "        },\n",
    "        'SpeedyFeed.pdf': {\n",
    "            \"base_model\": (0, 0),\n",
    "            \"adaption\": (0, 0),\n",
    "            \"task\": (0, 0),\n",
    "            \"paradigm\": (0, 0),\n",
    "        },\n",
    "        'RankGPT.pdf': {\n",
    "            \"base_model\": (0, 0),\n",
    "            \"adaption\": (0, 0),\n",
    "            \"task\": (0, 0),\n",
    "            \"paradigm\": (0, 0),\n",
    "        },\n",
    "    },\n",
    "    'template' : {\n",
    "        'ReprBERT.pdf': {\n",
    "            \"base_model\": (0, 0),\n",
    "            \"adaption\": (0, 0),\n",
    "            \"task\": (0, 0),\n",
    "            \"paradigm\": (0, 0),\n",
    "        },\n",
    "        'MESE.pdf': {\n",
    "            \"base_model\": (0, 0),\n",
    "            \"adaption\": (0, 0),\n",
    "            \"task\": (0, 0),\n",
    "            \"paradigm\": (0, 0),\n",
    "        },\n",
    "        'ChatGPT.pdf': {\n",
    "            \"base_model\": (0, 0),\n",
    "            \"adaption\": (0, 0),\n",
    "            \"task\": (0, 0),\n",
    "            \"paradigm\": (0, 0),\n",
    "        },\n",
    "        'LLM-Rate.pdf': {\n",
    "            \"base_model\": (0, 0),\n",
    "            \"adaption\": (0, 0),\n",
    "            \"task\": (0, 0),\n",
    "            \"paradigm\": (0, 0),\n",
    "        },\n",
    "        'PEPLER.pdf': {\n",
    "            \"base_model\": (0, 0),\n",
    "            \"adaption\": (0, 0),\n",
    "            \"task\": (0, 0),\n",
    "            \"paradigm\": (0, 0),\n",
    "        },\n",
    "        'Agent4Rec.pdf': {\n",
    "            \"base_model\": (0, 0),\n",
    "            \"adaption\": (0, 0),\n",
    "            \"task\": (0, 0),\n",
    "            \"paradigm\": (0, 0),\n",
    "        },\n",
    "        'GLRec.pdf': {\n",
    "            \"base_model\": (0, 0),\n",
    "            \"adaption\": (0, 0),\n",
    "            \"task\": (0, 0),\n",
    "            \"paradigm\": (0, 0),\n",
    "        },\n",
    "        'ONCE.pdf': {\n",
    "            \"base_model\": (0, 0),\n",
    "            \"adaption\": (0, 0),\n",
    "            \"task\": (0, 0),\n",
    "            \"paradigm\": (0, 0),\n",
    "        },\n",
    "        'UniCRS.pdf': {\n",
    "            \"base_model\": (0, 0),\n",
    "            \"adaption\": (0, 0),\n",
    "            \"task\": (0, 0),\n",
    "            \"paradigm\": (0, 0),\n",
    "        },\n",
    "        'SpeedyFeed.pdf': {\n",
    "            \"base_model\": (0, 0),\n",
    "            \"adaption\": (0, 0),\n",
    "            \"task\": (0, 0),\n",
    "            \"paradigm\": (0, 0),\n",
    "        },\n",
    "        'RankGPT.pdf': {\n",
    "            \"base_model\": (0, 0),\n",
    "            \"adaption\": (0, 0),\n",
    "            \"task\": (0, 0),\n",
    "            \"paradigm\": (0, 0),\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'base_model'\n",
    "single_answer_df = []\n",
    "multi_answer_df = []\n",
    "for sample in dataset:\n",
    "    file = sample['file']\n",
    "    if len(sample['qa'][question]) > 1:\n",
    "        multi_answer_df.append({'file': file, 'chatgpt_r': eval_results['chatgpt'][file][question][1], 'chatgpt_p': eval_results['chatgpt'][file][question][0], 'llama3_r': eval_results['openllm_llama3'][file][question][1], 'llama3_p': eval_results['openllm_llama3'][file][question][0]})\n",
    "    else:\n",
    "        single_answer_df.append({'file': file, 'chatgpt_p': eval_results['chatgpt'][file][question][0], 'llama3_p': eval_results['openllm_llama3'][file][question][0]})\n",
    "\n",
    "print(f'{question}_single_answer')\n",
    "df = pd.DataFrame(single_answer_df)\n",
    "print(df)\n",
    "print(f\"chatgpt_p: {df['chatgpt_p'].mean()}\", f\"llama3_p: {df['llama3_p'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{question}_multi_answer')\n",
    "df = pd.DataFrame(multi_answer_df)\n",
    "print(df)\n",
    "print(df['chatgpt_r'].mean(), df['chatgpt_p'].mean(), df['llama3_r'].mean(), df['llama3_p'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'task'\n",
    "single_answer_df = []\n",
    "multi_answer_df = []\n",
    "for sample in dataset:\n",
    "    file = sample['file']\n",
    "    if len(sample['qa'][question]) > 1:\n",
    "        multi_answer_df.append({'file': file, 'chatgpt_r': eval_results['chatgpt'][file][question][1], 'chatgpt_p': eval_results['chatgpt'][file][question][0], 'llama3_r': eval_results['openllm_llama3'][file][question][1], 'llama3_p': eval_results['openllm_llama3'][file][question][0]})\n",
    "    else:\n",
    "        single_answer_df.append({'file': file, 'chatgpt_p': eval_results['chatgpt'][file][question][0], 'llama3_p': eval_results['openllm_llama3'][file][question][0]})\n",
    "\n",
    "print(f'{question}_single_answer')\n",
    "df = pd.DataFrame(single_answer_df)\n",
    "print(df)\n",
    "print(f\"chatgpt_p: {df['chatgpt_p'].mean()}\", f\"llama3_p: {df['llama3_p'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{question}_multi_answer')\n",
    "df = pd.DataFrame(multi_answer_df)\n",
    "print(df)\n",
    "print(df['chatgpt_r'].mean(), df['chatgpt_p'].mean(), df['llama3_r'].mean(), df['llama3_p'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'adaption'\n",
    "single_answer_df = []\n",
    "multi_answer_df = []\n",
    "for sample in dataset:\n",
    "    file = sample['file']\n",
    "    if len(sample['qa'][question]) > 1:\n",
    "        multi_answer_df.append({'file': file, 'chatgpt_r': eval_results['chatgpt'][file][question][1], 'chatgpt_p': eval_results['chatgpt'][file][question][0], 'llama3_r': eval_results['openllm_llama3'][file][question][1], 'llama3_p': eval_results['openllm_llama3'][file][question][0]})\n",
    "    else:\n",
    "        single_answer_df.append({'file': file, 'chatgpt_p': eval_results['chatgpt'][file][question][0], 'llama3_p': eval_results['openllm_llama3'][file][question][0]})\n",
    "\n",
    "print(f'{question}_single_answer')\n",
    "df = pd.DataFrame(single_answer_df)\n",
    "print(df)\n",
    "print(f\"chatgpt_p: {df['chatgpt_p'].mean()}\", f\"llama3_p: {df['llama3_p'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{question}_multi_answer')\n",
    "df = pd.DataFrame(multi_answer_df)\n",
    "print(df)\n",
    "print(df['chatgpt_r'].mean(), df['chatgpt_p'].mean(), df['llama3_r'].mean(), df['llama3_p'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'paradigm'\n",
    "single_answer_df = []\n",
    "multi_answer_df = []\n",
    "for sample in dataset:\n",
    "    file = sample['file']\n",
    "    if len(sample['qa'][question]) > 1:\n",
    "        multi_answer_df.append({'file': file, 'chatgpt_r': eval_results['chatgpt'][file][question][1], 'chatgpt_p': eval_results['chatgpt'][file][question][0], 'llama3_r': eval_results['openllm_llama3'][file][question][1], 'llama3_p': eval_results['openllm_llama3'][file][question][0]})\n",
    "    else:\n",
    "        single_answer_df.append({'file': file, 'chatgpt_p': eval_results['chatgpt'][file][question][0], 'llama3_p': eval_results['openllm_llama3'][file][question][0]})\n",
    "\n",
    "print(f'{question}_single_answer')\n",
    "df = pd.DataFrame(single_answer_df)\n",
    "print(df)\n",
    "print(f\"chatgpt_p: {df['chatgpt_p'].mean()}\", f\"llama3_p: {df['llama3_p'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{question}_multi_answer')\n",
    "df = pd.DataFrame(multi_answer_df)\n",
    "print(df)\n",
    "print(df['chatgpt_r'].mean(), df['chatgpt_p'].mean(), df['llama3_r'].mean(), df['llama3_p'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spire.pdf.common import *\n",
    "from spire.pdf import *\n",
    "\n",
    "inputFile = \"../../data/systematic_review_papers/RankGPT.pdf\"\n",
    "\n",
    "# Load a pdf document\n",
    "inputfile = inputFile\n",
    "doc = PdfDocument()\n",
    "doc.LoadFromFile(inputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = doc.Pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookmarks = doc.Bookmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookmarks.Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookmarks.get_Item(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.ExtractText(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longdoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
