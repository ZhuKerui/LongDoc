{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_aclsum import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a range of different questions. Besides extracting information into a taxonomy, I am converting the following datasets into test questions:\n",
    "+ ACLSum (Summarize the Challenge, Approach, and Outcome in the paper)\n",
    "+ SciREX (extract the main results of a scientific article including Dataset, Metric, Task and Method)\n",
    "+ arxivDIGESTables (given a table schema for literature survey and extract targeted values from scientific papers to fill in the table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('words_alpha.txt') as f:\n",
    "    words_alpha = set(f.read().splitlines())\n",
    "doc_manager = DocManager(word_vocab=words_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACLSum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Data Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aclsum import ACLSum\n",
    "\n",
    "# Load per split (\"train\", \"val\", \"test\")\n",
    "split = \"train\"\n",
    "train = ACLSum(split)\n",
    "\n",
    "aclsum_dataset = list[dict]()\n",
    "for doc in train:\n",
    "    aclsum_dataset.append(Sample(\n",
    "        doc_file=f'https://aclanthology.org/{doc.id}.pdf',\n",
    "        doc_strs=[\n",
    "            'Abstract', \n",
    "            DocManager.remove_citations(DocManager.remove_space_before_punct(' '.join(doc.get_all_sentences(['abstract'])))), \n",
    "            'Introduction', \n",
    "            DocManager.remove_citations(DocManager.remove_space_before_punct(' '.join(doc.get_all_sentences(['introduction'])))), \n",
    "            'Conclusion', \n",
    "            DocManager.remove_citations(DocManager.remove_space_before_punct(' '.join(doc.get_all_sentences(['conclusion'])))), \n",
    "        ],\n",
    "        outline='Abstract\\nIntroduction\\nConclusion',\n",
    "        question_types=['challenge', 'approach', 'outcome'],\n",
    "        questions={\n",
    "            'challenge': 'Summarize the challenge of the paper, which is the current situation faced by the researcher. It will normally include a Problem Statement, the Motivation, a Hypothesis and/or a Goal.', \n",
    "            'approach': 'Summarize the approach of the paper: How they intend to carry out the investigation, comments on a theoretical model or framework.', \n",
    "            'outcome': 'Summarize the outcome of the paper: Overall conclusion that should reject or support the research hypothesis.'\n",
    "        },\n",
    "        answers={\n",
    "            'challenge': doc.summaries['challenge'], \n",
    "            'approach': doc.summaries['approach'], \n",
    "            'outcome': doc.summaries['outcome']\n",
    "        },\n",
    "        extractions={\n",
    "            'challenge': [DocManager.remove_citations(DocManager.remove_space_before_punct(sent)) for sent in doc.get_all_highlighted_sentences('challenge')],\n",
    "            'approach': [DocManager.remove_citations(DocManager.remove_space_before_punct(sent)) for sent in doc.get_all_highlighted_sentences('approach')],\n",
    "            'outcome': [DocManager.remove_citations(DocManager.remove_space_before_punct(sent)) for sent in doc.get_all_highlighted_sentences('outcome')],\n",
    "        }\n",
    "    ).model_dump())\n",
    "    \n",
    "with jsonlines.open(f'{ACLSUM_DIR}/{split}_dataset.jsonl', 'w') as f_out:\n",
    "    f_out.write_all(aclsum_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataset config\n",
    "split = 'train'\n",
    "\n",
    "load_from_pdf = False\n",
    "# load_from_pdf = True\n",
    "\n",
    "# Retrieval config\n",
    "# retrieval_method = 'rag'\n",
    "retrieval_method = 'rag_base'\n",
    "# retrieval_method = 'gen'\n",
    "\n",
    "# Chunk config\n",
    "# sent_chunk = True\n",
    "# max_seq_len = None\n",
    "# k = 10\n",
    "# sent_chunk = False\n",
    "# max_seq_len = None\n",
    "# k = 3\n",
    "sent_chunk = False\n",
    "max_seq_len = 100\n",
    "k = 10\n",
    "\n",
    "with jsonlines.open(f'{ACLSUM_DIR}/{split}_dataset.jsonl') as f_in:\n",
    "    aclsum_dataset = [Sample.model_validate(line) for line in f_in]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_samples = [sid for sid, sample in enumerate(aclsum_dataset) if not os.path.exists(f\"{ACLSUM_PDF_DIR}/outline_{sample.doc_file.split('/')[-1].replace('.pdf', '.txt')}\")]\n",
    "missed_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('gen', f'split--{split}', f'load_from_pdf--{load_from_pdf}', f'sent_chunk--{sent_chunk}', f'max_seq_len--{max_seq_len}')\n",
    "for question_type in ['challenge', 'approach', 'outcome']:\n",
    "    with open(f'{ACLSUM_EVALUATION_DIR}/{retrieval_method}/{split}_{question_type}_{load_from_pdf}_{sent_chunk}_{max_seq_len}.json') as f_in:\n",
    "        eval_results = json.load(f_in)\n",
    "        print('question_type', question_type)\n",
    "        print('recall', np.mean([result['recall'] for result in eval_results[:]]))\n",
    "        print('precision', np.mean([result['precision'] for result in eval_results[:]]))\n",
    "        print('f1', np.mean([result['f1'] for result in eval_results[:]]))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rag', f'split--{split}', f'load_from_pdf--{load_from_pdf}', f'sent_chunk--{sent_chunk}', f'max_seq_len--{max_seq_len}', f'k--{k}')\n",
    "for question_type in ['challenge', 'approach', 'outcome']:\n",
    "    eval_file = get_eval_file(retrieval_method, split, question_type, load_from_pdf, sent_chunk, max_seq_len, k, is_temp=False)\n",
    "    with open(eval_file) as f_in:\n",
    "        eval_results = json.load(f_in)\n",
    "        print('question_type', question_type)\n",
    "        print('recall', np.mean([result['recall'] for result in eval_results[:]]))\n",
    "        print('precision', np.mean([result['precision'] for result in eval_results[:]]))\n",
    "        print('f1', np.mean([result['f1'] for result in eval_results[:]]))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval2configs = {\n",
    "    'rag': [\n",
    "        {\n",
    "            'load_from_pdf': False, \n",
    "            'sent_chunk': True, \n",
    "            'max_seq_len': None, \n",
    "            'k': 10\n",
    "        },\n",
    "        {\n",
    "            'load_from_pdf': False, \n",
    "            'sent_chunk': False, \n",
    "            'max_seq_len': 100, \n",
    "            'k': 10\n",
    "        }\n",
    "    ],\n",
    "    'gen': [\n",
    "        {\n",
    "            'load_from_pdf': False, \n",
    "            'sent_chunk': True, \n",
    "            'max_seq_len': None, \n",
    "            'k': None\n",
    "        },\n",
    "        {\n",
    "            'load_from_pdf': True, \n",
    "            'sent_chunk': True, \n",
    "            'max_seq_len': None, \n",
    "            'k': None\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "retrieval2configs['rag_base'] = retrieval2configs['rag']\n",
    "\n",
    "\n",
    "sid = 4\n",
    "split = 'train'\n",
    "question_type = 'challenge'\n",
    "with jsonlines.open(f'{ACLSUM_DIR}/{split}_dataset.jsonl') as f_in:\n",
    "    aclsum_dataset = [Sample.model_validate(line) for line in f_in]\n",
    "\n",
    "sample = aclsum_dataset[sid]\n",
    "test2sents = dict[str, list[str]]()\n",
    "test2process = dict[str, list[dict]]()\n",
    "test2chunks = dict[str, list[str]]()\n",
    "\n",
    "for retrieval_method, retrieval_configs in retrieval2configs.items():\n",
    "    for retrieval_config in retrieval_configs:\n",
    "        load_doc_manager(doc_manager, sample, retrieval_config['load_from_pdf'])\n",
    "        doc_manager.build_chunks(sent_chunk=retrieval_config['sent_chunk'], max_seq_length=retrieval_config['max_seq_len'])\n",
    "        \n",
    "        unique_ngram2sent = get_sent_index([sent.text for section in doc_manager.sections if section.section_nlp_local for sent in section.section_nlp_local.sents])\n",
    "        if retrieval_config['load_from_pdf']:\n",
    "            valid_sent_ids = get_sent_ids([sent for block in sample.doc_strs if block not in ['Abstract', 'Introduction', 'Conclusion'] for sent in spacy_sent_tokenize(doc_manager.nlp, block)], unique_ngram2sent)\n",
    "            if -1 in valid_sent_ids:\n",
    "                print(f'Invalid sent id in sample {sid}, retrieval_config {retrieval_config}, {valid_sent_ids.count(-1)}/{len(valid_sent_ids)}')\n",
    "                valid_sent_ids = [sent_id for sent_id in valid_sent_ids if sent_id > -1]\n",
    "            valid_sent_ids = set(valid_sent_ids)\n",
    "        else:\n",
    "            valid_sent_ids = set(range(max(sent_id for ngram, (sent_id, sent) in unique_ngram2sent.items()) + 1))\n",
    "        \n",
    "        process, retrieved_sents = get_sents_and_process(\n",
    "            doc_manager=doc_manager,\n",
    "            retrieval_method=retrieval_method,\n",
    "            split=split,\n",
    "            sid=sid,\n",
    "            question_type=question_type,\n",
    "            **retrieval_config\n",
    "        )\n",
    "        valid_retrieved_sents = [sent for sent_id, sent in zip(get_sent_ids(retrieved_sents, unique_ngram2sent), retrieved_sents) if sent_id in valid_sent_ids]\n",
    "        test_name = f\"{retrieval_method}_{retrieval_config['load_from_pdf']}_{retrieval_config['sent_chunk']}_{retrieval_config['max_seq_len']}\"\n",
    "        test2sents[test_name] = valid_retrieved_sents\n",
    "        test2process[test_name] = process\n",
    "        test2chunks[test_name] = [chunk.page_content for chunk in doc_manager.chunks]\n",
    "        \n",
    "test2label = {\n",
    "    'rag_False_True_None': 'rag_sent', \n",
    "    'rag_False_False_100': 'rag_100', \n",
    "    'gen_False_True_None': 'gen', \n",
    "    'gen_True_True_None': 'gen_full', \n",
    "    'rag_base_False_True_None': 'rag_sent_base', \n",
    "    'rag_base_False_False_100': 'rag_100_base',\n",
    "    'GOLD': 'GOLD'\n",
    "}\n",
    "\n",
    "label2order = {\n",
    "    'rag_sent': 6, \n",
    "    'rag_100': 5, \n",
    "    'gen': 4, \n",
    "    'gen_full': 3, \n",
    "    'rag_sent_base': 2, \n",
    "    'rag_100_base': 1, \n",
    "    'GOLD': 0, \n",
    "}\n",
    "\n",
    "load_doc_manager(doc_manager, sample, False)\n",
    "source_sents = [sent.text for section in doc_manager.sections if section.section_nlp_local for sent in section.section_nlp_local.sents]\n",
    "unique_ngram2sent = get_sent_index(source_sents)\n",
    "\n",
    "sent_id2labels = [[] for _ in range(max(sent_id for ngram, (sent_id, sent) in unique_ngram2sent.items()) + 1)]\n",
    "for test_name, test_label in test2label.items():\n",
    "    if test_name == 'GOLD':\n",
    "        test_sents = sample.extractions[question_type]\n",
    "    else:\n",
    "        test_sents = test2sents[test_name]\n",
    "    for sent_id in get_sent_ids(test_sents, unique_ngram2sent):\n",
    "        sent_id2labels[sent_id].append(test_label)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipped_labels = {'gen_full', 'rag_sent', 'rag_100'}\n",
    "# skipped_labels = {'gen_full', 'gen'}\n",
    "skipped_labels = {'gen_full', 'gen', 'rag_sent', 'rag_100', 'rag_sent_base', 'rag_100_base'}\n",
    "with open(f'observations_{sid}.txt', 'w') as f_out:\n",
    "    for sent_id, sent in enumerate(source_sents):\n",
    "        f_out.write(f'{sent} --- {\", \".join(sorted([label for label in sent_id2labels[sent_id] if label not in skipped_labels], key=lambda x: label2order[x]))}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_file = get_eval_file('rag', split, question_type, load_from_pdf=False, sent_chunk=False, max_seq_len=100, k=10, is_temp=False)\n",
    "with open(eval_file) as f_in:\n",
    "    eval_results = json.load(f_in)\n",
    "eval_results[sid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eval_result in eval_results:\n",
    "    if eval_result['recall']:\n",
    "        print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2chunks.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2chunks['rag_base_False_False_100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.extractions['challenge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.questions['challenge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_manager.build_chunks(sent_chunk=True, max_seq_length=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_manager.vectorstore.similarity_search('Limitations of the current methods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_manager.vectorstore.similarity_search('We propose a new method to solve this problem.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = PARAGRAPH_SEP.join([f'Chunk {chunk.metadata[\"chunk_id\"]}: {chunk.page_content}' for chunk in doc_manager.chunks])\n",
    "similar_chunk_prompt = f'Below are text chunks from a paper:\\n\\n\\n\\n{content}\\n\\n\\n\\nSelect the Chunk ids that express similar general meaning as the following statement: \\n\\n{\"Previous [Method] has been used for [Task].\"}\\n\\nReturn only the selected chunk ids separated by commas, e.g. \"1, 3, 5\".'\n",
    "chat_completion = doc_manager.client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": similar_chunk_prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=doc_manager.tool_llm,\n",
    ")\n",
    "content = chat_completion.choices[0].message.content\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_manager.chunks[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "split = 'train'\n",
    "question_type = 'challenge'\n",
    "with jsonlines.open(f'{ACLSUM_DIR}/{split}_dataset.jsonl') as f_in:\n",
    "    aclsum_dataset = [Sample.model_validate(line) for line in f_in]\n",
    "eval_metrics = EvalMetrics()\n",
    "eval_results = list[dict]()\n",
    "for sid, sample in enumerate(tqdm(aclsum_dataset)):\n",
    "    load_doc_manager(doc_manager, sample, False)\n",
    "    doc_manager.build_chunks(sent_chunk=True, max_seq_length=None)\n",
    "    unique_ngram2sent = get_sent_index([sent.text for section in doc_manager.sections if section.section_nlp_local for sent in section.section_nlp_local.sents])\n",
    "    chunks = [chunk.page_content for chunk in doc_manager.chunks]\n",
    "    # random.shuffle(chunks)\n",
    "    content = PARAGRAPH_SEP.join([f'Chunk {chunk_id}: {chunk}' for chunk_id, chunk in enumerate(chunks)])\n",
    "    selected_chunk_ids = set[int]()\n",
    "    question2chunk_ids = dict[str, list[int]]()\n",
    "    for question in [\n",
    "        # \"[Task] is widely studied in the research.\",\n",
    "        \"Introduction of [Task].\",\n",
    "        # \"Previous [Method] has been used for [Task].\",\n",
    "        # \"Previous [Method] has drawbacks.\",\n",
    "        \"[Method] has been used for [Task].\",\n",
    "        \"[Method] has limitations.\",\n",
    "        # \"We propose a new [Method] to solve this problem.\",\n",
    "    ]:\n",
    "        similar_chunk_prompt = f'Below are text chunks from a paper:\\n\\n\\n\\n{content}\\n\\n\\n\\nRank the **TOP 5** Chunk ids that belong to the following topic: \\n\\n{question}\\n\\nReturn only the selected chunk ids separated by commas, e.g. \"1, 3, 5\".'\n",
    "        chat_completion = doc_manager.client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": similar_chunk_prompt,\n",
    "                }\n",
    "            ],\n",
    "            model=doc_manager.tool_llm,\n",
    "        )\n",
    "        try:\n",
    "            selected_chunk_ids.update(map(int, chat_completion.choices[0].message.content.split(', ')))\n",
    "            question2chunk_ids[question] = list(map(int, chat_completion.choices[0].message.content.split(', ')))\n",
    "        except:\n",
    "            question2chunk_ids[question] = []\n",
    "    \n",
    "    retrieved_sents = [sent for chunk_id in selected_chunk_ids if chunk_id < len(chunks)  for sent in spacy_sent_tokenize(doc_manager.nlp, chunks[chunk_id])]\n",
    "    if not retrieved_sents:\n",
    "        eval_result = {'f1': 0, 'precision': 0, 'recall': 0}\n",
    "    else:\n",
    "        retrieved_sent_ids = get_binary_sent_ids(retrieved_sents, unique_ngram2sent)\n",
    "        gold_sent_ids = get_binary_sent_ids(sample.extractions[question_type], unique_ngram2sent)\n",
    "        \n",
    "        eval_result:dict[str, Any] = eval_metrics.eval_precision_recall_f1(predictions=retrieved_sent_ids, references=gold_sent_ids)\n",
    "    eval_result.update({'sid': sid, 'sent_ids': retrieved_sents, 'question2chunk_ids': question2chunk_ids})\n",
    "    eval_results.append(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp_eval.json', 'w') as f_out:\n",
    "    json.dump(eval_results, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp_eval.json') as f_in:\n",
    "    eval_results = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('recall', np.mean([result['recall'] for result in eval_results[:]]))\n",
    "print('precision', np.mean([result['precision'] for result in eval_results[:]]))\n",
    "print('f1', np.mean([result['f1'] for result in eval_results[:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eval_result in eval_results:\n",
    "    if eval_result['recall'] < 0.5:\n",
    "        print(eval_result['sid'], eval_result['recall'], eval_result['precision'], eval_result['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aclsum_dataset[1].extractions['challenge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eval_result, sample in zip(eval_results, aclsum_dataset):\n",
    "    eval_result['missing'] = set(sample.extractions['challenge']).difference(eval_result['sent_ids'])\n",
    "    eval_result['shared'] = set(sample.extractions['challenge']).intersection(eval_result['sent_ids'])\n",
    "    eval_result['extra'] = set(eval_result['sent_ids']).difference(sample.extractions['challenge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_doc_manager(doc_manager, aclsum_dataset[0], False)\n",
    "doc_manager.build_chunks(sent_chunk=True, max_seq_length=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results[0]['missing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results[0]['extra']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results[0]['shared']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [chunk.page_content for chunk in doc_manager.chunks]\n",
    "[sent for chunk_id in eval_results[0]['question2chunk_ids']['[Method] has limitations.'] if chunk_id < len(chunks)  for sent in spacy_sent_tokenize(doc_manager.nlp, chunks[chunk_id])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aclsum_dataset[0].answers['challenge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aclsum_dataset[4].questions['outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = '''Results show that the proposed system outperforms significantly other stateof-the-art QE systems. This study is part of a bigger, ongoing project, aiming to develop a real-time QE system for Web search, where simplicity is the key to the success. Thus, what we learned from this study is particularly encouraging.'''\n",
    "\n",
    "# context = '''These models are trained on pairs of user queries and the titles of clicked documents using EM. Second, we present a ranker-based QE system, the heart of which is a MRF-based ranker in which the lexicon models are incorporated as features. We perform experiments on the Web search task using a real world data set.'''\n",
    "\n",
    "# context = '''The experimental results show that when implementing the sequence modeling layer with a single-layer Bi-LSTM, our method achieves considerable improvements over the state-of-theart methods in both inference speed and sequence labeling performance.'''\n",
    "\n",
    "context = '''Experimental studies on four benchmark Chinese NER datasets show that our method achieves an inference speed up to 6.15 times faster than those of state-ofthe-art methods, along with a better performance. The experimental results also show that the proposed method can be easily incorporated with pre-trained models like BERT.'''\n",
    "\n",
    "question = aclsum_dataset[4].questions['outcome']\n",
    "\n",
    "grade_doc_prompt = f\"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "\n",
    "Here is the retrieved document:\n",
    "\n",
    "{context}\n",
    "\n",
    "Here is the user question:\n",
    "\n",
    "{question}\n",
    "\n",
    "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.\n",
    "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. Briefly explain your reasoning for the grade.\"\"\"\n",
    "\n",
    "chat_completion = doc_manager.client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": grade_doc_prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=doc_manager.tool_llm,\n",
    ")\n",
    "\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = '''In this work, we propose a simple but effective method for incorporating the word lexicon into the character representations. This method avoids designing a complicated sequence modeling architecture, and for any neural NER model, it requires only subtle adjustment of the character representation layer to introduce the lexicon information. Experimental studies on four benchmark Chinese NER datasets show that our method achieves an inference speed up to 6.15 times faster than those of state-ofthe-art methods, along with a better performance. The experimental results also show that the proposed method can be easily incorporated with pre-trained models like BERT.'''\n",
    "\n",
    "question = aclsum_dataset[4].questions['outcome']\n",
    "\n",
    "grade_doc_prompt = f\"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "\n",
    "Here is the retrieved document:\n",
    "\n",
    "{context}\n",
    "\n",
    "Here is the user question:\n",
    "\n",
    "{question}\n",
    "\n",
    "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.\n",
    "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. Briefly explain your reasoning for the grade.\"\"\"\n",
    "\n",
    "chat_completion = doc_manager.client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": grade_doc_prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=doc_manager.tool_llm,\n",
    ")\n",
    "\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SciREX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Salient Entity Extraction\n",
    "  + Extract the salient Dataset, Method, Task and Metric of the paper.\n",
    "  + An entity is extracted if one of its mentions is being returned.\n",
    "+ Salient Entity Mention Extraction\n",
    "  + Extract the sentences where a salient entity's mention appear.\n",
    "  + An entity mention is extracted if the sentence containing the mention is extracted.\n",
    "+ Salient N-ary Relation Extraction\n",
    "  + Extract the Dataset, Method, Task and Metric tuples that are bounded together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Observation and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open('../../data/SciREX/train.jsonl') as f_in:\n",
    "    scirex_dataset = list(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = scirex_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample['n_ary_relations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample['doc_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample['words'][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "failed_ids = []\n",
    "for sample in tqdm(scirex_dataset[2:]):\n",
    "    paper_meta = requests.get(f\"https://api.semanticscholar.org/graph/v1/paper/{sample['doc_id']}\", params={'fields': 'externalIds'}).json()\n",
    "    while 'externalIds' not in paper_meta:\n",
    "        sleep(10)\n",
    "        paper_meta = requests.get(f\"https://api.semanticscholar.org/graph/v1/paper/{sample['doc_id']}\", params={'fields': 'externalIds'}).json()\n",
    "    if 'ArXiv' not in paper_meta['externalIds']:\n",
    "        failed_ids.append(sample['doc_id'])\n",
    "        continue\n",
    "    download_file(f\"https://arxiv.org/pdf/{paper_meta['externalIds']['ArXiv']}\", f\"../../data/SciREX/pdfs/{sample['doc_id']}.pdf\")\n",
    "    sleep(2)\n",
    "with open('../../data/SciREX/failed_ids.txt', 'w') as f_out:\n",
    "    f_out.write('\\n'.join(failed_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_manager = DocManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = f\"../../data/SciREX/pdfs/{test_sample['doc_id']}.pdf\"\n",
    "if os.path.exists(test_file):\n",
    "    doc_manager.load_doc(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_manager.outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_manager.get_section_by_header('6. Conclusion').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_manager.sections[5].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_manager.sections[3].blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_strs = list[str]()\n",
    "# for section_range in test_sample['sections']:\n",
    "#     section_words = test_sample['words'][section_range[0]:section_range[1]]\n",
    "#     if section_words[0] == 'section' and section_words[1] == ':':\n",
    "#         section_words = section_words[2:]\n",
    "#     doc_strs.append(' '.join(section_words))\n",
    "'https://arxiv.org/pdf/2210.14427'\n",
    "doc_manager.load_doc(doc_file='https://arxiv.org/pdf/1611.08323')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample['method_subrelations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan and Solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dkg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
