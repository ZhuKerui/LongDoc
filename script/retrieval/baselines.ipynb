{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kerui2/miniconda3/envs/longdoc/lib/python3.11/site-packages\n"
     ]
    }
   ],
   "source": [
    "# from sci_review.self_rag import SelfRAG\n",
    "from sci_review.base import Sample\n",
    "import jsonlines\n",
    "from sci_review.paper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a range of different questions. Besides extracting information into a taxonomy, I am converting the following datasets into test questions:\n",
    "+ ACLSum (Summarize the Challenge, Approach, and Outcome in the paper)\n",
    "+ SciREX (extract the main results of a scientific article including Dataset, Metric, Task and Method)\n",
    "+ arxivDIGESTables (given a table schema for literature survey and extract targeted values from scientific papers to fill in the table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACLSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aclsum import ACLSum\n",
    "\n",
    "# Load per split (\"train\", \"val\", \"test\")\n",
    "train = ACLSum(\"train\")\n",
    "\n",
    "aclsum_dataset = list[dict]()\n",
    "for doc in train:\n",
    "    aclsum_dataset.append(Sample(\n",
    "        doc_file=f'https://aclanthology.org/{doc.id}.pdf',\n",
    "        questions=[\n",
    "            'Summarize the challenge of the paper, which is the current situation faced by the researcher. It will normally include a Problem Statement, the Motivation, a Hypothesis and/or a Goal.', \n",
    "            'Summarize the approach of the paper: How they intend to carry out the investigation, comments on a theoretical model or framework.', \n",
    "            'Summarize the outcome of the paper: Overall conclusion that should reject or support the research hypothesis.'\n",
    "        ],\n",
    "        answers=[\n",
    "            doc.summaries['challenge'], \n",
    "            doc.summaries['approach'], \n",
    "            doc.summaries['outcome']\n",
    "        ]\n",
    "    ).model_dump())\n",
    "    \n",
    "with jsonlines.open('../../data/ACLSum/dataset.jsonl', 'w') as f_out:\n",
    "    f_out.write_all(aclsum_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open('../../data/ACLSum/dataset.jsonl') as f_in:\n",
    "    aclsum_dataset = [Sample.model_validate(line) for line in f_in]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SciREX\n",
    "\n",
    "+ Salient Entity Extraction\n",
    "  + Extract the salient Dataset, Method, Task and Metric of the paper.\n",
    "  + An entity is extracted if one of its mentions is being returned.\n",
    "+ Salient Entity Mention Extraction\n",
    "  + Extract the sentences where a salient entity's mention appear.\n",
    "  + An entity mention is extracted if the sentence containing the mention is extracted.\n",
    "+ Salient N-ary Relation Extraction\n",
    "  + Extract the Dataset, Method, Task and Metric tuples that are bounded together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Observation and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open('../../data/SciREX/train.jsonl') as f_in:\n",
    "    scirex_dataset = list(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = scirex_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample['n_ary_relations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample['doc_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample['words'][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_file(url, filename):\n",
    "    \"\"\"Downloads a file from a given URL and saves it with the specified filename.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise an exception if the request failed\n",
    "\n",
    "        with open(filename, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "\n",
    "        print(f\"File '{filename}' downloaded successfully.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "        \n",
    "\n",
    "failed_ids = []\n",
    "for sample in tqdm(scirex_dataset[2:]):\n",
    "    paper_meta = requests.get(f\"https://api.semanticscholar.org/graph/v1/paper/{sample['doc_id']}\", params={'fields': 'externalIds'}).json()\n",
    "    while 'externalIds' not in paper_meta:\n",
    "        sleep(10)\n",
    "        paper_meta = requests.get(f\"https://api.semanticscholar.org/graph/v1/paper/{sample['doc_id']}\", params={'fields': 'externalIds'}).json()\n",
    "    if 'ArXiv' not in paper_meta['externalIds']:\n",
    "        failed_ids.append(sample['doc_id'])\n",
    "        continue\n",
    "    download_file(f\"https://arxiv.org/pdf/{paper_meta['externalIds']['ArXiv']}\", f\"../../data/SciREX/pdfs/{sample['doc_id']}.pdf\")\n",
    "    sleep(2)\n",
    "with open('../../data/SciREX/failed_ids.txt', 'w') as f_out:\n",
    "    f_out.write('\\n'.join(failed_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/11/2025 18:34:32 - INFO - \t missing_keys: []\n",
      "01/11/2025 18:34:32 - INFO - \t unexpected_keys: []\n",
      "01/11/2025 18:34:32 - INFO - \t mismatched_keys: []\n",
      "01/11/2025 18:34:32 - INFO - \t error_msgs: []\n",
      "01/11/2025 18:34:32 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "01/11/2025 18:34:32 - INFO - \t Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n"
     ]
    }
   ],
   "source": [
    "doc_manager = DocManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/11/2025 18:34:43 - INFO - \t HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regeneration attempt: 1\n",
      "\n",
      "1. Introduction  \n",
      "2. Related Work  \n",
      "    2.1. Semantic Segmentation Approaches  \n",
      "    2.2. Network Architectures  \n",
      "3. Network Architectures for Segmentation  \n",
      "    3.1. Feed-Forward Networks  \n",
      "    3.2. Residual Networks (ResNets)  \n",
      "    3.3. Full-Resolution Residual Networks (FRRNs)  \n",
      "4. Training Procedure  \n",
      "5. Experimental Evaluation  \n",
      "    5.1. Residual Network Baseline  \n",
      "    5.2. Quantitative Evaluation  \n",
      "    5.3. Boundary Adherence  \n",
      "6. Conclusion  \n",
      "References  \n",
      "Appendix  \n",
      "    A. Gamma Augmentation  \n",
      "    B. Baseline Evaluation  \n",
      "    C. Qualitative Results  \n",
      "\n",
      "The specified section name does not appear at the beginning of any paragraph in the paper. Section names must either appear at the start of a paragraph or stand alone as an independent paragraph. Please ensure all section names meet these requirements. If not, the section name should be removed.\n",
      "\n",
      "B. Baseline Evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/11/2025 18:34:46 - INFO - \t HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "01/11/2025 18:34:46 - INFO - \t Tokenize 14 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f583a18b51d423f9fff84b6ce5c473b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/11/2025 18:34:48 - INFO - \t ***** Running Inference on 14 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1d93885a6f4d739c639f7f511af24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/11/2025 18:34:53 - INFO - \t Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "test_file = f\"../../data/SciREX/pdfs/{test_sample['doc_id']}.pdf\"\n",
    "if os.path.exists(test_file):\n",
    "    doc_manager.load_doc(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Introduction\n",
      "2. Related Work\n",
      "    Semantic Segmentation Approaches\n",
      "    Network Architectures\n",
      "3. Network Architectures for Segmentation\n",
      "    Feed-Forward Networks\n",
      "    Residual Networks (ResNets)\n",
      "    Full-Resolution Residual Networks (FRRNs)\n",
      "4. Training Procedure\n",
      "5. Experimental Evaluation\n",
      "    5.1. Residual Network Baseline\n",
      "    5.2. Quantitative Evaluation\n",
      "        Overview\n",
      "        Subsampling Factor.\n",
      "    5.3. Boundary Adherence\n",
      "6. Conclusion\n"
     ]
    }
   ],
   "source": [
    "print(doc_manager.outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6. Conclusion\\n\\nIn this paper we propose a novel network architecture for semantic segmentation in street scenes. Our architecture is clean, does not require additional post-processing, can be trained from scratch, shows superior boundary adherence, and reaches state-of-the-art results on the Cityscapes benchmark. We will provide code and all trained models. Since we do not incorporate design choices specifically tailored towards semantic segmentation, we believe that our architecture will also be applicable to other tasks such as stereo or optical flow where predictions are performed per pixel.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_manager.get_section_by_header('6. Conclusion').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../data/SciREX/pdfs/000f90380d768a85e2316225854fc377c079b5c4.pdf'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Feed-Forward Networks.\\n\\nUntil recently, the majority of feedforward networks, such as the VGG-variants [ 50 ], were composed of a linear sequence of layers. Each layer in such a network computes a function F and the output x n of the n -th layer is computed as where W n are the parameters of the layer (see 2 a). We refer to this class of network architectures as traditional feedforward networks.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_manager.sections[5].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DocBlock(text='Semantic Segmentation Approaches.', i=5, is_section_header=True, startswith_section_header=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_manager.sections[3].blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_strs = list[str]()\n",
    "# for section_range in test_sample['sections']:\n",
    "#     section_words = test_sample['words'][section_range[0]:section_range[1]]\n",
    "#     if section_words[0] == 'section' and section_words[1] == ':':\n",
    "#         section_words = section_words[2:]\n",
    "#     doc_strs.append(' '.join(section_words))\n",
    "'https://arxiv.org/pdf/2210.14427'\n",
    "doc_manager.load_doc(doc_file='https://arxiv.org/pdf/1611.08323')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample['method_subrelations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open('../../data/ACLSum/dataset.jsonl') as f_in:\n",
    "    aclsum_dataset = [Sample.model_validate(line) for line in f_in]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = aclsum_dataset[0]\n",
    "self_rag = SelfRAG(doc_file=sample.doc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = self_rag(sample.questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_rag.update_doc(doc_file=aclsum_dataset[1].doc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan and Solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"recursion_limit\": 50}\n",
    "inputs = {\"input\": \"what is the hometown of the mens 2024 Australia open winner?\"}\n",
    "async for event in app.astream(inputs, config=config):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\":\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longdoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
