{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sci_review.paper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a range of different questions. Besides extracting information into a taxonomy, I am converting the following datasets into test questions:\n",
    "+ ACLSum (Summarize the Challenge, Approach, and Outcome in the paper)\n",
    "+ SciREX (extract the main results of a scientific article including Dataset, Metric, Task and Method)\n",
    "+ arxivDIGESTables (given a table schema for literature survey and extract targeted values from scientific papers to fill in the table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/words_alpha.txt') as f:\n",
    "    words_alpha = set(f.read().splitlines())\n",
    "doc_manager = DocManager(word_vocab=words_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACLSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aclsum_base import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset config\n",
    "split = 'train'\n",
    "\n",
    "load_from_pdf = False\n",
    "# load_from_pdf = True\n",
    "\n",
    "# Retrieval config\n",
    "# retrieval_method = 'rag'\n",
    "# retrieval_method = 'rag_base'\n",
    "retrieval_method = 'gen'\n",
    "# retrieval_method = 'cls'\n",
    "\n",
    "# Chunk config\n",
    "sent_chunk = True\n",
    "max_seq_len = None\n",
    "k = 10\n",
    "# sent_chunk = False\n",
    "# max_seq_len = None\n",
    "# k = 3\n",
    "# sent_chunk = False\n",
    "# max_seq_len = 100\n",
    "# k = 10\n",
    "\n",
    "with jsonlines.open(f'{ACLSUM_DIR}/{split}_dataset.jsonl') as f_in:\n",
    "    aclsum_dataset = [Sample.model_validate(line) for line in f_in]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NotebookLM Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    sample = aclsum_dataset[i]\n",
    "    load_doc_manager(doc_manager, sample, True, ACLSUM_DIR, True)\n",
    "    with open(f'{pdf_dir(ACLSUM_DIR)}/{sample.doc_file.split(\":\")[1]}.txt', 'w') as f:\n",
    "        f.write(doc_manager.doc_spacy.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_manager.doc_spacy.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question_type, questions in aclsum_dataset[0].questions.items():\n",
    "    print(question_type)\n",
    "    for question in questions:\n",
    "        print(question)\n",
    "for sample in aclsum_dataset[0:15]:\n",
    "    print(sample.doc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retrieval_method, f'split--{split}', f'load_from_pdf--{load_from_pdf}', f'sent_chunk--{sent_chunk}', f'max_seq_len--{max_seq_len}', f'k--{k}')\n",
    "eval_results = defaultdict(list)\n",
    "for question_type in ['challenge', 'approach', 'outcome']:\n",
    "    eval_file = get_eval_file(retrieval_method, split, question_type, load_from_pdf, sent_chunk, max_seq_len, k, is_temp=False)\n",
    "    if not os.path.exists(eval_file):\n",
    "        continue\n",
    "    with open(eval_file) as f_in:\n",
    "        eval_results[question_type] = json.load(f_in)\n",
    "        print('question_type', question_type)\n",
    "        print('recall', np.mean([result['recall'] for result in eval_results[question_type][:]]))\n",
    "        print('precision', np.mean([result['precision'] for result in eval_results[question_type][:]]))\n",
    "        print('f1', np.mean([result['f1'] for result in eval_results[question_type][:]]))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aclsum_dataset[2].questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question_type in ['challenge', 'approach', 'outcome']:\n",
    "    print(question_type, sum(len(sample.extractions[question_type]) for sample in aclsum_dataset) / len(aclsum_dataset))\n",
    "# challenge 4.55\n",
    "# approach 7.23\n",
    "# outcome 4.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lens = list[int]()\n",
    "for sample in aclsum_dataset:\n",
    "    load_doc_manager(doc_manager, sample, False)\n",
    "    doc_manager.build_chunks(True, None)\n",
    "    doc_lens.append(len(doc_manager.chunks))\n",
    "print(sum(doc_lens) / len(doc_lens))\n",
    "# 38.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_type2sent_lens = defaultdict(list)\n",
    "for sid, sample in enumerate(aclsum_dataset):\n",
    "    load_doc_manager(doc_manager, sample, False)\n",
    "    doc_manager.build_chunks(True, None)\n",
    "    unique_ngram2sent = get_sent_index([sent.text for section in doc_manager.sections if section.section_nlp_local for sent in section.section_nlp_local.sents])\n",
    "    valid_sent_ids = set(range(max(sent_id for ngram, (sent_id, sent) in unique_ngram2sent.items()) + 1))\n",
    "    for question_type in ['challenge', 'approach', 'outcome']:\n",
    "        if sid in [60, 70, 76]:\n",
    "            continue\n",
    "        _, retrieved_sents = get_sents_and_process(\n",
    "                doc_manager=doc_manager,\n",
    "                retrieval_method=retrieval_method,\n",
    "                split=split,\n",
    "                sid=sid,\n",
    "                question_type=question_type,\n",
    "                load_from_pdf=False,\n",
    "                sent_chunk=True,\n",
    "                max_seq_len=None,\n",
    "                k=None,\n",
    "                is_temp=False)\n",
    "        question_type2sent_lens[question_type].append(len(retrieved_sents))\n",
    "for question_type in ['challenge', 'approach', 'outcome']:\n",
    "    print(question_type, sum(question_type2sent_lens[question_type]) / len(question_type2sent_lens[question_type]))\n",
    "# challenge 14.690721649484535\n",
    "# approach 19.649484536082475\n",
    "# outcome 9.371134020618557"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(eval_results['challenge'], key=lambda x: x['precision'], reverse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = aclsum_dataset[2]\n",
    "load_doc_manager(doc_manager, sample, load_from_pdf=False)\n",
    "doc_manager.build_chunks(sent_chunk=True, max_seq_length=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.questions['challenge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [phrase.text for phrase in doc_manager.doc_spacy._.phrases if phrase.rank > 0.04]\n",
    "question = sample.questions['challenge']\n",
    "question_break_down_prompt = f'''Refine the given general information-seeking question about a scientific paper by breaking it down into distinct subtopics, each representing a key aspect of the required information. Ensure each subtopic is concise, addresses a single point that can be explained in one sentence, and is logically connected to others through relevant entity types. Then, for each subtopic, generate three pseudo-sentences, each offering a different way the subtopic might be expressed in the paper. Below is an example of a refined question and its subtopics, along with pseudo-sentences for each subtopic.\n",
    "\n",
    "### Question:\n",
    "What are the key contributions of the paper?\n",
    "\n",
    "### Subtopics:\n",
    "1. **Summary of proposed [Method]**\n",
    "2. **Comparison with existing approaches to [Task]**\n",
    "3. **Performance improvements on [Dataset/Benchmark]**\n",
    "\n",
    "### Pseudo-sentences:\n",
    "\n",
    "1. **Summary of proposed [Method]**\n",
    "    - This paper introduces a novel [Method] for addressing [Task].\n",
    "    - We present [Method], which enhances efficiency in [Task].\n",
    "    - Our approach leverages [Method] to improve performance in [Task].\n",
    "2. **Comparison with existing approaches to [Task]**\n",
    "    - Unlike previous methods, [Method] achieves better generalization in [Task].\n",
    "    - Compared to existing models, [Method] reduces computational cost significantly.\n",
    "    - Our approach differs from prior work by introducing [Key Novel Feature].\n",
    "3. **Performance improvements on [Dataset/Benchmark]**\n",
    "    - Our method achieves state-of-the-art results on [Dataset].\n",
    "    - We report a [X]% improvement in accuracy over previous methods on [Benchmark].\n",
    "    - Experimental results demonstrate superior performance of [Method] on [Dataset].\n",
    "\n",
    "You should follow the format of the example. To assist you in generating subtopics and pseudo-sentences, we provide a list of keywords from the paper as context. Use these keywords to guide your understanding and develop relevant subtopics and pseudo-sentences. Additionally, apply your own knowledge and reasoning to refine the question and enhance your responses.\n",
    "\n",
    "Keywords:\n",
    "{keywords}\n",
    "\n",
    "### Question:\n",
    "{question}'''\n",
    "\n",
    "chat_completion = doc_manager.client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question_break_down_prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=doc_manager.tool_llm,\n",
    ")\n",
    "content = chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_question_breakdown(content:str):\n",
    "    subtopics:dict[str, list[str]] = defaultdict(list[str])\n",
    "    is_subtopic = False\n",
    "    is_pesudo_sentence = False\n",
    "    curr_subtopic = None\n",
    "    for line in content.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if not is_subtopic:\n",
    "            if line.startswith('### Subtopics:'):\n",
    "                is_subtopic = True\n",
    "        else:\n",
    "            if line.startswith('### Pseudo-sentences:'):\n",
    "                is_pesudo_sentence = True\n",
    "            elif not is_pesudo_sentence:\n",
    "                subtopics[line]\n",
    "            else:\n",
    "                if line in subtopics:\n",
    "                    curr_subtopic = line\n",
    "                else:\n",
    "                    subtopics[curr_subtopic].append(line.strip('- '))\n",
    "    return {subtopic.replace('*', ''): sentences for subtopic, sentences in subtopics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtopics = parse_question_breakdown(content)\n",
    "subtopics.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtopics['1. Current limitations of [Chinese NER] methods']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tuple(doc.page_content for doc, score in doc_manager.vectorstore.similarity_search_with_score(subtopics['2. Motivation for improving [NER] performance'][0], k=len(doc_manager.chunks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-m3')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-v2-m3')\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [('Furthermore, they indirectly interface via a soft-attention mechanism, which makes them comparatively isolated.', chunk.page_content) for chunk in doc_manager.chunks]\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(scores, doc_manager.chunks), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = tuple(doc.page_content for doc, score in doc_manager.vectorstore.similarity_search_with_relevance_scores(subtopics['1. Current limitations of [Chinese NER] methods'][0], k=len(doc_manager.chunks)))\n",
    "b = tuple(doc.page_content for doc, score in doc_manager.vectorstore.similarity_search_with_relevance_scores('1. Current limitations of [Chinese NER] methods', k=len(doc_manager.chunks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtopics['4. Goal of the proposed method in addressing NER challenges'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(((doc_manager.phrases[u], doc_manager.phrases[v], weight) for u, v, edge_type, weight in doc_manager.dkg.edges(data='similarity', keys=True) if edge_type == SHARED_TEXT), key=lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval2configs = {\n",
    "    'rag': [\n",
    "        # {\n",
    "        #     'load_from_pdf': False, \n",
    "        #     'sent_chunk': True, \n",
    "        #     'max_seq_len': None, \n",
    "        #     'k': 10\n",
    "        # },\n",
    "        # {\n",
    "        #     'load_from_pdf': False, \n",
    "        #     'sent_chunk': False, \n",
    "        #     'max_seq_len': 100, \n",
    "        #     'k': 10\n",
    "        # }\n",
    "    ],\n",
    "    'gen': [\n",
    "        {\n",
    "            'load_from_pdf': False, \n",
    "            'sent_chunk': True, \n",
    "            'max_seq_len': None, \n",
    "            'k': None\n",
    "        },\n",
    "        # {\n",
    "        #     'load_from_pdf': True, \n",
    "        #     'sent_chunk': True, \n",
    "        #     'max_seq_len': None, \n",
    "        #     'k': None\n",
    "        # }\n",
    "    ]\n",
    "}\n",
    "retrieval2configs['rag_base'] = retrieval2configs['rag']\n",
    "\n",
    "\n",
    "sid = 25\n",
    "split = 'train'\n",
    "question_type = 'challenge'\n",
    "with jsonlines.open(f'{ACLSUM_DIR}/{split}_dataset.jsonl') as f_in:\n",
    "    aclsum_dataset = [Sample.model_validate(line) for line in f_in]\n",
    "\n",
    "sample = aclsum_dataset[sid]\n",
    "test2sents = dict[str, list[str]]()\n",
    "test2process = dict[str, list[dict]]()\n",
    "test2chunks = dict[str, list[str]]()\n",
    "\n",
    "for retrieval_method, retrieval_configs in retrieval2configs.items():\n",
    "    for retrieval_config in retrieval_configs:\n",
    "        load_doc_manager(doc_manager, sample, retrieval_config['load_from_pdf'])\n",
    "        doc_manager.build_chunks(sent_chunk=retrieval_config['sent_chunk'], max_seq_length=retrieval_config['max_seq_len'])\n",
    "        \n",
    "        unique_ngram2sent = get_sent_index([sent.text for section in doc_manager.sections if section.section_nlp_local for sent in section.section_nlp_local.sents])\n",
    "        if retrieval_config['load_from_pdf']:\n",
    "            valid_sent_ids = get_sent_ids([sent for block in sample.doc_strs if block not in ['Abstract', 'Introduction', 'Conclusion'] for sent in spacy_sent_tokenize(doc_manager.nlp, block)], unique_ngram2sent)\n",
    "            if -1 in valid_sent_ids:\n",
    "                print(f'Invalid sent id in sample {sid}, retrieval_config {retrieval_config}, {valid_sent_ids.count(-1)}/{len(valid_sent_ids)}')\n",
    "                valid_sent_ids = [sent_id for sent_id in valid_sent_ids if sent_id > -1]\n",
    "            valid_sent_ids = set(valid_sent_ids)\n",
    "        else:\n",
    "            valid_sent_ids = set(range(max(sent_id for ngram, (sent_id, sent) in unique_ngram2sent.items()) + 1))\n",
    "        \n",
    "        process, retrieved_sents = get_sents_and_process(\n",
    "            doc_manager=doc_manager,\n",
    "            retrieval_method=retrieval_method,\n",
    "            split=split,\n",
    "            sid=sid,\n",
    "            question_type=question_type,\n",
    "            is_temp=True,\n",
    "            **retrieval_config\n",
    "        )\n",
    "        valid_retrieved_sents = [sent for sent_id, sent in zip(get_sent_ids(retrieved_sents, unique_ngram2sent), retrieved_sents) if sent_id in valid_sent_ids]\n",
    "        test_name = f\"{retrieval_method}_{retrieval_config['load_from_pdf']}_{retrieval_config['sent_chunk']}_{retrieval_config['max_seq_len']}\"\n",
    "        test2sents[test_name] = valid_retrieved_sents\n",
    "        test2process[test_name] = process\n",
    "        test2chunks[test_name] = [chunk.page_content for chunk in doc_manager.chunks]\n",
    "        \n",
    "test2label = {\n",
    "    'rag_False_True_None': 'rag_sent', \n",
    "    'rag_False_False_100': 'rag_100', \n",
    "    'gen_False_True_None': 'gen', \n",
    "    'gen_True_True_None': 'gen_full', \n",
    "    'rag_base_False_True_None': 'rag_sent_base', \n",
    "    'rag_base_False_False_100': 'rag_100_base',\n",
    "    'GOLD': 'GOLD'\n",
    "}\n",
    "\n",
    "label2order = {\n",
    "    'rag_sent': 6, \n",
    "    'rag_100': 5, \n",
    "    'gen': 4, \n",
    "    'gen_full': 3, \n",
    "    'rag_sent_base': 2, \n",
    "    'rag_100_base': 1, \n",
    "    'GOLD': 0, \n",
    "}\n",
    "\n",
    "load_doc_manager(doc_manager, sample, False)\n",
    "source_sents = [sent.text for section in doc_manager.sections if section.section_nlp_local for sent in section.section_nlp_local.sents]\n",
    "unique_ngram2sent = get_sent_index(source_sents)\n",
    "\n",
    "sent_id2labels = [[] for _ in range(max(sent_id for ngram, (sent_id, sent) in unique_ngram2sent.items()) + 1)]\n",
    "for test_name, test_label in test2label.items():\n",
    "    if test_name == 'GOLD':\n",
    "        test_sents = sample.extractions[question_type]\n",
    "    else:\n",
    "        if test_name not in test2sents:\n",
    "            continue\n",
    "        test_sents = test2sents[test_name]\n",
    "    for sent_id in get_sent_ids(test_sents, unique_ngram2sent):\n",
    "        sent_id2labels[sent_id].append(test_label)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.extractions[question_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = PARAGRAPH_SEP.join([f'Chunk {chunk.metadata[\"chunk_id\"]}: {chunk.page_content}' for chunk in doc_manager.chunks])\n",
    "prompt = f'Below are text chunks from a paper:\\n\\n\\n\\n{content}\\n\\n\\n\\nSelect the Chunk ids that are relevant to the following question: \\n\\n{sample.questions[question_type]}\\n\\nReturn only the selected chunk ids separated by commas, e.g. \"1, 3, 5\".'\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipped_labels = {'gen_full', 'rag_sent', 'rag_100'}\n",
    "# skipped_labels = {'gen_full', 'gen'}\n",
    "# skipped_labels = {'gen_full', 'gen', 'rag_sent', 'rag_100', 'rag_sent_base', 'rag_100_base'}\n",
    "skipped_labels = {'gen_full', 'rag_sent', 'rag_100', 'rag_sent_base', 'rag_100_base'}\n",
    "with open(f'observations_{sid}.txt', 'w') as f_out:\n",
    "    for sent_id, sent in enumerate(source_sents):\n",
    "        f_out.write(f'{sent} --- {\", \".join(sorted([label for label in sent_id2labels[sent_id] if label not in skipped_labels], key=lambda x: label2order[x]))}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_file = get_eval_file('rag', split, question_type, load_from_pdf=False, sent_chunk=False, max_seq_len=100, k=10, is_temp=False)\n",
    "with open(eval_file) as f_in:\n",
    "    eval_results = json.load(f_in)\n",
    "eval_results[sid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eval_result in eval_results:\n",
    "    if eval_result['recall']:\n",
    "        print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2chunks.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2chunks['rag_base_False_False_100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.extractions['challenge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.questions['challenge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_manager.build_chunks(sent_chunk=True, max_seq_length=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_manager.vectorstore.similarity_search('Limitations of the current methods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_manager.vectorstore.similarity_search('We propose a new method to solve this problem.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = PARAGRAPH_SEP.join([f'Chunk {chunk.metadata[\"chunk_id\"]}: {chunk.page_content}' for chunk in doc_manager.chunks])\n",
    "similar_chunk_prompt = f'Below are text chunks from a paper:\\n\\n\\n\\n{content}\\n\\n\\n\\nSelect the Chunk ids that express similar general meaning as the following statement: \\n\\n{\"Previous [Method] has been used for [Task].\"}\\n\\nReturn only the selected chunk ids separated by commas, e.g. \"1, 3, 5\".'\n",
    "chat_completion = doc_manager.client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": similar_chunk_prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=doc_manager.tool_llm,\n",
    ")\n",
    "content = chat_completion.choices[0].message.content\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_manager.chunks[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "split = 'train'\n",
    "question_type = 'challenge'\n",
    "with jsonlines.open(f'{ACLSUM_DIR}/{split}_dataset.jsonl') as f_in:\n",
    "    aclsum_dataset = [Sample.model_validate(line) for line in f_in]\n",
    "eval_metrics = EvalMetrics()\n",
    "eval_results = list[dict]()\n",
    "for sid, sample in enumerate(tqdm(aclsum_dataset)):\n",
    "    load_doc_manager(doc_manager, sample, False)\n",
    "    doc_manager.build_chunks(sent_chunk=True, max_seq_length=None)\n",
    "    unique_ngram2sent = get_sent_index([sent.text for section in doc_manager.sections if section.section_nlp_local for sent in section.section_nlp_local.sents])\n",
    "    chunks = [chunk.page_content for chunk in doc_manager.chunks]\n",
    "    # random.shuffle(chunks)\n",
    "    content = PARAGRAPH_SEP.join([f'Chunk {chunk_id}: {chunk}' for chunk_id, chunk in enumerate(chunks)])\n",
    "    selected_chunk_ids = set[int]()\n",
    "    question2chunk_ids = dict[str, list[int]]()\n",
    "    for question in [\n",
    "        # \"[Task] is widely studied in the research.\",\n",
    "        \"Introduction of [Task].\",\n",
    "        # \"Previous [Method] has been used for [Task].\",\n",
    "        # \"Previous [Method] has drawbacks.\",\n",
    "        \"[Method] has been used for [Task].\",\n",
    "        \"[Method] has limitations.\",\n",
    "        # \"We propose a new [Method] to solve this problem.\",\n",
    "    ]:\n",
    "        similar_chunk_prompt = f'Below are text chunks from a paper:\\n\\n\\n\\n{content}\\n\\n\\n\\nRank the **TOP 5** Chunk ids that belong to the following topic: \\n\\n{question}\\n\\nReturn only the selected chunk ids separated by commas, e.g. \"1, 3, 5\".'\n",
    "        chat_completion = doc_manager.client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": similar_chunk_prompt,\n",
    "                }\n",
    "            ],\n",
    "            model=doc_manager.tool_llm,\n",
    "        )\n",
    "        try:\n",
    "            selected_chunk_ids.update(map(int, chat_completion.choices[0].message.content.split(', ')))\n",
    "            question2chunk_ids[question] = list(map(int, chat_completion.choices[0].message.content.split(', ')))\n",
    "        except:\n",
    "            question2chunk_ids[question] = []\n",
    "    \n",
    "    retrieved_sents = [sent for chunk_id in selected_chunk_ids if chunk_id < len(chunks)  for sent in spacy_sent_tokenize(doc_manager.nlp, chunks[chunk_id])]\n",
    "    if not retrieved_sents:\n",
    "        eval_result = {'f1': 0, 'precision': 0, 'recall': 0}\n",
    "    else:\n",
    "        retrieved_sent_ids = get_binary_sent_ids(retrieved_sents, unique_ngram2sent)\n",
    "        gold_sent_ids = get_binary_sent_ids(sample.extractions[question_type], unique_ngram2sent)\n",
    "        \n",
    "        eval_result:dict[str, Any] = eval_metrics.eval_precision_recall_f1(predictions=retrieved_sent_ids, references=gold_sent_ids)\n",
    "    eval_result.update({'sid': sid, 'sent_ids': retrieved_sents, 'question2chunk_ids': question2chunk_ids})\n",
    "    eval_results.append(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp_eval.json', 'w') as f_out:\n",
    "    json.dump(eval_results, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp_eval.json') as f_in:\n",
    "    eval_results = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('recall', np.mean([result['recall'] for result in eval_results[:]]))\n",
    "print('precision', np.mean([result['precision'] for result in eval_results[:]]))\n",
    "print('f1', np.mean([result['f1'] for result in eval_results[:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eval_result in eval_results:\n",
    "    if eval_result['recall'] < 0.5:\n",
    "        print(eval_result['sid'], eval_result['recall'], eval_result['precision'], eval_result['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aclsum_dataset[1].extractions['challenge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eval_result, sample in zip(eval_results, aclsum_dataset):\n",
    "    eval_result['missing'] = set(sample.extractions['challenge']).difference(eval_result['sent_ids'])\n",
    "    eval_result['shared'] = set(sample.extractions['challenge']).intersection(eval_result['sent_ids'])\n",
    "    eval_result['extra'] = set(eval_result['sent_ids']).difference(sample.extractions['challenge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_doc_manager(doc_manager, aclsum_dataset[0], False)\n",
    "doc_manager.build_chunks(sent_chunk=True, max_seq_length=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results[0]['missing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results[0]['extra']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results[0]['shared']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [chunk.page_content for chunk in doc_manager.chunks]\n",
    "[sent for chunk_id in eval_results[0]['question2chunk_ids']['[Method] has limitations.'] if chunk_id < len(chunks)  for sent in spacy_sent_tokenize(doc_manager.nlp, chunks[chunk_id])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aclsum_dataset[0].answers['challenge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aclsum_dataset[4].questions['outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = '''Results show that the proposed system outperforms significantly other stateof-the-art QE systems. This study is part of a bigger, ongoing project, aiming to develop a real-time QE system for Web search, where simplicity is the key to the success. Thus, what we learned from this study is particularly encouraging.'''\n",
    "\n",
    "# context = '''These models are trained on pairs of user queries and the titles of clicked documents using EM. Second, we present a ranker-based QE system, the heart of which is a MRF-based ranker in which the lexicon models are incorporated as features. We perform experiments on the Web search task using a real world data set.'''\n",
    "\n",
    "# context = '''The experimental results show that when implementing the sequence modeling layer with a single-layer Bi-LSTM, our method achieves considerable improvements over the state-of-theart methods in both inference speed and sequence labeling performance.'''\n",
    "\n",
    "context = '''Experimental studies on four benchmark Chinese NER datasets show that our method achieves an inference speed up to 6.15 times faster than those of state-ofthe-art methods, along with a better performance. The experimental results also show that the proposed method can be easily incorporated with pre-trained models like BERT.'''\n",
    "\n",
    "question = aclsum_dataset[4].questions['outcome']\n",
    "\n",
    "grade_doc_prompt = f\"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "\n",
    "Here is the retrieved document:\n",
    "\n",
    "{context}\n",
    "\n",
    "Here is the user question:\n",
    "\n",
    "{question}\n",
    "\n",
    "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.\n",
    "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. Briefly explain your reasoning for the grade.\"\"\"\n",
    "\n",
    "chat_completion = doc_manager.client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": grade_doc_prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=doc_manager.tool_llm,\n",
    ")\n",
    "\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = '''In this work, we propose a simple but effective method for incorporating the word lexicon into the character representations. This method avoids designing a complicated sequence modeling architecture, and for any neural NER model, it requires only subtle adjustment of the character representation layer to introduce the lexicon information. Experimental studies on four benchmark Chinese NER datasets show that our method achieves an inference speed up to 6.15 times faster than those of state-ofthe-art methods, along with a better performance. The experimental results also show that the proposed method can be easily incorporated with pre-trained models like BERT.'''\n",
    "\n",
    "question = aclsum_dataset[4].questions['outcome']\n",
    "\n",
    "grade_doc_prompt = f\"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "\n",
    "Here is the retrieved document:\n",
    "\n",
    "{context}\n",
    "\n",
    "Here is the user question:\n",
    "\n",
    "{question}\n",
    "\n",
    "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.\n",
    "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. Briefly explain your reasoning for the grade.\"\"\"\n",
    "\n",
    "chat_completion = doc_manager.client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": grade_doc_prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=doc_manager.tool_llm,\n",
    ")\n",
    "\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciREX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Salient Entity Extraction\n",
    "  + Extract the salient Dataset, Method, Task and Metric of the paper.\n",
    "  + An entity is extracted if one of its mentions is being returned.\n",
    "+ Salient Entity Mention Extraction\n",
    "  + Extract the sentences where a salient entity's mention appear.\n",
    "  + An entity mention is extracted if the sentence containing the mention is extracted.\n",
    "+ Salient N-ary Relation Extraction\n",
    "  + Extract the Dataset, Method, Task and Metric tuples that are bounded together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Observation and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open('../../data/SciREX/train.jsonl') as f_in:\n",
    "    scirex_dataset = list(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = scirex_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample['n_ary_relations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample['doc_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample['words'][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "failed_ids = []\n",
    "for sample in tqdm(scirex_dataset[2:]):\n",
    "    paper_meta = requests.get(f\"https://api.semanticscholar.org/graph/v1/paper/{sample['doc_id']}\", params={'fields': 'externalIds'}).json()\n",
    "    while 'externalIds' not in paper_meta:\n",
    "        sleep(10)\n",
    "        paper_meta = requests.get(f\"https://api.semanticscholar.org/graph/v1/paper/{sample['doc_id']}\", params={'fields': 'externalIds'}).json()\n",
    "    if 'ArXiv' not in paper_meta['externalIds']:\n",
    "        failed_ids.append(sample['doc_id'])\n",
    "        continue\n",
    "    download_file(f\"https://arxiv.org/pdf/{paper_meta['externalIds']['ArXiv']}\", f\"../../data/SciREX/pdf/{sample['doc_id']}.pdf\")\n",
    "    sleep(2)\n",
    "with open('../../data/SciREX/failed_ids.txt', 'w') as f_out:\n",
    "    f_out.write('\\n'.join(failed_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "208291415"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'externalIds', 'url', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = requests.get(f\"https://api.semanticscholar.org/graph/v1/paper/CorpusID:13530374\", params={'fields': 'externalIds'}).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = requests.get(f\"https://api.semanticscholar.org/graph/v1/paper/ACL:2020.aacl-main.88\", params={'fields': 'externalIds'}).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_manager = DocManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = f\"../../data/SciREX/pdfs/{test_sample['doc_id']}.pdf\"\n",
    "if os.path.exists(test_file):\n",
    "    doc_manager.load_doc(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_manager.outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_manager.get_section_by_header('6. Conclusion').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_manager.sections[5].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_manager.sections[3].blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_strs = list[str]()\n",
    "# for section_range in test_sample['sections']:\n",
    "#     section_words = test_sample['words'][section_range[0]:section_range[1]]\n",
    "#     if section_words[0] == 'section' and section_words[1] == ':':\n",
    "#         section_words = section_words[2:]\n",
    "#     doc_strs.append(' '.join(section_words))\n",
    "'https://arxiv.org/pdf/2210.14427'\n",
    "doc_manager.load_doc(doc_file='https://arxiv.org/pdf/1611.08323')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample['method_subrelations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArxivDIGESTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arxivdigestables_base import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tables_collection.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(table['in_text_ref'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table['caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table['in_text_ref']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[5, 3], [1], [5, 2]]\n",
    "a.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(table['row_bib_map'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table['table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table['in_text_ref'][0]['text'].replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table['table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(table['table'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Data Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/ArxivDIGESTables/papers.jsonl') as f_in:\n",
    "    papers = [json.loads(line) for line in f_in]\n",
    "with open('../../data/ArxivDIGESTables/tables.jsonl') as f_in:\n",
    "    tables = [json.loads(line) for line in f_in]\n",
    "with open('../../data/ArxivDIGESTables/full_texts.jsonl') as f_in:\n",
    "    full_texts = [json.loads(line) for line in f_in]\n",
    "\n",
    "full_texts_collection.insert_many(full_texts)\n",
    "papers_collection.insert_many(papers)\n",
    "tables_collection.insert_many(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = papers_collection.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../reference_repo/arxivDIGESTables/predictions/predictions.jsonl') as f_in:\n",
    "    predictions = [json.loads(line) for line in f_in]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tables_collection.find_one({'tabid': 'bb09b7e1-2ab7-4193-922a-1b1b93486e83'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_collection.find_one({'corpus_id': 208291415})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QASA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/QASA/testset_answerable_1554_v1.1.json') as f_in:\n",
    "    qasa_dataset = [sample for _, sample in sorted((int(sid), sample) for sid, sample in json.load(f_in).items())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = qasa_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['question_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['evidential_info'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['s2orc_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/QASA/testset_unanswerable_244_v1.1.json') as f_in:\n",
    "    qasa_dataset = json.load(f_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sci_review.framework import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "with jsonlines.open('../../data/ACLSum/train_rag_10_train_dataset.json') as f_in:\n",
    "    aclsum_dataset = [Sample.model_validate(line) for line in f_in]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aclsum_dataset[0].relevant_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = 'The paper addresses the challenge of answer extraction in open-domain Question Answering (QA) systems, which has become increasingly crucial as these systems aim to find exact answers rather than mere text snippets 2.0 (Chunk 3). A significant issue is that traditional methods heavily rely on Named Entity Recognition (NER), which can lead to performance degradation due to errors in NER, as it may not effectively identify and classify a wide range of named entities (Chunk 8). The authors note that existing systems often struggle with larger candidate answer sets when a general NER is employed, resulting in more difficult answer extraction tasks (Chunk 9). Furthermore, previous approaches that operate at the surface word level, such as density-based ranking and pattern matching, are inadequate for capturing the deeper linguistic relationships necessary for effective answer extraction (Chunk 9). The paper proposes a novel method that explores the correlation of dependency relation paths to rank candidate answers, motivated by the observation that proper answers and question phrases share similar relations (Chunk 10, 19). This approach aims to overcome the limitations of existing methods by incorporating a Maximum Entropy-based ranking model that estimates path weights from training data, thereby enhancing the performance of QA systems, especially for more challenging questions where NER may not provide sufficient support (Chunk 10, 76).'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in re.findall(r'\\(Chunk \\d+(?:, \\d+)*\\)', response):\n",
    "    print(c[7:-1].split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'\\(Chunk \\d+(?:, \\d+)*\\)', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenize(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dkg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
