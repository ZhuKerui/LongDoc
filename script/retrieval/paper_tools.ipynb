{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import re\n",
    "import pymupdf\n",
    "import pymupdf4llm\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import os\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.environ['OPENAI_AUTO_SURVEY'])\n",
    "\n",
    "import spacy\n",
    "import spacy.tokens\n",
    "import pytextrank\n",
    "from fastcoref import spacy_component\n",
    "import numpy as np\n",
    "\n",
    "# nlp.add_pipe(\"fastcoref\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_general_term_prompt = '''The following task involves extracting specific information from a paper. The task description may include general terms that represent types of concrete entities, which need to be identified and clarified based on the paper. Your objective is to identify and list **ALL** general terms or concepts in the task description that might be open to interpretation, require further specification using details from the paper and be critical in locating relevant information in the paper. These include:\n",
    "    1.\tSpecific entities, models, methods, or processes referenced in the description.\n",
    "    2.\tBroad categories or classifications that require more detailed breakdowns.\n",
    "    3.\tImplicit generalizations or assumptions that could benefit from contextual examples or precise definitions.\n",
    "\n",
    "Some general terms may refer to the same entity in the description. You should only list one general term for each entity. Make sure you cover **ALL** the entities.\n",
    "\n",
    "Task: {task}'''\n",
    "\n",
    "\n",
    "organize_general_term_prompt = '''Understand the hierarchy among the general terms you listed above with respect to the \"Parent-Child\" relationship:\n",
    "\n",
    "Parent Concept:\n",
    "A parent concept represents a broader, overarching idea or category that serves as the foundation for related subordinate ideas. It is independent and provides the contextual framework or structure for its associated dependent concepts.\n",
    "\n",
    "Child Concept:\n",
    "A child concept is a more specific, subordinate idea that derives meaning, classification, or context from its associated parent concept. It depends on the parent concept for its definition and existence within a hierarchical structure.\n",
    "\n",
    "Organize the general terms you listed above hierarchically based on their dependencies, ensuring that parent concepts are listed first, followed by their dependent child concepts. Use indentation to represent the hierarchy, with the format as follows:\n",
    "\n",
    "1.\tParent concept\n",
    "    1.1 Dependent child concept\n",
    "        1.1.1 Dependent grandchild concept\n",
    "        1.1.2 Dependent grandchild concept\n",
    "    1.2 Dependent child concept\n",
    "2.\tParent concept\n",
    "    2.1 Dependent child concept\n",
    "\n",
    "Only use the general terms identified in your previous response to create this hierarchical structure.'''\n",
    "\n",
    "generate_checkpoint_prompt = '''To find the relevant information step by step, break down the task into a series of simple, single-step questions. Each question should be narrowly focused, collecting or verifying only one attribute of one entity or entity type, or serving as a follow-up to refine the scope with one additional attribute. Each question can be either a \"What\" question or a \"True or False\" question. Always start with questions for the low level entities (child entities) and then move forward to questions for their parent entity. This structured approach ensures clarity and precision in locating relevant information from the paper.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task = '''Extract the modeling paradigms proposed in the paper that satisfy the following type:\n",
    "\n",
    "# LLM Embeddings + RS. This modeling paradigm views the language model as a feature extractor, which feeds the features of items and users into LLMs and outputs corresponding embeddings. A traditional RS model can utilize knowledge-aware embeddings for various recommendation tasks.'''\n",
    "\n",
    "task = '''Extract the modeling paradigms proposed in the paper that satisfy the following type:\n",
    "\n",
    "LLM as RS. This paradigm aims to directly transfer pre-trained LLM into a powerful recommendation system. The input sequence usually consists of the profile description, behavior prompt, and task instruction. The output sequence is expected to offer a reasonable recommendation result.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub-questions are designed to find necessary information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": extract_general_term_prompt.format(task=task),\n",
    "        }\n",
    "    ],\n",
    "    model=GPT_MODEL_EXPENSIVE,\n",
    ")\n",
    "general_term_str = chat_completion.choices[0].message.content\n",
    "print(general_term_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": extract_general_term_prompt.format(task=task),\n",
    "        }, {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": general_term_str,\n",
    "        }, {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": organize_general_term_prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=GPT_MODEL_EXPENSIVE,\n",
    ")\n",
    "general_term_hierarchy_str = chat_completion.choices[0].message.content\n",
    "print(general_term_hierarchy_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": extract_general_term_prompt.format(task=task),\n",
    "        }, {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": general_term_str,\n",
    "        }, {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": organize_general_term_prompt,\n",
    "        }, {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": general_term_hierarchy_str,\n",
    "        }, {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": generate_checkpoint_prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=GPT_MODEL_EXPENSIVE,\n",
    ")\n",
    "checkpoint_str = chat_completion.choices[0].message.content\n",
    "print(checkpoint_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some necessary sub-questions are not asked in the above example.\n",
    "The sub-questions above are more about asking definition of terms.\n",
    "Check how other papers do question decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sci_review.paper import *\n",
    "from sci_review.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_file = '../../data/systematic_review_papers/planning/CALM.pdf'\n",
    "doc_file = 'aclsum.pdf'\n",
    " # open a document\n",
    "# out = open(\"output.txt\", \"wb\") # create a text output\n",
    "# for page in doc: # iterate the document pages\n",
    "#     text = page.get_text().encode(\"utf8\") # get plain text (is in UTF-8)\n",
    "#     out.write(text) # write text of page\n",
    "#     out.write(bytes((12,))) # write page delimiter (form feed 0x0C)\n",
    "# out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = DocManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.load_doc(doc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in doc.dkg[0][1].items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_node = 0\n",
    "edge_type_test = defaultdict(list)\n",
    "for nbr in doc.dkg[test_node].keys():\n",
    "    for edge_type, edge_data in doc.dkg[test_node][nbr].items():\n",
    "        edge_type_test[edge_type].append((doc.phrases[test_node], doc.phrases[nbr], edge_data))\n",
    "edge_type_test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_type_test['subj_obj'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_sent_edges = []\n",
    "for subj, obj, edge_data in edge_type_test['subj_obj']:\n",
    "    for path in edge_data['paths']:\n",
    "        if subj.sent.start != path[0].sent.start:\n",
    "            cross_sent_edges.append((subj, obj, path))\n",
    "print(len(cross_sent_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prons2corefs = [(p, section.pron_root2corefs[p.root.i]) for section in doc.sections for p in section.prons if any(p.sent.start != coref.sent.start for coref in section.pron_root2corefs[p.root.i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# advcl, ccomp: 1. subj in the clause and the objs in the clause; 2. subj in the head clause and the objs are all the nouns in the clause\n",
    "# acl: clausal modifier of noun (adjectival clause), 1. subj in the clause and the objs in the clause; 2. subj be the head of the clause and objs should be all the nouns in the clause\n",
    "# xcomp: open clausal complement, 1. subjs are subj/obj in the head clause and the objs are all the nouns in the clause\n",
    "# relcl: if the clause has a noun phrase as the subj, then the head of the clause is the object, otherwise the head of the clause is the subj\n",
    "# pcomp: 1. subj in the clause and the objs in the clause; 2. subj in the head clause and the objs are all the nouns in the clause; 3. noun phrase head of the clause as the subj\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dep2verbs = defaultdict(list[Token])\n",
    "# for section in doc.sections:\n",
    "#     if section.section_nlp_local is None:\n",
    "#         continue\n",
    "#     for token in section.section_nlp_local:\n",
    "#         if token.pos_ == 'VERB':\n",
    "#             dep2verbs[token.dep_].append(token)\n",
    "\n",
    "for token in doc.doc_spacy:\n",
    "    if token.pos_ == 'VERB':\n",
    "        dep2verbs[token.dep_].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(dep, len(dep2verbs[dep])) for dep in sorted(dep2verbs, key=lambda k: len(dep2verbs[k]), reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_verbs = [token for token in dep2verbs['acomp'] if doc.tid2phrase_id[token.i] < 0]\n",
    "print(len(check_verbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_verb = check_verbs[0]\n",
    "print(test_verb)\n",
    "print(test_verb.sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy.displacy\n",
    "\n",
    "# spacy.displacy.render(subjs[1].sent)\n",
    "# sent = list(doc.sections[0].section_nlp_local.sents)[0]\n",
    "# sent = doc.phrases[98].sent\n",
    "sent = test_verb.sent\n",
    "spacy.displacy.render(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# advcl, ccomp: 1. subj in the clause and the objs in the clause; 2. subj in the head clause and the objs are all the nouns in the clause\n",
    "# acl: clausal modifier of noun (adjectival clause), 1. subj in the clause and the objs in the clause; 2. subj be the head of the clause and objs should be all the nouns in the clause\n",
    "# xcomp: open clausal complement, 1. subjs are subj/obj in the head clause and the objs are all the nouns in the clause\n",
    "# relcl: if the clause has a noun phrase as the subj, then the head of the clause is the object, otherwise the head of the clause is the subj\n",
    "# pcomp: 1. subj in the clause and the objs in the clause; 2. subj in the head clause and the objs are all the nouns in the clause; 3. noun phrase head of the clause as the subj\n",
    "\n",
    "ADVCL, CCOMP, ACL, XCOMP, RELCL, PCOMP = 'advcl', 'ccomp', 'acl', 'xcomp', 'relcl', 'pcomp'\n",
    "\n",
    "section = doc.sections[0]\n",
    "clause_deps = {ADVCL, CCOMP, ACL, XCOMP, RELCL, PCOMP}\n",
    "dkg = nx.MultiDiGraph()\n",
    "\n",
    "last_subjs, last_sent_start = set[int](), -1\n",
    "sent = doc.phrases[98].sent\n",
    "sent = section.section_nlp_local[sent.start-section.section_nlp_global.start: sent.end-section.section_nlp_global.start]\n",
    "        \n",
    "dep_trees = list[nx.DiGraph]()\n",
    "sent_dep_tree = nx.Graph()\n",
    "roots = [sent.root]\n",
    "tid2tree_id = dict[int, int]()\n",
    "root2clause_roots = defaultdict(list[int])\n",
    "# root2head_id = dict[int, int]()\n",
    "clause_id2subjs = defaultdict(list[tuple[int, Span]])\n",
    "clause_id2objs = defaultdict(list[tuple[int, Span]])\n",
    "\n",
    "while roots:\n",
    "    dep_tree = nx.DiGraph()\n",
    "    root = roots.pop()\n",
    "    dep_tree.add_node(root.i, dep=root.dep_ if root.dep_ not in clause_deps else f'{root.dep_}_ROOT')\n",
    "    dep_tree.graph['root'] = root.i\n",
    "    tokens = [root]\n",
    "    tid2tree_id[root.i] = len(dep_trees)\n",
    "    while tokens:\n",
    "        token = tokens.pop()\n",
    "        for child in token.children:\n",
    "            sent_dep_tree.add_edge(token.i, child.i)\n",
    "            if child.dep_ in clause_deps and doc.tid2phrase_id[child.i + section.section_nlp_global.start] < 0:\n",
    "                roots.append(child)\n",
    "                # root2head_id[child.i] = token.i\n",
    "                root2clause_roots[root.i].append(token.i)\n",
    "            else:\n",
    "                tid2tree_id[child.i] = len(dep_trees)\n",
    "                dep_tree.add_node(child.i, dep=child.dep_)\n",
    "                dep_tree.add_edge(token.i, child.i)\n",
    "                tokens.append(child)\n",
    "    dep_trees.append(dep_tree)\n",
    "    \n",
    "new_last_subjs = set[int]()\n",
    "\n",
    "# Collect subjs and objs in each clause and add in-clause subj-obj edges (coreference outside the clause is considered)\n",
    "for clause_id, dep_tree in enumerate(dep_trees):\n",
    "    # Find the noun phrases\n",
    "    noun_phrases = list[Span]()\n",
    "    phrase_ids = {doc.tid2phrase_id[section.section_nlp_global.start + node] for node in dep_tree.nodes}\n",
    "    for phrase_id in phrase_ids:\n",
    "        if phrase_id >= 0:\n",
    "            noun_phrase = doc.phrases[phrase_id]\n",
    "            noun_phrases.append(section.section_nlp_local[noun_phrase.start-section.section_nlp_global.start: noun_phrase.end-section.section_nlp_global.start])\n",
    "    \n",
    "    for np_label, noun_phrase in [('np', np) for np in noun_phrases] + [('p', pron) for pron in section.prons if dep_tree.has_node(pron.root.i)]:\n",
    "        root_phrase = noun_phrase\n",
    "\n",
    "        while root_phrase.root.dep_ in {'conj', 'appos'}:\n",
    "            root_phrase_id:int = doc.tid2phrase_id[section.section_nlp_global.start + root_phrase.root.head.i]\n",
    "            if root_phrase_id < 0:\n",
    "                break\n",
    "            root_phrase_global = doc.phrases[root_phrase_id]\n",
    "            root_phrase = section.section_nlp_local[root_phrase_global.start-section.section_nlp_global.start: root_phrase_global.end-section.section_nlp_global.start]\n",
    "        \n",
    "        global_phrase_id = -1\n",
    "        if np_label == 'np':\n",
    "            global_phrase_id = doc.tid2phrase_id[section.section_nlp_global.start + noun_phrase.root.i]\n",
    "        else:\n",
    "            for coref in section.pron_root2corefs[noun_phrase.root.i][::-1]:\n",
    "                coref_phrase_id = doc.tid2phrase_id[section.section_nlp_global.start + coref.root.i]\n",
    "                if coref_phrase_id >= 0:\n",
    "                    global_phrase_id = coref_phrase_id\n",
    "                    break\n",
    "        if global_phrase_id < 0:\n",
    "            continue\n",
    "        \n",
    "        if 'subj' in root_phrase.root.dep_:\n",
    "            clause_id2subjs[clause_id].append((global_phrase_id, root_phrase))\n",
    "            if dep_tree.nodes[dep_tree.graph['root']]['dep'] == 'ROOT':\n",
    "                new_last_subjs.add(global_phrase_id)\n",
    "        else:\n",
    "            clause_id2objs[clause_id].append((global_phrase_id, root_phrase))\n",
    "\n",
    "    undirected_dep_tree = dep_tree.to_undirected()\n",
    "    for (subj, subj_root), (obj, obj_root) in itertools.product(clause_id2subjs[clause_id], clause_id2objs[clause_id]):\n",
    "        path = tuple([path_id + section.section_nlp_global.start for path_id in nx.shortest_path(undirected_dep_tree, source=subj_root.root.i, target=obj_root.root.i)[1:-1]])\n",
    "        if not dkg.has_edge(subj, obj, key=SUBJ_OBJ):\n",
    "            dkg.add_edge(subj, obj, key=SUBJ_OBJ, paths=[], weight=1)\n",
    "        dkg[subj][obj][SUBJ_OBJ]['paths'].append(path)\n",
    "        # for last_subj in last_subjs:\n",
    "        #     if dkg.has_edge(last_subj, obj, key=ADJACENT):\n",
    "        #         dkg[last_subj][obj][ADJACENT]['sent_range'].append((last_sent_start + section.section_nlp_global.start, sent.end + section.section_nlp_global.start))\n",
    "        #     else:\n",
    "        #         dkg.add_edge(last_subj, obj, key=ADJACENT, sent_range=[(last_sent_start + section.section_nlp_global.start, sent.end + section.section_nlp_global.start)], weight=2)\n",
    "        \n",
    "# Collect cross-clause edges\n",
    "for clause_id, dep_tree in enumerate(dep_trees):\n",
    "    for sub_clause_root in root2clause_roots[dep_tree.graph['root']]:\n",
    "        sub_clause_id = tid2tree_id[sub_clause_root]\n",
    "        sub_dep_tree = dep_trees[sub_clause_id]\n",
    "        if section.section_nlp_local[sub_clause_root].dep_ in {ADVCL, CCOMP}:\n",
    "            for (subj, subj_root), (obj, obj_root) in itertools.product(clause_id2subjs[clause_id], clause_id2subjs[sub_clause_id] + clause_id2objs[sub_clause_id]):\n",
    "                path = tuple([path_id + section.section_nlp_global.start for path_id in nx.shortest_path(sent_dep_tree, source=subj_root.root.i, target=obj_root.root.i)[1:-1]])\n",
    "                if not dkg.has_edge(subj, obj, key=SUBJ_OBJ):\n",
    "                    dkg.add_edge(subj, obj, key=SUBJ_OBJ, paths=[], weight=1)\n",
    "                dkg[subj][obj][SUBJ_OBJ]['paths'].append(path)\n",
    "        elif section.section_nlp_local[sub_clause_root].dep_ == ACL:\n",
    "            clause_head_tid = section.section_nlp_local[sub_clause_root].head.i\n",
    "            clause_head_noun_phrase_id = doc.tid2phrase_id[clause_head_tid + section.section_nlp_global.start]\n",
    "            if clause_head_noun_phrase_id < 0:\n",
    "                if clause_head_tid in section.pron_root2corefs:\n",
    "                    for coref in section.pron_root2corefs[clause_head_tid][::-1]:\n",
    "                        coref_phrase_id = doc.tid2phrase_id[section.section_nlp_global.start + coref.root.i]\n",
    "                        if coref_phrase_id >= 0:\n",
    "                            clause_head_noun_phrase_id = coref_phrase_id\n",
    "                            break\n",
    "            if clause_head_noun_phrase_id < 0:\n",
    "                continue\n",
    "            for (obj, obj_root) in clause_id2subjs[sub_clause_id] + clause_id2objs[sub_clause_id]:\n",
    "                path = tuple([path_id + section.section_nlp_global.start for path_id in nx.shortest_path(sent_dep_tree, source=clause_head_tid, target=obj_root.root.i)[1:-1]])\n",
    "                if not dkg.has_edge(clause_head_noun_phrase_id, obj, key=SUBJ_OBJ):\n",
    "                    dkg.add_edge(clause_head_noun_phrase_id, obj, key=SUBJ_OBJ, paths=[], weight=1)\n",
    "                dkg[clause_head_noun_phrase_id][obj][SUBJ_OBJ]['paths'].append(path)\n",
    "        elif section.section_nlp_local[sub_clause_root].dep_ == XCOMP:\n",
    "            for (subj, subj_root), (obj, obj_root) in itertools.product(clause_id2subjs[clause_id], clause_id2subjs[sub_clause_id] + clause_id2objs[sub_clause_id]):\n",
    "                path = tuple([path_id + section.section_nlp_global.start for path_id in nx.shortest_path(sent_dep_tree, source=subj_root.root.i, target=obj_root.root.i)[1:-1]])\n",
    "                if not dkg.has_edge(subj, obj, key=SUBJ_OBJ):\n",
    "                    dkg.add_edge(subj, obj, key=SUBJ_OBJ, paths=[], weight=1)\n",
    "                dkg[subj][obj][SUBJ_OBJ]['paths'].append(path)\n",
    "            for (subj, subj_root), (obj, obj_root) in itertools.product(clause_id2objs[clause_id], clause_id2subjs[sub_clause_id] + clause_id2objs[sub_clause_id]):\n",
    "                path = tuple([path_id + section.section_nlp_global.start for path_id in range(subj_root.root.i+1, obj_root.root.i)])\n",
    "                if not dkg.has_edge(subj, obj, key=SUBJ_OBJ):\n",
    "                    dkg.add_edge(subj, obj, key=SUBJ_OBJ, paths=[], weight=1)\n",
    "                dkg[subj][obj][SUBJ_OBJ]['paths'].append(path)\n",
    "        elif section.section_nlp_local[sub_clause_root].dep_ == RELCL:\n",
    "            clause_head_tid = section.section_nlp_local[sub_clause_root].head.i\n",
    "            clause_head_noun_phrase_id = doc.tid2phrase_id[clause_head_tid + section.section_nlp_global.start]\n",
    "            if clause_head_noun_phrase_id < 0:\n",
    "                if clause_head_tid in section.pron_root2corefs:\n",
    "                    for coref in section.pron_root2corefs[clause_head_tid][::-1]:\n",
    "                        coref_phrase_id = doc.tid2phrase_id[section.section_nlp_global.start + coref.root.i]\n",
    "                        if coref_phrase_id >= 0:\n",
    "                            clause_head_noun_phrase_id = coref_phrase_id\n",
    "                            break\n",
    "            if clause_head_noun_phrase_id < 0:\n",
    "                continue\n",
    "            if clause_id2subjs[sub_clause_id]:\n",
    "                for (subj, subj_root) in clause_id2subjs[sub_clause_id]:\n",
    "                    path = tuple([path_id + section.section_nlp_global.start for path_id in nx.shortest_path(sent_dep_tree, source=subj_root.root.i, target=clause_head_tid)[1:-1]])\n",
    "                    if not dkg.has_edge(subj, clause_head_noun_phrase_id, key=SUBJ_OBJ):\n",
    "                        dkg.add_edge(subj, clause_head_noun_phrase_id, key=SUBJ_OBJ, paths=[], weight=1)\n",
    "                    dkg[subj][clause_head_noun_phrase_id][SUBJ_OBJ]['paths'].append(path)\n",
    "            else:\n",
    "                for (obj, obj_root) in clause_id2objs[sub_clause_id]:\n",
    "                    path = tuple([path_id + section.section_nlp_global.start for path_id in nx.shortest_path(sent_dep_tree, source=clause_head_tid, target=obj_root.root.i)[1:-1]])\n",
    "                    if not dkg.has_edge(clause_head_noun_phrase_id, obj, key=SUBJ_OBJ):\n",
    "                        dkg.add_edge(clause_head_noun_phrase_id, obj, key=SUBJ_OBJ, paths=[], weight=1)\n",
    "                    dkg[clause_head_noun_phrase_id][obj][SUBJ_OBJ]['paths'].append(path)\n",
    "        elif section.section_nlp_local[sub_clause_root].dep_ == PCOMP:\n",
    "            for (subj, subj_root), (obj, obj_root) in itertools.product(clause_id2subjs[clause_id], clause_id2subjs[sub_clause_id] + clause_id2objs[sub_clause_id]):\n",
    "                path = tuple([path_id + section.section_nlp_global.start for path_id in nx.shortest_path(sent_dep_tree, source=subj_root.root.i, target=obj_root.root.i)[1:-1]])\n",
    "                if not dkg.has_edge(subj, obj, key=SUBJ_OBJ):\n",
    "                    dkg.add_edge(subj, obj, key=SUBJ_OBJ, paths=[], weight=1)\n",
    "                dkg[subj][obj][SUBJ_OBJ]['paths'].append(path)\n",
    "            clause_head = section.section_nlp_local[sub_clause_root].head\n",
    "            while clause_head.pos_ not in {'NOUN', 'PRON', 'PROPN'}:\n",
    "                new_clause_head = clause_head.head\n",
    "                if new_clause_head == clause_head:\n",
    "                    break\n",
    "                clause_head = new_clause_head\n",
    "            if clause_head.pos_ not in {'NOUN', 'PRON', 'PROPN'}:\n",
    "                continue\n",
    "            clause_head_tid = clause_head.i\n",
    "            clause_head_noun_phrase_id = doc.tid2phrase_id[clause_head_tid + section.section_nlp_global.start]\n",
    "            if clause_head_noun_phrase_id < 0:\n",
    "                if clause_head_tid in section.pron_root2corefs:\n",
    "                    for coref in section.pron_root2corefs[clause_head_tid][::-1]:\n",
    "                        coref_phrase_id = doc.tid2phrase_id[section.section_nlp_global.start + coref.root.i]\n",
    "                        if coref_phrase_id >= 0:\n",
    "                            clause_head_noun_phrase_id = coref_phrase_id\n",
    "                            break\n",
    "            if clause_head_noun_phrase_id < 0:\n",
    "                continue\n",
    "            for (obj, obj_root) in clause_id2subjs[sub_clause_id] + clause_id2objs[sub_clause_id]:\n",
    "                path = tuple([path_id + section.section_nlp_global.start for path_id in nx.shortest_path(sent_dep_tree, source=clause_head_tid, target=obj_root.root.i)[1:-1]])\n",
    "                if not dkg.has_edge(clause_head_noun_phrase_id, obj, key=SUBJ_OBJ):\n",
    "                    dkg.add_edge(clause_head_noun_phrase_id, obj, key=SUBJ_OBJ, paths=[], weight=1)\n",
    "                dkg[clause_head_noun_phrase_id][obj][SUBJ_OBJ]['paths'].append(path)\n",
    "            \n",
    "\n",
    "last_subjs = new_last_subjs\n",
    "last_sent_start = sent.start\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.render(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dkg.edges(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section.pron_root2corefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subj, obj, edge_data in dkg.edges(data=True):\n",
    "    print(f'{doc.phrases[subj].text}, {doc.phrases[obj].text}, {edge_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dkg[101][102][SUBJ_OBJ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot DKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "import dash_cytoscape as cyto\n",
    "from dash import html\n",
    "\n",
    "# [{'data': {'id': token.text, 'label': token.text}, 'position': {'x': token.idx - sent[0].idx, 'y': 50}} for token in sent]\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "app.layout = html.Div([\n",
    "    cyto.Cytoscape(\n",
    "        id='cytoscape',\n",
    "        # elements=[\n",
    "        #     {'data': {'id': 'one', 'label': 'Node 1'}, 'position': {'x': 50, 'y': 50}},\n",
    "        #     {'data': {'id': 'two', 'label': 'Node 2'}, 'position': {'x': 200, 'y': 200}},\n",
    "        #     {'data': {'source': 'one', 'target': 'two','label': 'Node 1 to 2'}}\n",
    "        # ],\n",
    "        elements=[{'data': {'id': token.text, 'label': token.text}, 'position': {'x': (token.idx - sent[0].idx) * 6, 'y': 0}} for token in sent],\n",
    "        layout={'name': 'preset'},\n",
    "        style={'width': '100%', 'height': '100px', 'backgroundColor': 'white'},\n",
    "        stylesheet=[\n",
    "            {\n",
    "                'selector': 'node',\n",
    "                'style': {\n",
    "                    'label': 'data(label)',\n",
    "                    'font-family': 'Courier',\n",
    "                    'font-size': '10',\n",
    "                    'font-weight': 'bold',\n",
    "                    'text-halign': 'right',\n",
    "                    'width': '1',\n",
    "                    'height': '1',\n",
    "                }\n",
    "            },\n",
    "            # {\n",
    "            #     'selector': 'edge',\n",
    "            #     'style': {\n",
    "            #         'label': 'data(label)',\n",
    "            #         'font-family': 'Courier New',\n",
    "            #         'font-size': '12px'\n",
    "            #     }\n",
    "            # }\n",
    "        ]\n",
    "    )\n",
    "])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longdoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
