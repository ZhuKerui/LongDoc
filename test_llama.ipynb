{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZhuKerui/LongDoc/blob/main/test_llama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiLYiY_b4DMG"
      },
      "outputs": [],
      "source": [
        "# If on Google Colab\n",
        "!pip install huggingface_hub transformers xopen accelerate bitsandbytes sentencepiece datasets\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "cur_folder = '/content/gdrive/MyDrive/Colab_Notebooks/fwd/LongDoc/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cur_folder = './'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1LFCSTN4BG_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
        "from huggingface_hub import login\n",
        "login('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')\n",
        "# login(os.environ['HUGGINGFACE_ACCESS_TOKEN'])\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6u30kX24BHA"
      },
      "source": [
        "# Lost in the middle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DuFQyn24BHB"
      },
      "outputs": [],
      "source": [
        "import dataclasses\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import pathlib\n",
        "import random\n",
        "import sys\n",
        "from copy import deepcopy\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from xopen import xopen\n",
        "from pydantic.dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple, Type, TypeVar\n",
        "import string\n",
        "import statistics\n",
        "import regex\n",
        "\n",
        "T = TypeVar(\"T\")\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Document:\n",
        "    title: str\n",
        "    text: str\n",
        "    id: Optional[str] = None\n",
        "    score: Optional[float] = None\n",
        "    hasanswer: Optional[bool] = None\n",
        "    isgold: Optional[bool] = None\n",
        "    original_retrieval_index: Optional[int] = None\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls: Type[T], data: dict) -> T:\n",
        "        data = deepcopy(data)\n",
        "        if not data:\n",
        "            raise ValueError(\"Must provide data for creation of Document from dict.\")\n",
        "        id = data.pop(\"id\", None)\n",
        "        score = data.pop(\"score\", None)\n",
        "        # Convert score to float if it's provided.\n",
        "        if score is not None:\n",
        "            score = float(score)\n",
        "        return cls(**dict(data, id=id, score=score))\n",
        "\n",
        "\n",
        "def get_qa_prompt(\n",
        "    question: str, documents: List[Document], file_name: str = None\n",
        "):\n",
        "    with open(file_name) as f:\n",
        "        prompt_template = f.read().rstrip(\"\\n\")\n",
        "\n",
        "    # Format the documents into strings\n",
        "    formatted_documents = []\n",
        "    for document_index, document in enumerate(documents):\n",
        "        formatted_documents.append(f\"Document [{document_index+1}](Title: {document.title}) {document.text}\")\n",
        "    return prompt_template.format(question=question, search_results=\"\\n\".join(formatted_documents))\n",
        "\n",
        "\n",
        "def normalize_answer(s: str) -> str:\n",
        "    \"\"\"Normalization from the SQuAD evaluation script.\n",
        "\n",
        "    See https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n",
        "    \"\"\"\n",
        "\n",
        "    def remove_articles(text):\n",
        "        return regex.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def best_subspan_em(prediction: str, ground_truths: List[str]) -> float:\n",
        "    normalized_prediction = normalize_answer(prediction)\n",
        "\n",
        "    for ground_truth in ground_truths:\n",
        "        normalized_ground_truth = normalize_answer(ground_truth)\n",
        "        if normalized_ground_truth.lower() in normalized_prediction.lower():\n",
        "            return 1.0\n",
        "    return 0.0\n",
        "\n",
        "METRICS = [\n",
        "    (best_subspan_em, \"best_subspan_em\"),\n",
        "]\n",
        "\n",
        "def get_metrics_for_example(example):\n",
        "    gold_answers = example[\"answers\"]\n",
        "    model_answer = example[\"model_answer\"]\n",
        "\n",
        "    # NOTE: we take everything up to the first newline, since otherwise models could hack\n",
        "    # the metric by simply copying te input context (as the gold answer is guaranteed\n",
        "    # to occur in the input context).\n",
        "    model_answer = model_answer.split(\"\\n\")[0].strip()\n",
        "\n",
        "    example_metrics = {}\n",
        "    for (metric, metric_name) in METRICS:\n",
        "        example_metrics[metric_name] = metric(prediction=model_answer, ground_truths=gold_answers)\n",
        "    return (example_metrics, example)\n",
        "\n",
        "def generate_response(model:AutoModel, tokenizer:AutoTokenizer, input:str, model_label:str, generation_config:GenerationConfig=None):\n",
        "    with torch.no_grad():\n",
        "        if model_label.endswith('base'):\n",
        "            model_inputs = tokenizer(input, return_tensors=\"pt\").to(\"cuda:1\")\n",
        "            output = model.generate(**model_inputs, max_new_tokens=20)#generation_config=generation_config)\n",
        "            response = tokenizer.decode(output[0], skip_special_tokens=True)[len(input):]\n",
        "        else:\n",
        "            if model_label.startswith('chatglm'):\n",
        "                response, history = model.chat(tokenizer, input, history=[])\n",
        "            else:\n",
        "                model_inputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": input}], return_tensors=\"pt\").to(\"cuda:1\")\n",
        "                output = model.generate(model_inputs, max_new_tokens=20)#generation_config=generation_config)\n",
        "                response = tokenizer.decode(output[0][len(model_inputs):], skip_special_tokens=True)\n",
        "            \n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQQXXSAk4BHC"
      },
      "source": [
        "## choose model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hfsBm7c4BHC"
      },
      "source": [
        "### llama-7b-chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MW5mgYaw4BHD"
      },
      "outputs": [],
      "source": [
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "model_label = 'llama2-7b-chat'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwqVqEvzP8W1"
      },
      "source": [
        "### vicuna-7b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuMqGmWsPtmI"
      },
      "outputs": [],
      "source": [
        "model_name = \"lmsys/vicuna-7b-v1.5\"\n",
        "model_label = 'vicuna-7b'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### vicuna-7b-16k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = \"lmsys/vicuna-7b-v1.5-16k\"\n",
        "model_label = 'vicuna-7b-16k'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### chatglm2-6b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoModel\n",
        "# model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\")\n",
        "# model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\")\n",
        "# model = AutoModel.from_pretrained(\"mosaicml/mpt-7b\")\n",
        "# model = AutoModel.from_pretrained(\"mosaicml/mpt-7b-chat\")\n",
        "# model = AutoModel.from_pretrained(\"tiiuae/falcon-7b\")\n",
        "# model = AutoModel.from_pretrained(\"togethercomputer/RedPajama-INCITE-7B-Base\")\n",
        "model = AutoModel.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "049h-wHhQK7e"
      },
      "source": [
        "## load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlPgpgArP1Fh"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda:1\", load_in_4bit=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l7B3F894BHE"
      },
      "source": [
        "## Collect predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PtkD0FG4BHE"
      },
      "outputs": [],
      "source": [
        "total_doc_num = 10\n",
        "position = 0\n",
        "prompt_file = 'qa.prompt'\n",
        "start_idx = 100\n",
        "end_idx = 200\n",
        "last_break_idx = 0\n",
        "generation_config = GenerationConfig(max_new_tokens=20, output_attentions=True)\n",
        "\n",
        "# Create directory for output path if it doesn't exist.\n",
        "input_path = cur_folder + 'data/lost-in-the-middle/qa_data/%d_total_documents/nq-open-%d_total_documents_gold_at_%d.jsonl.gz' % (total_doc_num, total_doc_num, position)\n",
        "output_path = cur_folder + 'data/lost-in-the-middle/qa_predictions/%s-prediction-%d-%d-%d-%d-%s.jsonl.gz' % (model_label, total_doc_num, position, start_idx, end_idx, prompt_file)\n",
        "pathlib.Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "prompt_path = cur_folder + f'data/lost-in-the-middle/prompts/{prompt_file}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiowqQYy4BHF"
      },
      "outputs": [],
      "source": [
        "# Fetch all of the prompts\n",
        "with xopen(input_path) as fin:\n",
        "    samples = []\n",
        "    for idx, line in enumerate(fin):\n",
        "        if idx >= start_idx:\n",
        "            if (idx - start_idx) < last_break_idx:\n",
        "                continue\n",
        "            if idx >= end_idx:\n",
        "                break\n",
        "        else:\n",
        "            continue\n",
        "        input_example = json.loads(line)\n",
        "        samples.append(input_example)\n",
        "\n",
        "examples = []\n",
        "prompts = []\n",
        "all_model_documents = []\n",
        "for input_example in samples:\n",
        "    question = input_example[\"question\"]\n",
        "\n",
        "    documents = []\n",
        "    for ctx in deepcopy(input_example[\"ctxs\"]):\n",
        "        documents.append(Document.from_dict(ctx))\n",
        "\n",
        "    prompt = get_qa_prompt(\n",
        "        question,\n",
        "        documents,\n",
        "        file_name=prompt_path,\n",
        "    )\n",
        "\n",
        "    prompts.append(prompt)\n",
        "    examples.append(deepcopy(input_example))\n",
        "    all_model_documents.append(documents)\n",
        "\n",
        "with torch.no_grad():\n",
        "    with xopen(output_path, \"a\") as f:\n",
        "        for example, model_documents, prompt in tqdm(zip(examples, all_model_documents, prompts), total=len(prompts)):\n",
        "\n",
        "            model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "            output = model.generate(**model_inputs, generation_config=generation_config)\n",
        "            response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "            output_example = deepcopy(example)\n",
        "            # Add some extra metadata to the output example\n",
        "            output_example[\"model_prompt\"] = prompt\n",
        "            output_example[\"model_documents\"] = [dataclasses.asdict(document) for document in model_documents]\n",
        "            output_example[\"model_answer\"] = response[len(prompt):]\n",
        "            f.write(json.dumps(output_example) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "182dSg-PJ1Tv"
      },
      "source": [
        "## Eval predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.listdir('data/lost-in-the-middle/qa_predictions/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCUAkBtsIMXj"
      },
      "outputs": [],
      "source": [
        "for f_name in os.listdir('data/lost-in-the-middle/qa_predictions/'):\n",
        "    if f_name == '4bit':\n",
        "        continue\n",
        "    if not f_name.startswith(\"mistral\"):\n",
        "        continue\n",
        "    all_examples = []\n",
        "    with xopen(f'data/lost-in-the-middle/qa_predictions/{f_name}') as f_in:\n",
        "        for line in f_in:\n",
        "            input_example = json.loads(line)\n",
        "            all_examples.append(input_example)\n",
        "\n",
        "    # Compute normal metrics in parallel, if applicable\n",
        "    all_example_metrics = []\n",
        "    for example in all_examples:\n",
        "        all_example_metrics.append(get_metrics_for_example(example))\n",
        "\n",
        "    # Average metrics across examples\n",
        "    for (_, metric_name) in METRICS:\n",
        "        try:\n",
        "            average_metric_value = statistics.mean(\n",
        "                example_metrics[metric_name] for (example_metrics, _) in all_example_metrics\n",
        "            )\n",
        "            print(f\"{f_name}: {metric_name}: {average_metric_value}\")\n",
        "        except:\n",
        "            print(f_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(all_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_examples[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p = {\n",
        "    \"vicuna-7b\": [0.21, 0.33, 0.28, 0.35],\n",
        "    # \"vicuna-7b-16k\": [0.17, 0.41, 0.3, 0.34],\n",
        "    # \"llama-2-7b-32k\": [0.16, 0.29, 0.34, 0.55],\n",
        "    # \"llama-2-7b-32k-instruct\": [0.18, 0.58, 0.61, 0.59],\n",
        "    \"llama-2-7b-chat\": [0.17, 0.28, 0.23, 0.26],\n",
        "    \"llama-2-7b\": [0.19, 0.19, 0.19, 0.39],\n",
        "    # \"llama-2-7b_re\": [0.16, 0.07, 0.04, 0.14],\n",
        "    # \"llama-2-7b-32k_re\": [0.14, 0.33, 0.26, 0.5],\n",
        "    # \"llama-2-7b-32k-instruct_re\": [0.21, 0.47, 0.51, 0.53]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Make the analysis comprehesive, with other models/datasets\n",
        "# 2. Check the chat/instruct training data, check their length\n",
        "# 3. Maybe devise some datasets ourselves and finetune\n",
        "# 4. Check models in lost in the middle, find model in the intermediate, like extended context windows but not instruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = [2,3,4]\n",
        "for label, y in p.items():\n",
        "    plt.plot(x, y[1:], label=label)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LongBench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import json\n",
        "from transformers import AutoTokenizer, LlamaForCausalLM\n",
        "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer, DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import random\n",
        "import pathlib\n",
        "import re\n",
        "# from llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn\n",
        "# import openai\n",
        "from time import time, sleep\n",
        "from typing import List\n",
        "\n",
        "\n",
        "piece_length = 300\n",
        "\n",
        "prediction_path = cur_folder + \"data/longbench/prediction/\"\n",
        "\n",
        "def seed_everything(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    \n",
        "def output_path(sample:str, model:str, dataset:str=None):\n",
        "    out_path = 'pred_subsample_mqa'\n",
        "    if not os.path.exists(out_path):\n",
        "        os.makedirs(out_path)\n",
        "    if not os.path.exists(f\"{out_path}/{sample}\"):\n",
        "        os.makedirs(f\"{out_path}/{sample}\")\n",
        "    if not os.path.exists(f\"{out_path}/{sample}/{model}\"):\n",
        "        os.makedirs(f\"{out_path}/{sample}/{model}\")\n",
        "    if dataset:\n",
        "        out_path = f\"{out_path}/{sample}/{model}/{dataset}.jsonl\"\n",
        "    else:\n",
        "        out_path = f\"{out_path}/{sample}/{model}/\"\n",
        "    return out_path\n",
        "\n",
        "def split_context(json_obj, tokenizer):\n",
        "    tokenized_context:torch.Tensor = tokenizer(json_obj['context'], truncation=False, return_tensors=\"pt\").input_ids[0]\n",
        "    return [tokenizer.decode(tokenized_context[i * piece_length : (i + 1) * piece_length], skip_special_tokens=True) for i in range((len(tokenized_context) + piece_length - 1) // piece_length)]\n",
        "\n",
        "def build_chat(tokenizer, prompt, model_name):\n",
        "    if \"chatglm\" in model_name:\n",
        "        prompt = tokenizer.build_prompt(prompt)\n",
        "    elif \"longchat\" in model_name or \"vicuna\" in model_name:\n",
        "        from fastchat.model import get_conversation_template\n",
        "        conv = get_conversation_template(\"vicuna\")\n",
        "        conv.append_message(conv.roles[0], prompt)\n",
        "        conv.append_message(conv.roles[1], None)\n",
        "        prompt = conv.get_prompt()        \n",
        "    elif \"llama2\" in model_name:\n",
        "        prompt = f\"[INST]{prompt}[/INST]\"\n",
        "    elif \"xgen\" in model_name:\n",
        "        header = (\n",
        "            \"A chat between a curious human and an artificial intelligence assistant. \"\n",
        "            \"The assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n\"\n",
        "        )\n",
        "        prompt = header + f\" ### Human: {prompt}\\n###\"\n",
        "    elif \"internlm\" in model_name:\n",
        "        prompt = f\"<|User|>:{prompt}<eoh>\\n<|Bot|>:\"\n",
        "    return prompt\n",
        "\n",
        "def construct_prompt(prompt_format:str, json_obj, tokenizer, model_name, use_chat:bool):\n",
        "    prompt = prompt_format.format(**json_obj)\n",
        "    # truncate to fit max_length (we suggest truncate in the middle, since the left and right side may contain crucial instructions)\n",
        "    # tokenized_prompt = tokenizer(prompt, truncation=False, return_tensors=\"pt\").input_ids[0]\n",
        "    # if dataset not in [\"trec\", \"triviaqa\", \"samsum\", \"lsht\", \"lcc\", \"repobench-p\"]: # chat models are better off without build prompts on these tasks\n",
        "    if use_chat:\n",
        "        prompt = build_chat(tokenizer, prompt, model_name)\n",
        "    return prompt\n",
        "\n",
        "def get_pred(model, tokenizer, data, max_input_length, max_text_length, max_gen, prompt_format, subsample_prompt_format, dataset, device, model_name, out_path, start_idx=0, sample:str='llm'):\n",
        "    if sample == 'dpr':\n",
        "        ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "        ctx_model = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\").cuda()\n",
        "        q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "        q_model = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\").cuda()\n",
        "    \n",
        "    context_num_per_run = max_input_length // piece_length\n",
        "    \n",
        "    print(dataset)\n",
        "    for obj_idx, json_obj in enumerate(tqdm(data)):\n",
        "        if obj_idx < start_idx:\n",
        "            continue\n",
        "        \n",
        "        contexts = split_context(json_obj, tokenizer)\n",
        "        scores = []\n",
        "        if sample == 'llm':\n",
        "            subsample_idxs:List[list] = []\n",
        "            # while len(contexts) > context_num_per_run or len(subsample_idxs) == 0:\n",
        "            #     new_contexts = []\n",
        "            #     subsample_idxs.append([])\n",
        "            #     for run in range((len(contexts) + context_num_per_run - 1) // context_num_per_run):\n",
        "            #         temp_context = contexts[run * context_num_per_run : (run + 1) * context_num_per_run]\n",
        "            #         json_obj['context'] = '\\n\\n'.join(['%d: %s' % (idx, c) for idx, c in enumerate(temp_context)])\n",
        "                \n",
        "            #         prompt = construct_prompt(subsample_prompt_format, json_obj, tokenizer, max_length, model_name)\n",
        "\n",
        "            #         try:\n",
        "            #             subsamples = openai_get(model2path[model_name], prompt)\n",
        "            #             # Collect selected paragraph\n",
        "            #             temp_num = []\n",
        "            #             context_idxs = []\n",
        "            #             for a in subsamples:\n",
        "            #                 if a.isnumeric():\n",
        "            #                     temp_num.append(a)\n",
        "            #                 else:\n",
        "            #                     if temp_num:\n",
        "            #                         context_idxs.append(int(''.join(temp_num)))\n",
        "            #                         temp_num = []\n",
        "            #             if temp_num:\n",
        "            #                 context_idxs.append(int(''.join(temp_num)))\n",
        "            #             context_idxs = list(set([idx for idx in context_idxs if idx < len(temp_context)]))\n",
        "            #             context_idxs.sort()\n",
        "                        \n",
        "            #             for idx in context_idxs:\n",
        "            #                 new_contexts.append(temp_context[idx])\n",
        "            #                 subsample_idxs[-1].append(idx + run * context_num_per_run)\n",
        "                            \n",
        "            #         except Exception as e:\n",
        "            #             pass\n",
        "                        \n",
        "            #     contexts = new_contexts\n",
        "        \n",
        "        elif sample == 'dpr':\n",
        "            with torch.no_grad():\n",
        "                ctx_input_ids = ctx_tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')['input_ids'].cuda()\n",
        "                ctx_embeddings = ctx_model(ctx_input_ids).pooler_output\n",
        "                q_input_ids = q_tokenizer(json_obj['input'], padding=True, truncation=True, return_tensors='pt')['input_ids'].cuda()\n",
        "                q_embeddings = q_model(q_input_ids).pooler_output\n",
        "                scores = torch.matmul(q_embeddings, ctx_embeddings.T)[0]\n",
        "                subsample_idxs = [torch.topk(scores, min(len(contexts), (context_num_per_run + 1) // 2)).indices.tolist()]\n",
        "                scores = scores.tolist()\n",
        "                contexts = [contexts[idx] for idx in subsample_idxs[0]]\n",
        "        \n",
        "        # elif sample == 'llm-s':\n",
        "        #     for context in contexts:\n",
        "        #         prompt = f\"Give a score between 0 and 1 to describe how helpful a given context is to answer a given question. A score closer to 0 indicates the context is less helpful and a score closer to 1 indicates the context is more helpful. Ony give me the score and do not provide any explanation.\\n\\nContext: {context}\\n\\nQuestion: {json_obj['input']}\\n\\nScore:\"\n",
        "        #         score = openai_get(model2path[model_name], prompt, 0.1, 0.2)\n",
        "        #         score = re.findall(r\"[-+]?[0-9]*\\.?[0-9]+\", score)\n",
        "        #         if not score:\n",
        "        #             scores.append(0)\n",
        "        #         else:\n",
        "        #             scores.append(float(score[0]))\n",
        "        #     subsample_idxs = [torch.topk(torch.tensor(scores), min(len(contexts), (context_num_per_run + 1) // 2)).indices.tolist()]\n",
        "        #     contexts = [contexts[idx] for idx in subsample_idxs[0]]\n",
        "            \n",
        "        try:\n",
        "            subsample_context = '\\n\\n'.join(contexts)\n",
        "            json_obj['context'] = subsample_context\n",
        "            prompt = construct_prompt(prompt_format, json_obj, tokenizer, max_input_length, model_name)\n",
        "            \n",
        "            pred = openai_get(model2path[model_name], prompt)\n",
        "            \n",
        "        except Exception as e:\n",
        "            pred = 'ERROR'\n",
        "            sleep(10)\n",
        "            \n",
        "        with open(out_path, 'a', encoding=\"utf-8\") as f_out:\n",
        "            json.dump({\"pred\": pred, \"answers\": json_obj[\"answers\"], \"all_classes\": json_obj[\"all_classes\"], \"length\": json_obj[\"length\"], \"subsample_context\": subsample_context, \"subsample_idxs\": subsample_idxs, 'retriever_score': scores}, f_out, ensure_ascii=False)\n",
        "            f_out.write('\\n')\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = \"\"\n",
        "model_label = \"\"\n",
        "sample = 'dpr' # ['llm', 'dpr', 'llm-s', 'none']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed_everything(42)\n",
        "model2path = json.load(open(\"data/longbench/config/model2path.json\", \"r\"))\n",
        "model2maxlen = json.load(open(\"data/longbench/config/model2maxlen.json\", \"r\"))\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# define your model\n",
        "model = LlamaForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "max_length = model2maxlen[model_name]\n",
        "\n",
        "datasets = [\n",
        "    # \"narrativeqa\", \n",
        "            # \"qasper\", \n",
        "            # \"multifieldqa_en\", \n",
        "            # \"multifieldqa_zh\", \n",
        "            # \"hotpotqa\", \n",
        "            # \"2wikimqa\", \n",
        "            \"musique\", \\\n",
        "            # \"dureader\", \"gov_report\", \"qmsum\", \"multi_news\", \"vcsum\", \"trec\", \"triviaqa\", #\\ \"samsum\", \n",
        "            # \"lsht\", \"passage_count\", \"passage_retrieval_en\", \"passage_retrieval_zh\", \"lcc\", \"repobench-p\"\n",
        "            ]\n",
        "# we design specific prompt format and max generation length for each task, feel free to modify them to optimize model output\n",
        "dataset2prompt = json.load(open(\"data/longbench/config/dataset2prompt.json\", \"r\"))\n",
        "dataset2maxlen = json.load(open(\"data/longbench/config/dataset2maxlen.json\", \"r\"))\n",
        "dataset2subsample_prompt = json.load(open(\"data/longbench/config/dataset2subsample_prompt_mqa.json\", \"r\"))\n",
        "# predict on each dataset\n",
        "for dataset in datasets:\n",
        "    data = load_dataset('THUDM/LongBench', dataset, split='test')\n",
        "    if not os.path.exists(prediction_path):\n",
        "        os.mkdir(prediction_path)\n",
        "    out_path = prediction_path + output_path(sample, model_label, dataset)\n",
        "    \n",
        "    prompt_format = dataset2prompt[dataset]\n",
        "    max_gen = dataset2maxlen[dataset]\n",
        "    subsample_prompt_format = dataset2subsample_prompt[dataset]\n",
        "    get_pred(model, tokenizer, data, max_length, max_gen, prompt_format, subsample_prompt_format, dataset, device, model_name, out_path, 130, args.sample)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chat/Instruct Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import load_dataset\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "baize_dataset = load_dataset('linkanjarad/baize-chat-data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "temp_map = defaultdict(list)\n",
        "for data in baize_dataset['train']:\n",
        "    temp_map[data['dataset_origin']].append(data['chat_sample'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "temp_map.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(temp_map['alpaca'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scrolls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scrolls_dataset = load_dataset(\"tau/scrolls\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qasper_dataset = load_dataset(\"tau/scrolls\", \"qasper\")\n",
        "qmsum_dataset = load_dataset(\"tau/scrolls\", \"qmsum\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qasper_dataset['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qmsum_dataset['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HC3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hc3_dataset = load_dataset('Hello-SimpleAI/HC3', 'all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hc3_dataset['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alpaca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "alpaca_dataset = load_dataset('tatsu-lab/alpaca')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "alpaca_dataset['train'][2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for data in alpaca_dataset['train']:\n",
        "    if data['input']:\n",
        "        print(True)\n",
        "        break\n",
        "data"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
