{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZhuKerui/LongDoc/blob/main/test_llama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If on Google Colab\n",
        "!pip install huggingface_hub transformers xopen accelerate bitsandbytes sentencepiece\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "cur_folder = '/content/gdrive/MyDrive/Colab_Notebooks/fwd/LongDoc/'"
      ],
      "metadata": {
        "id": "QiLYiY_b4DMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1LFCSTN4BG_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "from huggingface_hub import login\n",
        "login('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')\n",
        "# login(os.environ['HUGGINGFACE_ACCESS_TOKEN'])\n",
        "from transformers import LlamaForCausalLM, AutoTokenizer, GenerationConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6u30kX24BHA"
      },
      "source": [
        "# Lost in the middle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DuFQyn24BHB"
      },
      "outputs": [],
      "source": [
        "import dataclasses\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import pathlib\n",
        "import random\n",
        "import sys\n",
        "from copy import deepcopy\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from xopen import xopen\n",
        "from pydantic.dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple, Type, TypeVar\n",
        "import string\n",
        "import statistics\n",
        "import regex\n",
        "\n",
        "T = TypeVar(\"T\")\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Document:\n",
        "    title: str\n",
        "    text: str\n",
        "    id: Optional[str] = None\n",
        "    score: Optional[float] = None\n",
        "    hasanswer: Optional[bool] = None\n",
        "    isgold: Optional[bool] = None\n",
        "    original_retrieval_index: Optional[int] = None\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls: Type[T], data: dict) -> T:\n",
        "        data = deepcopy(data)\n",
        "        if not data:\n",
        "            raise ValueError(\"Must provide data for creation of Document from dict.\")\n",
        "        id = data.pop(\"id\", None)\n",
        "        score = data.pop(\"score\", None)\n",
        "        # Convert score to float if it's provided.\n",
        "        if score is not None:\n",
        "            score = float(score)\n",
        "        return cls(**dict(data, id=id, score=score))\n",
        "\n",
        "\n",
        "def get_qa_prompt(\n",
        "    question: str, documents: List[Document], file_name: str = None\n",
        "):\n",
        "    with open(file_name) as f:\n",
        "        prompt_template = f.read().rstrip(\"\\n\")\n",
        "\n",
        "    # Format the documents into strings\n",
        "    formatted_documents = []\n",
        "    for document_index, document in enumerate(documents):\n",
        "        formatted_documents.append(f\"Document [{document_index+1}](Title: {document.title}) {document.text}\")\n",
        "    return prompt_template.format(question=question, search_results=\"\\n\".join(formatted_documents))\n",
        "\n",
        "\n",
        "def normalize_answer(s: str) -> str:\n",
        "    \"\"\"Normalization from the SQuAD evaluation script.\n",
        "\n",
        "    See https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n",
        "    \"\"\"\n",
        "\n",
        "    def remove_articles(text):\n",
        "        return regex.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def best_subspan_em(prediction: str, ground_truths: List[str]) -> float:\n",
        "    normalized_prediction = normalize_answer(prediction)\n",
        "\n",
        "    for ground_truth in ground_truths:\n",
        "        normalized_ground_truth = normalize_answer(ground_truth)\n",
        "        if normalized_ground_truth.lower() in normalized_prediction.lower():\n",
        "            return 1.0\n",
        "    return 0.0\n",
        "\n",
        "METRICS = [\n",
        "    (best_subspan_em, \"best_subspan_em\"),\n",
        "]\n",
        "\n",
        "def get_metrics_for_example(example):\n",
        "    gold_answers = example[\"answers\"]\n",
        "    model_answer = example[\"model_answer\"]\n",
        "\n",
        "    # NOTE: we take everything up to the first newline, since otherwise models could hack\n",
        "    # the metric by simply copying te input context (as the gold answer is guaranteed\n",
        "    # to occur in the input context).\n",
        "    model_answer = model_answer.split(\"\\n\")[0].strip()\n",
        "\n",
        "    example_metrics = {}\n",
        "    for (metric, metric_name) in METRICS:\n",
        "        example_metrics[metric_name] = metric(prediction=model_answer, ground_truths=gold_answers)\n",
        "    return (example_metrics, example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQQXXSAk4BHC"
      },
      "source": [
        "## choose model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hfsBm7c4BHC"
      },
      "source": [
        "### llama-7b-chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MW5mgYaw4BHD"
      },
      "outputs": [],
      "source": [
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "model_label = 'llama2-7b-chat'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### vicuna-7b"
      ],
      "metadata": {
        "id": "TwqVqEvzP8W1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"lmsys/vicuna-7b-v1.5\"\n",
        "model_label = 'vicuna-7b'"
      ],
      "metadata": {
        "id": "HuMqGmWsPtmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load model"
      ],
      "metadata": {
        "id": "049h-wHhQK7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LlamaForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ],
      "metadata": {
        "id": "HlPgpgArP1Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l7B3F894BHE"
      },
      "source": [
        "## Collect predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PtkD0FG4BHE"
      },
      "outputs": [],
      "source": [
        "total_doc_num = 10\n",
        "position = 0\n",
        "prompt_file = 'qa.prompt'\n",
        "start_idx = 100\n",
        "end_idx = 200\n",
        "last_break_idx = 0\n",
        "generation_config = GenerationConfig(max_new_tokens=20)\n",
        "\n",
        "# Create directory for output path if it doesn't exist.\n",
        "input_path = cur_folder + 'data/lost-in-the-middle/qa_data/%d_total_documents/nq-open-%d_total_documents_gold_at_%d.jsonl.gz' % (total_doc_num, total_doc_num, position)\n",
        "output_path = cur_folder + 'data/lost-in-the-middle/qa_predictions/%s-prediction-%d-%d-%d-%d-%s.jsonl.gz' % (model_label, total_doc_num, position, start_idx, end_idx, prompt_file)\n",
        "pathlib.Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "prompt_path = cur_folder + f'data/lost-in-the-middle/prompts/{prompt_file}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiowqQYy4BHF"
      },
      "outputs": [],
      "source": [
        "# Fetch all of the prompts\n",
        "with xopen(input_path) as fin:\n",
        "    samples = []\n",
        "    for idx, line in enumerate(fin):\n",
        "        if idx >= start_idx:\n",
        "            if (idx - start_idx) < last_break_idx:\n",
        "                continue\n",
        "            if idx >= end_idx:\n",
        "                break\n",
        "        else:\n",
        "            continue\n",
        "        input_example = json.loads(line)\n",
        "        samples.append(input_example)\n",
        "\n",
        "examples = []\n",
        "prompts = []\n",
        "all_model_documents = []\n",
        "for input_example in samples:\n",
        "    question = input_example[\"question\"]\n",
        "\n",
        "    documents = []\n",
        "    for ctx in deepcopy(input_example[\"ctxs\"]):\n",
        "        documents.append(Document.from_dict(ctx))\n",
        "\n",
        "    prompt = get_qa_prompt(\n",
        "        question,\n",
        "        documents,\n",
        "        file_name=prompt_path,\n",
        "    )\n",
        "\n",
        "    prompts.append(prompt)\n",
        "    examples.append(deepcopy(input_example))\n",
        "    all_model_documents.append(documents)\n",
        "\n",
        "with torch.no_grad():\n",
        "    with xopen(output_path, \"a\") as f:\n",
        "        for example, model_documents, prompt in tqdm(zip(examples, all_model_documents, prompts), total=len(prompts)):\n",
        "\n",
        "            model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "            output = model.generate(**model_inputs, generation_config=generation_config)\n",
        "            response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "            output_example = deepcopy(example)\n",
        "            # Add some extra metadata to the output example\n",
        "            output_example[\"model_prompt\"] = prompt\n",
        "            output_example[\"model_documents\"] = [dataclasses.asdict(document) for document in model_documents]\n",
        "            output_example[\"model_answer\"] = response[len(prompt):]\n",
        "            f.write(json.dumps(output_example) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eval predictions"
      ],
      "metadata": {
        "id": "182dSg-PJ1Tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_examples = []\n",
        "with xopen(output_path) as fin:\n",
        "    for line in tqdm(fin):\n",
        "        input_example = json.loads(line)\n",
        "        all_examples.append(input_example)\n",
        "\n",
        "# Compute normal metrics in parallel, if applicable\n",
        "all_example_metrics = []\n",
        "for example in tqdm(all_examples):\n",
        "    all_example_metrics.append(get_metrics_for_example(example))\n",
        "\n",
        "# Average metrics across examples\n",
        "for (_, metric_name) in METRICS:\n",
        "    average_metric_value = statistics.mean(\n",
        "        example_metrics[metric_name] for (example_metrics, _) in all_example_metrics\n",
        "    )\n",
        "    print(f\"{metric_name}: {average_metric_value}\")"
      ],
      "metadata": {
        "id": "WCUAkBtsIMXj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}