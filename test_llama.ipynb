{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZhuKerui/LongDoc/blob/main/test_llama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiLYiY_b4DMG",
        "outputId": "676fbc58-f7e4-47c4-f474-dcbe354a0966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n",
            "Installing collected packages: huggingface_hub\n",
            "Successfully installed huggingface_hub-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "R1LFCSTN4BG_",
        "outputId": "971665a1-b118-4f8e-fdbd-815bc68b51e6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-120ae1d3d966>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_VISIBLE_DEVICES'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'HUGGINGFACE_ACCESS_TOKEN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLlamaForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerationConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'HUGGINGFACE_ACCESS_TOKEN'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# If on falcon\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "from huggingface_hub import login\n",
        "login('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')\n",
        "# login(os.environ['HUGGINGFACE_ACCESS_TOKEN'])\n",
        "from transformers import LlamaForCausalLM, AutoTokenizer, GenerationConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6u30kX24BHA"
      },
      "source": [
        "# Lost in the middle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DuFQyn24BHB"
      },
      "outputs": [],
      "source": [
        "import dataclasses\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import pathlib\n",
        "import random\n",
        "import sys\n",
        "from copy import deepcopy\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from xopen import xopen\n",
        "from pydantic.dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple, Type, TypeVar\n",
        "T = TypeVar(\"T\")\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Document:\n",
        "    title: str\n",
        "    text: str\n",
        "    id: Optional[str] = None\n",
        "    score: Optional[float] = None\n",
        "    hasanswer: Optional[bool] = None\n",
        "    isgold: Optional[bool] = None\n",
        "    original_retrieval_index: Optional[int] = None\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls: Type[T], data: dict) -> T:\n",
        "        data = deepcopy(data)\n",
        "        if not data:\n",
        "            raise ValueError(\"Must provide data for creation of Document from dict.\")\n",
        "        id = data.pop(\"id\", None)\n",
        "        score = data.pop(\"score\", None)\n",
        "        # Convert score to float if it's provided.\n",
        "        if score is not None:\n",
        "            score = float(score)\n",
        "        return cls(**dict(data, id=id, score=score))\n",
        "\n",
        "\n",
        "def get_qa_prompt(\n",
        "    question: str, documents: List[Document], file_name: str = None\n",
        "):\n",
        "    with open(file_name) as f:\n",
        "        prompt_template = f.read().rstrip(\"\\n\")\n",
        "\n",
        "    # Format the documents into strings\n",
        "    formatted_documents = []\n",
        "    for document_index, document in enumerate(documents):\n",
        "        formatted_documents.append(f\"Document [{document_index+1}](Title: {document.title}) {document.text}\")\n",
        "    return prompt_template.format(question=question, search_results=\"\\n\".join(formatted_documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQQXXSAk4BHC"
      },
      "source": [
        "## load model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hfsBm7c4BHC"
      },
      "source": [
        "### llama 7b chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MW5mgYaw4BHD"
      },
      "outputs": [],
      "source": [
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "model = LlamaForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "model_label = 'llama2-7b'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l7B3F894BHE"
      },
      "source": [
        "## Collect predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PtkD0FG4BHE"
      },
      "outputs": [],
      "source": [
        "total_doc_num = 20\n",
        "position = 9\n",
        "prompt_file = 'qa.prompt'\n",
        "start_idx = 100\n",
        "end_idx = 200\n",
        "last_break_idx = 0\n",
        "generation_config = GenerationConfig(max_new_tokens=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiowqQYy4BHF"
      },
      "outputs": [],
      "source": [
        "# Create directory for output path if it doesn't exist.\n",
        "input_path = 'data/lost-in-the-middle/qa_data/%d_total_documents/nq-open-%d_total_documents_gold_at_%d.jsonl.gz' % (total_doc_num, total_doc_num, position)\n",
        "output_path = 'data/lost-in-the-middle/qa_predictions/%s-prediction-%d-%d-%d-%d-%s.jsonl.gz' % (model_label, total_doc_num, position, start_idx, end_idx, prompt_file)\n",
        "pathlib.Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Fetch all of the prompts\n",
        "with xopen(input_path) as fin:\n",
        "    samples = []\n",
        "    for idx, line in enumerate(fin):\n",
        "        if idx >= start_idx:\n",
        "            if (idx - start_idx) < last_break_idx:\n",
        "                continue\n",
        "            if idx >= end_idx:\n",
        "                break\n",
        "        else:\n",
        "            continue\n",
        "        input_example = json.loads(line)\n",
        "        samples.append(input_example)\n",
        "\n",
        "\n",
        "examples = []\n",
        "prompts = []\n",
        "all_model_documents = []\n",
        "for input_example in samples:\n",
        "    question = input_example[\"question\"]\n",
        "\n",
        "    documents = []\n",
        "    for ctx in deepcopy(input_example[\"ctxs\"]):\n",
        "        documents.append(Document.from_dict(ctx))\n",
        "\n",
        "    prompt = get_qa_prompt(\n",
        "        question,\n",
        "        documents,\n",
        "        file_name='data/lost-in-the-middle/prompts/qa.prompt',\n",
        "    )\n",
        "\n",
        "    prompts.append(prompt)\n",
        "    examples.append(deepcopy(input_example))\n",
        "    all_model_documents.append(documents)\n",
        "\n",
        "with torch.no_grad():\n",
        "    with xopen(output_path, \"a\") as f:\n",
        "        for example, model_documents, prompt in tqdm(zip(examples, all_model_documents, prompts), total=len(prompts)):\n",
        "\n",
        "            model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "            output = model.generate(**model_inputs, generation_config=generation_config)\n",
        "            response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "            output_example = deepcopy(example)\n",
        "            # Add some extra metadata to the output example\n",
        "            output_example[\"model_prompt\"] = prompt\n",
        "            output_example[\"model_documents\"] = [dataclasses.asdict(document) for document in model_documents]\n",
        "            output_example[\"model_answer\"] = response[len(prompt):]\n",
        "            f.write(json.dumps(output_example) + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}