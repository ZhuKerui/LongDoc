{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "login(os.environ['HUGGINGFACE_ACCESS_TOKEN'])\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lost in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import pathlib\n",
    "import random\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from xopen import xopen\n",
    "from pydantic.dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple, Type, TypeVar\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Document:\n",
    "    title: str\n",
    "    text: str\n",
    "    id: Optional[str] = None\n",
    "    score: Optional[float] = None\n",
    "    hasanswer: Optional[bool] = None\n",
    "    isgold: Optional[bool] = None\n",
    "    original_retrieval_index: Optional[int] = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls: Type[T], data: dict) -> T:\n",
    "        data = deepcopy(data)\n",
    "        if not data:\n",
    "            raise ValueError(\"Must provide data for creation of Document from dict.\")\n",
    "        id = data.pop(\"id\", None)\n",
    "        score = data.pop(\"score\", None)\n",
    "        # Convert score to float if it's provided.\n",
    "        if score is not None:\n",
    "            score = float(score)\n",
    "        return cls(**dict(data, id=id, score=score))\n",
    "\n",
    "\n",
    "def get_qa_prompt(\n",
    "    question: str, documents: List[Document], file_name: str = None\n",
    "):\n",
    "    with open(file_name) as f:\n",
    "        prompt_template = f.read().rstrip(\"\\n\")\n",
    "\n",
    "    # Format the documents into strings\n",
    "    formatted_documents = []\n",
    "    for document_index, document in enumerate(documents):\n",
    "        formatted_documents.append(f\"Document [{document_index+1}](Title: {document.title}) {document.text}\")\n",
    "    return prompt_template.format(question=question, search_results=\"\\n\".join(formatted_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llama 7b chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model = LlamaForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model_label = 'llama2-7b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_doc_num = 20\n",
    "position = 9\n",
    "prompt_file = 'qa.prompt'\n",
    "start_idx = 100\n",
    "end_idx = 200\n",
    "last_break_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for output path if it doesn't exist.\n",
    "input_path = 'data/lost-in-the-middle/qa_data/%d_total_documents/nq-open-%d_total_documents_gold_at_%d.jsonl.gz' % (total_doc_num, total_doc_num, position)\n",
    "output_path = 'data/lost-in-the-middle/qa_predictions/%s-prediction-%d-%d-%d-%d-%s.jsonl.gz' % (model_label, total_doc_num, position, start_idx, end_idx, prompt_file)\n",
    "pathlib.Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Fetch all of the prompts\n",
    "with xopen(input_path) as fin:\n",
    "    samples = []\n",
    "    for idx, line in enumerate(fin):\n",
    "        if idx >= start_idx:\n",
    "            if (idx - start_idx) < last_break_idx:\n",
    "                continue\n",
    "            if idx >= end_idx:\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "        input_example = json.loads(line)\n",
    "        samples.append(input_example)\n",
    "        \n",
    "        \n",
    "examples = []\n",
    "prompts = []\n",
    "all_model_documents = []\n",
    "for input_example in samples:\n",
    "    question = input_example[\"question\"]\n",
    "    \n",
    "    documents = []\n",
    "    for ctx in deepcopy(input_example[\"ctxs\"]):\n",
    "        documents.append(Document.from_dict(ctx))\n",
    "\n",
    "    prompt = get_qa_prompt(\n",
    "        question,\n",
    "        documents,\n",
    "        file_name='data/lost-in-the-middle/prompts/qa.prompt',\n",
    "    )\n",
    "\n",
    "    prompts.append(prompt)\n",
    "    examples.append(deepcopy(input_example))\n",
    "    all_model_documents.append(documents)\n",
    "\n",
    "with xopen(output_path, \"a\") as f:\n",
    "    for example, model_documents, prompt in tqdm(zip(examples, all_model_documents, prompts), total=len(prompts)):\n",
    "        \n",
    "        model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "        output = model.generate(**model_inputs)\n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "        output_example = deepcopy(example)\n",
    "        # Add some extra metadata to the output example\n",
    "        output_example[\"model_prompt\"] = prompt\n",
    "        output_example[\"model_documents\"] = [dataclasses.asdict(document) for document in model_documents]\n",
    "        output_example[\"model_answer\"] = response[len(prompt):]\n",
    "        f.write(json.dumps(output_example) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "petals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
